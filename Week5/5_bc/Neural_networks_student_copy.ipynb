{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linthevanrooij/FAIP/blob/main/Neural_networks_student_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "26ae0979",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "26ae0979",
        "outputId": "5c09411e-7517-4268-b9fb-8c300cde9365"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABD0lEQVR4nO3dsQ2AMAwAQYzYf+WwQigQL3RXp7D0cpPGs9ZaB586vx4AERJECBAhQIQAEQJECBAhQISAa/fhzLw5xy/tfkbYhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFAhAARAkQIECFg+wDq7jFPnrMJASIEiBAgQoAIASIEiBAgQoAIATd81Aq/6iHcCwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 100x100 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "x = np.zeros((1,1)) * 255\n",
        "plt.figure(figsize = (1,1))\n",
        "plt.axis('off')\n",
        "plt.imshow(x,  cmap='gray', vmin=0, vmax=255)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e32b319",
      "metadata": {
        "id": "4e32b319"
      },
      "source": [
        "# An Introduction to Neural Networks\n",
        "In this notebook, the goal is to understand the basics of implementing a fully connected neural network in python using the pytorch library. This is done by completing three steps:\n",
        "<ul>\n",
        "  <li>Loading the MNIST data-set and preformatting the data. The MNIST data set consist out of a number of samples of handwirtten numbers, recorded on a 28x28 grid with values between 0 and 1 for each element.</li>\n",
        "  <li>Constructing a deep and fully connected neural network based on some given hyperparameters. </li>\n",
        "  <li>Training and validating this network on the loaded data-set. Here, we will vary some hyperparamters and investigate their influence on the final result.</li>\n",
        "</ul>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916634f9",
      "metadata": {
        "id": "916634f9"
      },
      "source": [
        "## Loading required Libraries\n",
        "In a first step, we have to load the packages we want to uses. This includes the numpy package for preprocessing the data, the pytorch package (torch) for constructing and training the neural network, and the matplotlib package for visualizing the results. We also define a function that allows us the easy representation of any sample from the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6d7a1fc7",
      "metadata": {
        "id": "6d7a1fc7"
      },
      "outputs": [],
      "source": [
        "# Load numpy library\n",
        "import numpy as np\n",
        "# Load pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# Load matplotlib library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_in_and_out(x, y_true, y_pred = np.array([None, None])):\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x,  cmap='gray', vmin=0, vmax=255)\n",
        "    if (y_pred == None).any():\n",
        "        plt.title('This figure is labelled as a {}.'.format(y_true))\n",
        "    else:\n",
        "        plt.title('This figure is labelled as a {}, \\n while beeing classified as {} (probability: {:0.3f}).'.format(y_true, int(y_pred[0]), y_pred[1]))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce823ebb",
      "metadata": {
        "id": "ce823ebb"
      },
      "source": [
        "## Loading the data-set\n",
        "After setting up our python file, we have to load the MNIST data set. This consists out of a training set and a testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9a04e8c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a04e8c0",
        "outputId": "64426b6f-5b85-4af9-943e-1465b4fd8779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 15357583.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 434283.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4023711.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5278617.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets as datasets\n",
        "# load the training set\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
        "# load the test set\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f5ab12",
      "metadata": {
        "id": "84f5ab12"
      },
      "source": [
        "## Transform the data-set\n",
        "After loading the dataset, we have to load transform it into a better form.\n",
        "Currently, both data set consist out of a list of samples (60 000 training samples and 10 000 testing samples), each sample being a tuple of an image and an integer label. Our goal is to transform those into numpy arrays for input and label of training and testing set each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3313e5b5",
      "metadata": {
        "id": "3313e5b5"
      },
      "outputs": [],
      "source": [
        "# Set up empty numpy arrays:\n",
        "x_train = mnist_trainset.data.numpy() # training input\n",
        "y_train = mnist_trainset.targets.numpy() # training labels\n",
        "\n",
        "x_test = mnist_testset.data.numpy() # testing input\n",
        "y_test = mnist_testset.targets.numpy() # testing labels\n",
        "\n",
        "# test if data loaded correctly\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0c1c6a",
      "metadata": {
        "id": "6a0c1c6a"
      },
      "source": [
        "## Showing an example\n",
        "We can show how one sample looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FUSCCEZGG_g8",
      "metadata": {
        "id": "FUSCCEZGG_g8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a385ecdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "a385ecdb",
        "outputId": "e523dfa3-c04e-42c8-cf7d-b490a2be23ad"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "i_sample = 8\n",
        "plot_in_and_out(x_train[i_sample], y_train[i_sample])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74b23b5",
      "metadata": {
        "id": "c74b23b5"
      },
      "source": [
        "## Transforming the labels\n",
        "The neural network will be constructed to have ten possible outputs, as there are ten possible digits (0,1,...,9). Consequently, the last layer of the network will have ten nodes. Each node here represents the probabilty of the input being the corresponding number. For example, the output [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] will correspond to the network being 100% sure that the input is an 0.\n",
        "\n",
        "To train such a network, our labels of the training set therefore have to be transformed into a so-called one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "37a03bdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37a03bdd",
        "outputId": "4be82a94-0fb5-495c-849f-73869ad95b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The one-hot encoding corresponding to label 1: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# set number of classes\n",
        "num_classes = 10\n",
        "# create array full of zeros\n",
        "y_train_cat = np.zeros((len(y_train), num_classes))\n",
        "# override specific entry in each row with a 1\n",
        "y_train_cat[np.arange(len(y_train)), y_train] = 1\n",
        "\n",
        "print('The one-hot encoding corresponding to label {}:'.format(y_train[i_sample]), y_train_cat[i_sample,:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30956c95",
      "metadata": {
        "id": "30956c95"
      },
      "source": [
        "## Example: Constructing a neural network\n",
        "The goal is now to create a dense neural netwrok which can classify a given number according to its input. <br>\n",
        "Such an network consists out of three parts, which are the <span style=\"color: #ff8080\">input layer</span>, the <span style=\"color: #8080ff\">hidden layers</span> and the <span style=\"color: #80ff80\">output layer</span>.\n",
        "\n",
        "![An examplar structure of a feed-forward neural network](Neural_Network.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee3695ad",
      "metadata": {
        "id": "ee3695ad"
      },
      "source": [
        "In pytorch, such networks are commonly constructed as a model class.\n",
        "For a 3 layer dense neural network, where the input samples have the dimensionality of 28x28, and the hidden layer has 100 nueron, while the output layer has 10, it would be implemented in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64b0028",
      "metadata": {
        "id": "c64b0028"
      },
      "outputs": [],
      "source": [
        "class Neural_network(nn.Module): #inherit the nn.Module class for backpropagation and training functionalities\n",
        "    # Build the layers of the network, and initializes the parameters\n",
        "    def __init__(self):\n",
        "        super(Neural_network, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 100, bias = True)  # fully connected layer from 784 to 100 dimensions (28*28) first -> hidden\n",
        "        self.fc2 = nn.Linear(100, 10, bias = True) # fully connected layer from 100 to 10 dimensions hidden -> output\n",
        "\n",
        "    # Build the forward call\n",
        "    def forward(self, x): # x is the input of dimensionality n x 28 x 28\n",
        "        x = torch.flatten(x, start_dim = 1) # x is reshaped into a n x 784 dimensional input\n",
        "        x = self.fc1(x) # apply the first fully connected layer, x now has shape n x 100\n",
        "        x = F.relu(x) # We apply a ReLU activation to the hidden layer\n",
        "        x = self.fc2(x) # We apply the second fully connected layer, x now has shape n x 10\n",
        "        x = F.softmax(x, dim = -1) # We apply the softmax activation function\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b02639",
      "metadata": {
        "id": "c9b02639"
      },
      "source": [
        "The softmax activation function takes an input x and transforms it into y so that the sum over y is equal to 1. To avoid any influence of the mean of x, one uses the fromula y = exp(x) / sum(exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a28a42",
      "metadata": {
        "id": "a1a28a42"
      },
      "source": [
        "The parts of the model, as well as their respective parameters, can then be displayed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490849f1",
      "metadata": {
        "id": "490849f1",
        "outputId": "e11b6571-7a94-4870-bf31-845215754529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural_network(\n",
            "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Number of parameter arrays: 4\n",
            "The shape of the parameter arrays:\n",
            "torch.Size([100, 784])\n",
            "torch.Size([100])\n",
            "torch.Size([10, 100])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # set random seed for variabl initialization\n",
        "net = Neural_network()\n",
        "print(net)\n",
        "print('')\n",
        "params = list(net.parameters())\n",
        "print('Number of parameter arrays: ' + str(len(params)))\n",
        "print('The shape of the parameter arrays:')\n",
        "for param in params:\n",
        "    print(param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb01ffd",
      "metadata": {
        "id": "ffb01ffd"
      },
      "source": [
        "## Training the model\n",
        "After writing the model class, we now can train the parameters of the model on a data-set. Here, we have to do a number of steps.\n",
        "\n",
        "### Defining a loss function:\n",
        "The network is able to process an input, but to be able to learn, it has to be able to evaluate the input. This is the purpose of the loss function. Here, we will use the mean squared error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634b7966",
      "metadata": {
        "id": "634b7966"
      },
      "outputs": [],
      "source": [
        "loss_func = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c479a0",
      "metadata": {
        "id": "b2c479a0"
      },
      "source": [
        "The we also have to define an optimizer. This takes the gradients of loss function in regard to the parameters and then changes the parameters accordingly. Here, we use the Adam algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f652acb",
      "metadata": {
        "id": "8f652acb"
      },
      "outputs": [],
      "source": [
        "# optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b8d1e0",
      "metadata": {
        "id": "50b8d1e0"
      },
      "source": [
        "Finally, we have to normalize the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186a6f26",
      "metadata": {
        "id": "186a6f26"
      },
      "outputs": [],
      "source": [
        "x_train_norm = x_train.astype('float32')\n",
        "\n",
        "x_mean = x_train_norm.mean(axis = 0, keepdims = True)\n",
        "x_train_norm -= x_mean\n",
        "\n",
        "x_std = (x_train_norm.std(axis = 0, keepdims = True) + 1e-8)\n",
        "x_train_norm /= x_std\n",
        "\n",
        "x_test_norm = (x_test.astype('float32') - x_mean)/x_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69744f66",
      "metadata": {
        "id": "69744f66"
      },
      "source": [
        "Now we have to just iterate over a number of epochs and number of batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a309b43c",
      "metadata": {
        "id": "a309b43c",
        "outputId": "917f576e-9173-4312-c8c9-e4a12f6b0c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for epoch 1/100: 1.5230e-02\n",
            "Loss for epoch 2/100: 6.2644e-03\n",
            "Loss for epoch 3/100: 4.5620e-03\n",
            "Loss for epoch 4/100: 3.5355e-03\n",
            "Loss for epoch 5/100: 2.8777e-03\n",
            "Loss for epoch 6/100: 2.3886e-03\n",
            "Loss for epoch 7/100: 1.9578e-03\n",
            "Loss for epoch 8/100: 1.6776e-03\n",
            "Loss for epoch 9/100: 1.4489e-03\n",
            "Loss for epoch 10/100: 1.2894e-03\n",
            "Loss for epoch 11/100: 1.1228e-03\n",
            "Loss for epoch 12/100: 1.0270e-03\n",
            "Loss for epoch 13/100: 9.5519e-04\n",
            "Loss for epoch 14/100: 8.2692e-04\n",
            "Loss for epoch 15/100: 7.8747e-04\n",
            "Loss for epoch 16/100: 7.7874e-04\n",
            "Loss for epoch 17/100: 7.3504e-04\n",
            "Loss for epoch 18/100: 7.3963e-04\n",
            "Loss for epoch 19/100: 6.2635e-04\n",
            "Loss for epoch 20/100: 6.6493e-04\n",
            "Loss for epoch 21/100: 6.9556e-04\n",
            "Loss for epoch 22/100: 6.1997e-04\n",
            "Loss for epoch 23/100: 5.9584e-04\n",
            "Loss for epoch 24/100: 5.6180e-04\n",
            "Loss for epoch 25/100: 5.2951e-04\n",
            "Loss for epoch 26/100: 5.0684e-04\n",
            "Loss for epoch 27/100: 4.8956e-04\n",
            "Loss for epoch 28/100: 4.9748e-04\n",
            "Loss for epoch 29/100: 5.5249e-04\n",
            "Loss for epoch 30/100: 5.4182e-04\n",
            "Loss for epoch 31/100: 5.0938e-04\n",
            "Loss for epoch 32/100: 5.3307e-04\n",
            "Loss for epoch 33/100: 5.4040e-04\n",
            "Loss for epoch 34/100: 4.9699e-04\n",
            "Loss for epoch 35/100: 4.7176e-04\n",
            "Loss for epoch 36/100: 4.4481e-04\n",
            "Loss for epoch 37/100: 3.9965e-04\n",
            "Loss for epoch 38/100: 4.1939e-04\n",
            "Loss for epoch 39/100: 3.8745e-04\n",
            "Loss for epoch 40/100: 4.6835e-04\n",
            "Loss for epoch 41/100: 5.8298e-04\n",
            "Loss for epoch 42/100: 5.7714e-04\n",
            "Loss for epoch 43/100: 5.0692e-04\n",
            "Loss for epoch 44/100: 5.1218e-04\n",
            "Loss for epoch 45/100: 4.3169e-04\n",
            "Loss for epoch 46/100: 4.2272e-04\n",
            "Loss for epoch 47/100: 4.1343e-04\n",
            "Loss for epoch 48/100: 3.8114e-04\n",
            "Loss for epoch 49/100: 4.4143e-04\n",
            "Loss for epoch 50/100: 4.9017e-04\n",
            "Loss for epoch 51/100: 4.8836e-04\n",
            "Loss for epoch 52/100: 5.2208e-04\n",
            "Loss for epoch 53/100: 4.4801e-04\n",
            "Loss for epoch 54/100: 3.7818e-04\n",
            "Loss for epoch 55/100: 4.0004e-04\n",
            "Loss for epoch 56/100: 3.7843e-04\n",
            "Loss for epoch 57/100: 3.8843e-04\n",
            "Loss for epoch 58/100: 5.3222e-04\n",
            "Loss for epoch 59/100: 5.6195e-04\n",
            "Loss for epoch 60/100: 5.3024e-04\n",
            "Loss for epoch 61/100: 4.6041e-04\n",
            "Loss for epoch 62/100: 4.5685e-04\n",
            "Loss for epoch 63/100: 4.6296e-04\n",
            "Loss for epoch 64/100: 3.9139e-04\n",
            "Loss for epoch 65/100: 3.7804e-04\n",
            "Loss for epoch 66/100: 3.7436e-04\n",
            "Loss for epoch 67/100: 4.2427e-04\n",
            "Loss for epoch 68/100: 4.9444e-04\n",
            "Loss for epoch 69/100: 4.7457e-04\n",
            "Loss for epoch 70/100: 5.2338e-04\n",
            "Loss for epoch 71/100: 4.8527e-04\n",
            "Loss for epoch 72/100: 4.9501e-04\n",
            "Loss for epoch 73/100: 4.9978e-04\n",
            "Loss for epoch 74/100: 4.8058e-04\n",
            "Loss for epoch 75/100: 4.3723e-04\n",
            "Loss for epoch 76/100: 4.0963e-04\n",
            "Loss for epoch 77/100: 3.8017e-04\n",
            "Loss for epoch 78/100: 3.8730e-04\n",
            "Loss for epoch 79/100: 4.8795e-04\n",
            "Loss for epoch 80/100: 4.6226e-04\n",
            "Loss for epoch 81/100: 4.2746e-04\n",
            "Loss for epoch 82/100: 4.5380e-04\n",
            "Loss for epoch 83/100: 4.5307e-04\n",
            "Loss for epoch 84/100: 3.9585e-04\n",
            "Loss for epoch 85/100: 3.7233e-04\n",
            "Loss for epoch 86/100: 4.2523e-04\n",
            "Loss for epoch 87/100: 4.8804e-04\n",
            "Loss for epoch 88/100: 4.3326e-04\n",
            "Loss for epoch 89/100: 4.1470e-04\n",
            "Loss for epoch 90/100: 4.3014e-04\n",
            "Loss for epoch 91/100: 4.5639e-04\n",
            "Loss for epoch 92/100: 5.0529e-04\n",
            "Loss for epoch 93/100: 4.4073e-04\n",
            "Loss for epoch 94/100: 4.4329e-04\n",
            "Loss for epoch 95/100: 3.7239e-04\n",
            "Loss for epoch 96/100: 3.6236e-04\n",
            "Loss for epoch 97/100: 3.6541e-04\n",
            "Loss for epoch 98/100: 4.1164e-04\n",
            "Loss for epoch 99/100: 4.7647e-04\n",
            "Loss for epoch 100/100: 4.8398e-04\n"
          ]
        }
      ],
      "source": [
        "epochs = 100 # how many times do we want to go through the whole data set\n",
        "batch_size = 200 # how many samples do we process before updating weights\n",
        "batches = int(np.floor(len(y_train)/batch_size)) # how many batches are there when dividing the whole data set\n",
        "\n",
        "net.train() # set network to training mode\n",
        "Index = np.arange(len(y_train)) #Index, so we can randomly shuffle inputs and outputs\n",
        "\n",
        "np.random.seed(0) # set random seed for shuffling\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    np.random.shuffle(Index) # shuffle indices, so we do not circle during optimization\n",
        "\n",
        "    loss_epoch = 0\n",
        "\n",
        "    for batch in range(batches):\n",
        "        Index_batch = Index[batch * batch_size:(batch + 1) * batch_size]\n",
        "        x_batch = torch.from_numpy(x_train_norm[Index_batch]) # Get respective input data and transform into torch tensor\n",
        "        y_batch = torch.from_numpy(y_train_cat[Index_batch].astype('float32')) # Get respective output data and transform into torch tensor\n",
        "\n",
        "        # delete gradients from optimizer (otherwise, gradients are cummulative summed up over all previous batches)\n",
        "        optimizer.zero_grad()\n",
        "        # predict the output for the given inputs (forward pass)\n",
        "        y_batch_pred = net(x_batch)\n",
        "        # calculate the loss of the predicted input (forward pass)\n",
        "        loss = loss_func(y_batch_pred, y_batch)\n",
        "        # get the gradients of the trainable paramters for the given loss (backward pass)\n",
        "        loss.backward()\n",
        "        # apply the gradients and change weights\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss\n",
        "\n",
        "    loss_epoch /= batches\n",
        "    print('Loss for epoch {}/{}: {:0.4e}'.format(epoch,epochs, loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193ca335",
      "metadata": {
        "id": "193ca335"
      },
      "source": [
        "## Testing the model\n",
        "Finally, the model has to be tested:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f25e4f1",
      "metadata": {
        "id": "5f25e4f1"
      },
      "outputs": [],
      "source": [
        "net.eval() # Set model inot evaluation mode\n",
        "with torch.no_grad(): # Only build forwards graph => faster method\n",
        "    y_test_pred = net(torch.from_numpy(x_test_norm))\n",
        "y_test_pred = y_test_pred.detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b9bdf8",
      "metadata": {
        "id": "d2b9bdf8"
      },
      "source": [
        "To get a prediction, we now have to find the label with the highest probability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166afefe",
      "metadata": {
        "id": "166afefe"
      },
      "outputs": [],
      "source": [
        "y_pred = np.concatenate((y_test_pred.argmax(axis = 1)[:,np.newaxis],          # Get the number with the highest probability\n",
        "                         y_test_pred.max(axis = 1)[:,np.newaxis]), axis = 1)  # Get the corresponding probability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b89c7ed",
      "metadata": {
        "id": "3b89c7ed"
      },
      "source": [
        "We can now visualize the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92a093b",
      "metadata": {
        "id": "a92a093b",
        "outputId": "2d26fa68-d774-4ac4-ff7f-dff88a6f720b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "i_sample_test = 4\n",
        "plot_in_and_out(x_test[i_sample_test], y_test[i_sample_test], y_pred[i_sample_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83eabe1",
      "metadata": {
        "id": "e83eabe1"
      },
      "source": [
        "### Determine Accuracy\n",
        "Finally, one can determine the accuracy of the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2e195d",
      "metadata": {
        "id": "9a2e195d",
        "outputId": "38c1cfca-d184-4511-f289-dcee654f6de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of the model on the test set is 97.32%\n"
          ]
        }
      ],
      "source": [
        "accuracy = np.mean(y_test == y_pred[:,0])\n",
        "print('The accuracy of the model on the test set is {:0.2f}%'.format(100*accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b65bf5e1",
      "metadata": {
        "id": "b65bf5e1",
        "outputId": "b58f70e9-22ca-4709-9e0a-2286ff8d7b04"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "failures = np.where(y_test != y_pred[:,0])[0]\n",
        "\n",
        "i_sample_failure = failures[-1]\n",
        "plot_in_and_out(x_test[i_sample_failure], y_test[i_sample_failure], y_pred[i_sample_failure])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7253a176",
      "metadata": {
        "id": "7253a176"
      },
      "source": [
        "## Varying the hyperparamters\n",
        "After seeing the example, we now have the goal of building a function which allows us to evaluate the performance for a certain implementation of a neural network, given a number of hyperparamters. The goal is to implement a neural network with two hidden layers, using relu activation in all but the last layers.\n",
        "The following things are to be varied:\n",
        "<ul>\n",
        "  <li>The optimizer: Use Adam as well as SDG</li>\n",
        "  <li>The batch size: Use 10, 100, 1000, 10000 </li>\n",
        "  <li>The number of neuron in the second hidden layer: Use 10, 50, 100 </li>\n",
        "</ul>  \n",
        "Meanwhile, 100 epochs are to be used for training, and the first hidden layer should have 100 neurons.\n",
        "<br>\n",
        "The accuracy of the trained models on the test has to be calculated for a comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1de9eb9e",
      "metadata": {
        "id": "1de9eb9e"
      },
      "outputs": [],
      "source": [
        "class Neural_network(nn.Module): #inherit the nn.Module class for backpropagation and training functionalities\n",
        "    # Build the layers of the network, and initializes the parameters\n",
        "    def __init__(self, num_neurons):\n",
        "        super(Neural_network, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 100, bias = True)  # fully connected layer from 784 to 100 dimensions (28*28) first -> hidden\n",
        "        self.fc2 = nn.Linear(100, num_neurons)\n",
        "        self.fc3 = nn.Linear(num_neurons, 10, bias = True) # fully connected layer from 100 to 10 dimensions hidden -> output\n",
        "\n",
        "    # Build the forward call\n",
        "    def forward(self, x): # x is the input of dimensionality n x 28 x 28\n",
        "        x = torch.flatten(x, start_dim = 1) # x is reshaped into a n x 784 dimensional input\n",
        "        x = self.fc1(x) # apply the first fully connected layer, x now has shape n x 100\n",
        "        x = F.relu(x) # We apply a ReLU activation to the hidden layer\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x) # We apply the second fully connected layer, x now has shape n x 10\n",
        "        x = F.softmax(x, dim = -1) # We apply the softmax activation function\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b446298f",
      "metadata": {
        "id": "b446298f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_implementations(x_train, x_test, y_train, y_train_cat, opt, epochs, batch_size, num_neurons):\n",
        "    torch.manual_seed(0)\n",
        "    net = Neural_network(num_neurons=num_neurons)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # OPTIMIZERS\n",
        "    if opt == \"SGD\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "    else:\n",
        "        optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "\n",
        "    ## NORMALIZE\n",
        "    x_train_norm = x_train.astype('float32')\n",
        "\n",
        "    x_mean = x_train_norm.mean(axis = 0, keepdims = True)\n",
        "    x_train_norm -= x_mean\n",
        "\n",
        "    x_std = (x_train_norm.std(axis = 0, keepdims = True) + 1e-8)\n",
        "    x_train_norm /= x_std\n",
        "\n",
        "    x_test_norm = (x_test.astype('float32') - x_mean)/x_std\n",
        "\n",
        "    ## TRAINING THE MODEL!!\n",
        "    batches = int(np.floor(len(y_train)/batch_size)) # how many batches are there when dividing the whole data set\n",
        "\n",
        "    net.train() # set network to training mode\n",
        "    Index = np.arange(len(y_train)) #Index, so we can randomly shuffle inputs and outputs\n",
        "\n",
        "    np.random.seed(0) # set random seed for shuffling\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        np.random.shuffle(Index) # shuffle indices, so we do not circle during optimization\n",
        "\n",
        "        loss_epoch = 0\n",
        "\n",
        "        for batch in range(batches):\n",
        "            Index_batch = Index[batch * batch_size:(batch + 1) * batch_size]\n",
        "            x_batch = torch.from_numpy(x_train_norm[Index_batch]) # Get respective input data and transform into torch tensor\n",
        "            y_batch = torch.from_numpy(y_train_cat[Index_batch].astype('float32')) # Get respective output data and transform into torch tensor\n",
        "\n",
        "            # delete gradients from optimizer (otherwise, gradients are cummulative summed up over all previous batches)\n",
        "            optimizer.zero_grad()\n",
        "            # predict the output for the given inputs (forward pass)\n",
        "\n",
        "            y_batch_pred = net(x_batch)\n",
        "            # calculate the loss of the predicted input (forward pass)\n",
        "            loss = loss_func(y_batch_pred, y_batch)\n",
        "            # get the gradients of the trainable paramters for the given loss (backward pass)\n",
        "            loss.backward()\n",
        "            # apply the gradients and change weights\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_epoch += loss\n",
        "\n",
        "        loss_epoch /= batches\n",
        "        print('Loss for epoch {}/{}: {:0.4e}'.format(epoch,epochs, loss_epoch) )\n",
        "\n",
        "        net.eval() # Set model into evaluation mode\n",
        "        with torch.no_grad(): # Only build forwards graph => faster method\n",
        "            y_test_pred = net(torch.from_numpy(x_test_norm))\n",
        "        y_test_pred = y_test_pred.detach().numpy()\n",
        "\n",
        "        y_pred = np.concatenate((y_test_pred.argmax(axis = 1)[:,np.newaxis],          # Get the number with the highest probability\n",
        "                                y_test_pred.max(axis = 1)[:,np.newaxis]), axis = 1)  # Get the corresponding probability\n",
        "        accuracy = np.mean(y_test == y_pred[:,0])\n",
        "    return accuracy\n",
        "\n",
        "## PRINTING PARAMETERS\n",
        "#  # set random seed for variabl initialization\n",
        "# print(net)\n",
        "# print('')\n",
        "# params = list(net.parameters())\n",
        "# print('Number of parameter arrays: ' + str(len(params)))\n",
        "# print('The shape of the parameter arrays:')\n",
        "# for param in params:\n",
        "#     print(param.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "4ad6c957",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ad6c957",
        "outputId": "7b3c8972-4930-4e26-8777-b2343abef375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 10, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 9.0409e-02\n",
            "Loss for epoch 2/100: 9.0126e-02\n",
            "Loss for epoch 3/100: 8.9826e-02\n",
            "Loss for epoch 4/100: 8.9498e-02\n",
            "Loss for epoch 5/100: 8.9126e-02\n",
            "Loss for epoch 6/100: 8.8677e-02\n",
            "Loss for epoch 7/100: 8.8114e-02\n",
            "Loss for epoch 8/100: 8.7394e-02\n",
            "Loss for epoch 9/100: 8.6425e-02\n",
            "Loss for epoch 10/100: 8.5036e-02\n",
            "Loss for epoch 11/100: 8.2998e-02\n",
            "Loss for epoch 12/100: 8.0452e-02\n",
            "Loss for epoch 13/100: 7.8282e-02\n",
            "Loss for epoch 14/100: 7.6850e-02\n",
            "Loss for epoch 15/100: 7.5847e-02\n",
            "Loss for epoch 16/100: 7.5003e-02\n",
            "Loss for epoch 17/100: 7.4185e-02\n",
            "Loss for epoch 18/100: 7.3325e-02\n",
            "Loss for epoch 19/100: 7.2386e-02\n",
            "Loss for epoch 20/100: 7.1352e-02\n",
            "Loss for epoch 21/100: 7.0249e-02\n",
            "Loss for epoch 22/100: 6.9133e-02\n",
            "Loss for epoch 23/100: 6.8055e-02\n",
            "Loss for epoch 24/100: 6.7023e-02\n",
            "Loss for epoch 25/100: 6.6007e-02\n",
            "Loss for epoch 26/100: 6.4963e-02\n",
            "Loss for epoch 27/100: 6.3843e-02\n",
            "Loss for epoch 28/100: 6.2600e-02\n",
            "Loss for epoch 29/100: 6.1215e-02\n",
            "Loss for epoch 30/100: 5.9718e-02\n",
            "Loss for epoch 31/100: 5.8156e-02\n",
            "Loss for epoch 32/100: 5.6504e-02\n",
            "Loss for epoch 33/100: 5.4660e-02\n",
            "Loss for epoch 34/100: 5.2538e-02\n",
            "Loss for epoch 35/100: 5.0212e-02\n",
            "Loss for epoch 36/100: 4.7893e-02\n",
            "Loss for epoch 37/100: 4.5694e-02\n",
            "Loss for epoch 38/100: 4.3620e-02\n",
            "Loss for epoch 39/100: 4.1726e-02\n",
            "Loss for epoch 40/100: 4.0085e-02\n",
            "Loss for epoch 41/100: 3.8711e-02\n",
            "Loss for epoch 42/100: 3.7554e-02\n",
            "Loss for epoch 43/100: 3.6553e-02\n",
            "Loss for epoch 44/100: 3.5656e-02\n",
            "Loss for epoch 45/100: 3.4828e-02\n",
            "Loss for epoch 46/100: 3.4036e-02\n",
            "Loss for epoch 47/100: 3.3250e-02\n",
            "Loss for epoch 48/100: 3.2439e-02\n",
            "Loss for epoch 49/100: 3.1571e-02\n",
            "Loss for epoch 50/100: 3.0606e-02\n",
            "Loss for epoch 51/100: 2.9525e-02\n",
            "Loss for epoch 52/100: 2.8366e-02\n",
            "Loss for epoch 53/100: 2.7234e-02\n",
            "Loss for epoch 54/100: 2.6214e-02\n",
            "Loss for epoch 55/100: 2.5294e-02\n",
            "Loss for epoch 56/100: 2.4426e-02\n",
            "Loss for epoch 57/100: 2.3580e-02\n",
            "Loss for epoch 58/100: 2.2751e-02\n",
            "Loss for epoch 59/100: 2.1951e-02\n",
            "Loss for epoch 60/100: 2.1199e-02\n",
            "Loss for epoch 61/100: 2.0507e-02\n",
            "Loss for epoch 62/100: 1.9877e-02\n",
            "Loss for epoch 63/100: 1.9307e-02\n",
            "Loss for epoch 64/100: 1.8790e-02\n",
            "Loss for epoch 65/100: 1.8321e-02\n",
            "Loss for epoch 66/100: 1.7894e-02\n",
            "Loss for epoch 67/100: 1.7504e-02\n",
            "Loss for epoch 68/100: 1.7146e-02\n",
            "Loss for epoch 69/100: 1.6815e-02\n",
            "Loss for epoch 70/100: 1.6510e-02\n",
            "Loss for epoch 71/100: 1.6227e-02\n",
            "Loss for epoch 72/100: 1.5962e-02\n",
            "Loss for epoch 73/100: 1.5716e-02\n",
            "Loss for epoch 74/100: 1.5483e-02\n",
            "Loss for epoch 75/100: 1.5265e-02\n",
            "Loss for epoch 76/100: 1.5058e-02\n",
            "Loss for epoch 77/100: 1.4862e-02\n",
            "Loss for epoch 78/100: 1.4676e-02\n",
            "Loss for epoch 79/100: 1.4499e-02\n",
            "Loss for epoch 80/100: 1.4330e-02\n",
            "Loss for epoch 81/100: 1.4169e-02\n",
            "Loss for epoch 82/100: 1.4015e-02\n",
            "Loss for epoch 83/100: 1.3866e-02\n",
            "Loss for epoch 84/100: 1.3723e-02\n",
            "Loss for epoch 85/100: 1.3586e-02\n",
            "Loss for epoch 86/100: 1.3453e-02\n",
            "Loss for epoch 87/100: 1.3325e-02\n",
            "Loss for epoch 88/100: 1.3201e-02\n",
            "Loss for epoch 89/100: 1.3081e-02\n",
            "Loss for epoch 90/100: 1.2965e-02\n",
            "Loss for epoch 91/100: 1.2852e-02\n",
            "Loss for epoch 92/100: 1.2743e-02\n",
            "Loss for epoch 93/100: 1.2636e-02\n",
            "Loss for epoch 94/100: 1.2532e-02\n",
            "Loss for epoch 95/100: 1.2431e-02\n",
            "Loss for epoch 96/100: 1.2332e-02\n",
            "Loss for epoch 97/100: 1.2236e-02\n",
            "Loss for epoch 98/100: 1.2141e-02\n",
            "Loss for epoch 99/100: 1.2049e-02\n",
            "Loss for epoch 100/100: 1.1959e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 10, 'num_neurons': 10}: 0.9214\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 10, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 8.9879e-02\n",
            "Loss for epoch 2/100: 8.9494e-02\n",
            "Loss for epoch 3/100: 8.9092e-02\n",
            "Loss for epoch 4/100: 8.8664e-02\n",
            "Loss for epoch 5/100: 8.8194e-02\n",
            "Loss for epoch 6/100: 8.7668e-02\n",
            "Loss for epoch 7/100: 8.7063e-02\n",
            "Loss for epoch 8/100: 8.6349e-02\n",
            "Loss for epoch 9/100: 8.5483e-02\n",
            "Loss for epoch 10/100: 8.4397e-02\n",
            "Loss for epoch 11/100: 8.2987e-02\n",
            "Loss for epoch 12/100: 8.1096e-02\n",
            "Loss for epoch 13/100: 7.8497e-02\n",
            "Loss for epoch 14/100: 7.4957e-02\n",
            "Loss for epoch 15/100: 7.0596e-02\n",
            "Loss for epoch 16/100: 6.6273e-02\n",
            "Loss for epoch 17/100: 6.2640e-02\n",
            "Loss for epoch 18/100: 5.9569e-02\n",
            "Loss for epoch 19/100: 5.6801e-02\n",
            "Loss for epoch 20/100: 5.4243e-02\n",
            "Loss for epoch 21/100: 5.1836e-02\n",
            "Loss for epoch 22/100: 4.9497e-02\n",
            "Loss for epoch 23/100: 4.7190e-02\n",
            "Loss for epoch 24/100: 4.4977e-02\n",
            "Loss for epoch 25/100: 4.2937e-02\n",
            "Loss for epoch 26/100: 4.1098e-02\n",
            "Loss for epoch 27/100: 3.9450e-02\n",
            "Loss for epoch 28/100: 3.7975e-02\n",
            "Loss for epoch 29/100: 3.6655e-02\n",
            "Loss for epoch 30/100: 3.5466e-02\n",
            "Loss for epoch 31/100: 3.4382e-02\n",
            "Loss for epoch 32/100: 3.3374e-02\n",
            "Loss for epoch 33/100: 3.2412e-02\n",
            "Loss for epoch 34/100: 3.1464e-02\n",
            "Loss for epoch 35/100: 3.0502e-02\n",
            "Loss for epoch 36/100: 2.9506e-02\n",
            "Loss for epoch 37/100: 2.8474e-02\n",
            "Loss for epoch 38/100: 2.7432e-02\n",
            "Loss for epoch 39/100: 2.6416e-02\n",
            "Loss for epoch 40/100: 2.5441e-02\n",
            "Loss for epoch 41/100: 2.4498e-02\n",
            "Loss for epoch 42/100: 2.3573e-02\n",
            "Loss for epoch 43/100: 2.2669e-02\n",
            "Loss for epoch 44/100: 2.1802e-02\n",
            "Loss for epoch 45/100: 2.0992e-02\n",
            "Loss for epoch 46/100: 2.0249e-02\n",
            "Loss for epoch 47/100: 1.9575e-02\n",
            "Loss for epoch 48/100: 1.8967e-02\n",
            "Loss for epoch 49/100: 1.8417e-02\n",
            "Loss for epoch 50/100: 1.7919e-02\n",
            "Loss for epoch 51/100: 1.7468e-02\n",
            "Loss for epoch 52/100: 1.7056e-02\n",
            "Loss for epoch 53/100: 1.6680e-02\n",
            "Loss for epoch 54/100: 1.6334e-02\n",
            "Loss for epoch 55/100: 1.6015e-02\n",
            "Loss for epoch 56/100: 1.5720e-02\n",
            "Loss for epoch 57/100: 1.5446e-02\n",
            "Loss for epoch 58/100: 1.5190e-02\n",
            "Loss for epoch 59/100: 1.4951e-02\n",
            "Loss for epoch 60/100: 1.4726e-02\n",
            "Loss for epoch 61/100: 1.4514e-02\n",
            "Loss for epoch 62/100: 1.4315e-02\n",
            "Loss for epoch 63/100: 1.4126e-02\n",
            "Loss for epoch 64/100: 1.3947e-02\n",
            "Loss for epoch 65/100: 1.3776e-02\n",
            "Loss for epoch 66/100: 1.3612e-02\n",
            "Loss for epoch 67/100: 1.3457e-02\n",
            "Loss for epoch 68/100: 1.3308e-02\n",
            "Loss for epoch 69/100: 1.3165e-02\n",
            "Loss for epoch 70/100: 1.3028e-02\n",
            "Loss for epoch 71/100: 1.2896e-02\n",
            "Loss for epoch 72/100: 1.2769e-02\n",
            "Loss for epoch 73/100: 1.2647e-02\n",
            "Loss for epoch 74/100: 1.2528e-02\n",
            "Loss for epoch 75/100: 1.2413e-02\n",
            "Loss for epoch 76/100: 1.2302e-02\n",
            "Loss for epoch 77/100: 1.2195e-02\n",
            "Loss for epoch 78/100: 1.2090e-02\n",
            "Loss for epoch 79/100: 1.1988e-02\n",
            "Loss for epoch 80/100: 1.1890e-02\n",
            "Loss for epoch 81/100: 1.1793e-02\n",
            "Loss for epoch 82/100: 1.1700e-02\n",
            "Loss for epoch 83/100: 1.1608e-02\n",
            "Loss for epoch 84/100: 1.1519e-02\n",
            "Loss for epoch 85/100: 1.1432e-02\n",
            "Loss for epoch 86/100: 1.1348e-02\n",
            "Loss for epoch 87/100: 1.1265e-02\n",
            "Loss for epoch 88/100: 1.1184e-02\n",
            "Loss for epoch 89/100: 1.1104e-02\n",
            "Loss for epoch 90/100: 1.1027e-02\n",
            "Loss for epoch 91/100: 1.0951e-02\n",
            "Loss for epoch 92/100: 1.0877e-02\n",
            "Loss for epoch 93/100: 1.0804e-02\n",
            "Loss for epoch 94/100: 1.0733e-02\n",
            "Loss for epoch 95/100: 1.0663e-02\n",
            "Loss for epoch 96/100: 1.0595e-02\n",
            "Loss for epoch 97/100: 1.0528e-02\n",
            "Loss for epoch 98/100: 1.0462e-02\n",
            "Loss for epoch 99/100: 1.0397e-02\n",
            "Loss for epoch 100/100: 1.0333e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 10, 'num_neurons': 50}: 0.9315\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 10, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 9.0203e-02\n",
            "Loss for epoch 2/100: 8.9879e-02\n",
            "Loss for epoch 3/100: 8.9544e-02\n",
            "Loss for epoch 4/100: 8.9189e-02\n",
            "Loss for epoch 5/100: 8.8798e-02\n",
            "Loss for epoch 6/100: 8.8355e-02\n",
            "Loss for epoch 7/100: 8.7829e-02\n",
            "Loss for epoch 8/100: 8.7175e-02\n",
            "Loss for epoch 9/100: 8.6316e-02\n",
            "Loss for epoch 10/100: 8.5150e-02\n",
            "Loss for epoch 11/100: 8.3650e-02\n",
            "Loss for epoch 12/100: 8.1992e-02\n",
            "Loss for epoch 13/100: 8.0306e-02\n",
            "Loss for epoch 14/100: 7.8525e-02\n",
            "Loss for epoch 15/100: 7.6634e-02\n",
            "Loss for epoch 16/100: 7.4651e-02\n",
            "Loss for epoch 17/100: 7.2477e-02\n",
            "Loss for epoch 18/100: 7.0137e-02\n",
            "Loss for epoch 19/100: 6.8047e-02\n",
            "Loss for epoch 20/100: 6.6326e-02\n",
            "Loss for epoch 21/100: 6.4666e-02\n",
            "Loss for epoch 22/100: 6.2800e-02\n",
            "Loss for epoch 23/100: 6.0558e-02\n",
            "Loss for epoch 24/100: 5.7907e-02\n",
            "Loss for epoch 25/100: 5.5043e-02\n",
            "Loss for epoch 26/100: 5.2232e-02\n",
            "Loss for epoch 27/100: 4.9568e-02\n",
            "Loss for epoch 28/100: 4.7047e-02\n",
            "Loss for epoch 29/100: 4.4716e-02\n",
            "Loss for epoch 30/100: 4.2593e-02\n",
            "Loss for epoch 31/100: 4.0605e-02\n",
            "Loss for epoch 32/100: 3.8641e-02\n",
            "Loss for epoch 33/100: 3.6628e-02\n",
            "Loss for epoch 34/100: 3.4579e-02\n",
            "Loss for epoch 35/100: 3.2577e-02\n",
            "Loss for epoch 36/100: 3.0692e-02\n",
            "Loss for epoch 37/100: 2.8940e-02\n",
            "Loss for epoch 38/100: 2.7307e-02\n",
            "Loss for epoch 39/100: 2.5793e-02\n",
            "Loss for epoch 40/100: 2.4422e-02\n",
            "Loss for epoch 41/100: 2.3207e-02\n",
            "Loss for epoch 42/100: 2.2142e-02\n",
            "Loss for epoch 43/100: 2.1210e-02\n",
            "Loss for epoch 44/100: 2.0391e-02\n",
            "Loss for epoch 45/100: 1.9669e-02\n",
            "Loss for epoch 46/100: 1.9029e-02\n",
            "Loss for epoch 47/100: 1.8457e-02\n",
            "Loss for epoch 48/100: 1.7944e-02\n",
            "Loss for epoch 49/100: 1.7481e-02\n",
            "Loss for epoch 50/100: 1.7061e-02\n",
            "Loss for epoch 51/100: 1.6678e-02\n",
            "Loss for epoch 52/100: 1.6328e-02\n",
            "Loss for epoch 53/100: 1.6005e-02\n",
            "Loss for epoch 54/100: 1.5707e-02\n",
            "Loss for epoch 55/100: 1.5431e-02\n",
            "Loss for epoch 56/100: 1.5173e-02\n",
            "Loss for epoch 57/100: 1.4932e-02\n",
            "Loss for epoch 58/100: 1.4707e-02\n",
            "Loss for epoch 59/100: 1.4494e-02\n",
            "Loss for epoch 60/100: 1.4294e-02\n",
            "Loss for epoch 61/100: 1.4105e-02\n",
            "Loss for epoch 62/100: 1.3925e-02\n",
            "Loss for epoch 63/100: 1.3754e-02\n",
            "Loss for epoch 64/100: 1.3591e-02\n",
            "Loss for epoch 65/100: 1.3435e-02\n",
            "Loss for epoch 66/100: 1.3285e-02\n",
            "Loss for epoch 67/100: 1.3143e-02\n",
            "Loss for epoch 68/100: 1.3005e-02\n",
            "Loss for epoch 69/100: 1.2873e-02\n",
            "Loss for epoch 70/100: 1.2745e-02\n",
            "Loss for epoch 71/100: 1.2622e-02\n",
            "Loss for epoch 72/100: 1.2503e-02\n",
            "Loss for epoch 73/100: 1.2388e-02\n",
            "Loss for epoch 74/100: 1.2276e-02\n",
            "Loss for epoch 75/100: 1.2168e-02\n",
            "Loss for epoch 76/100: 1.2062e-02\n",
            "Loss for epoch 77/100: 1.1960e-02\n",
            "Loss for epoch 78/100: 1.1861e-02\n",
            "Loss for epoch 79/100: 1.1765e-02\n",
            "Loss for epoch 80/100: 1.1671e-02\n",
            "Loss for epoch 81/100: 1.1579e-02\n",
            "Loss for epoch 82/100: 1.1490e-02\n",
            "Loss for epoch 83/100: 1.1402e-02\n",
            "Loss for epoch 84/100: 1.1317e-02\n",
            "Loss for epoch 85/100: 1.1234e-02\n",
            "Loss for epoch 86/100: 1.1153e-02\n",
            "Loss for epoch 87/100: 1.1073e-02\n",
            "Loss for epoch 88/100: 1.0995e-02\n",
            "Loss for epoch 89/100: 1.0919e-02\n",
            "Loss for epoch 90/100: 1.0844e-02\n",
            "Loss for epoch 91/100: 1.0771e-02\n",
            "Loss for epoch 92/100: 1.0699e-02\n",
            "Loss for epoch 93/100: 1.0629e-02\n",
            "Loss for epoch 94/100: 1.0560e-02\n",
            "Loss for epoch 95/100: 1.0491e-02\n",
            "Loss for epoch 96/100: 1.0425e-02\n",
            "Loss for epoch 97/100: 1.0360e-02\n",
            "Loss for epoch 98/100: 1.0295e-02\n",
            "Loss for epoch 99/100: 1.0232e-02\n",
            "Loss for epoch 100/100: 1.0170e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 10, 'num_neurons': 100}: 0.9344\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 100, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 9.0534e-02\n",
            "Loss for epoch 2/100: 9.0506e-02\n",
            "Loss for epoch 3/100: 9.0479e-02\n",
            "Loss for epoch 4/100: 9.0451e-02\n",
            "Loss for epoch 5/100: 9.0423e-02\n",
            "Loss for epoch 6/100: 9.0396e-02\n",
            "Loss for epoch 7/100: 9.0368e-02\n",
            "Loss for epoch 8/100: 9.0340e-02\n",
            "Loss for epoch 9/100: 9.0312e-02\n",
            "Loss for epoch 10/100: 9.0284e-02\n",
            "Loss for epoch 11/100: 9.0255e-02\n",
            "Loss for epoch 12/100: 9.0227e-02\n",
            "Loss for epoch 13/100: 9.0199e-02\n",
            "Loss for epoch 14/100: 9.0170e-02\n",
            "Loss for epoch 15/100: 9.0141e-02\n",
            "Loss for epoch 16/100: 9.0113e-02\n",
            "Loss for epoch 17/100: 9.0083e-02\n",
            "Loss for epoch 18/100: 9.0054e-02\n",
            "Loss for epoch 19/100: 9.0025e-02\n",
            "Loss for epoch 20/100: 8.9995e-02\n",
            "Loss for epoch 21/100: 8.9965e-02\n",
            "Loss for epoch 22/100: 8.9935e-02\n",
            "Loss for epoch 23/100: 8.9904e-02\n",
            "Loss for epoch 24/100: 8.9873e-02\n",
            "Loss for epoch 25/100: 8.9842e-02\n",
            "Loss for epoch 26/100: 8.9811e-02\n",
            "Loss for epoch 27/100: 8.9779e-02\n",
            "Loss for epoch 28/100: 8.9748e-02\n",
            "Loss for epoch 29/100: 8.9715e-02\n",
            "Loss for epoch 30/100: 8.9683e-02\n",
            "Loss for epoch 31/100: 8.9650e-02\n",
            "Loss for epoch 32/100: 8.9617e-02\n",
            "Loss for epoch 33/100: 8.9584e-02\n",
            "Loss for epoch 34/100: 8.9550e-02\n",
            "Loss for epoch 35/100: 8.9516e-02\n",
            "Loss for epoch 36/100: 8.9482e-02\n",
            "Loss for epoch 37/100: 8.9447e-02\n",
            "Loss for epoch 38/100: 8.9412e-02\n",
            "Loss for epoch 39/100: 8.9376e-02\n",
            "Loss for epoch 40/100: 8.9339e-02\n",
            "Loss for epoch 41/100: 8.9303e-02\n",
            "Loss for epoch 42/100: 8.9265e-02\n",
            "Loss for epoch 43/100: 8.9227e-02\n",
            "Loss for epoch 44/100: 8.9189e-02\n",
            "Loss for epoch 45/100: 8.9149e-02\n",
            "Loss for epoch 46/100: 8.9109e-02\n",
            "Loss for epoch 47/100: 8.9068e-02\n",
            "Loss for epoch 48/100: 8.9026e-02\n",
            "Loss for epoch 49/100: 8.8984e-02\n",
            "Loss for epoch 50/100: 8.8940e-02\n",
            "Loss for epoch 51/100: 8.8896e-02\n",
            "Loss for epoch 52/100: 8.8850e-02\n",
            "Loss for epoch 53/100: 8.8803e-02\n",
            "Loss for epoch 54/100: 8.8756e-02\n",
            "Loss for epoch 55/100: 8.8707e-02\n",
            "Loss for epoch 56/100: 8.8657e-02\n",
            "Loss for epoch 57/100: 8.8606e-02\n",
            "Loss for epoch 58/100: 8.8553e-02\n",
            "Loss for epoch 59/100: 8.8500e-02\n",
            "Loss for epoch 60/100: 8.8445e-02\n",
            "Loss for epoch 61/100: 8.8389e-02\n",
            "Loss for epoch 62/100: 8.8332e-02\n",
            "Loss for epoch 63/100: 8.8273e-02\n",
            "Loss for epoch 64/100: 8.8213e-02\n",
            "Loss for epoch 65/100: 8.8152e-02\n",
            "Loss for epoch 66/100: 8.8089e-02\n",
            "Loss for epoch 67/100: 8.8025e-02\n",
            "Loss for epoch 68/100: 8.7959e-02\n",
            "Loss for epoch 69/100: 8.7891e-02\n",
            "Loss for epoch 70/100: 8.7822e-02\n",
            "Loss for epoch 71/100: 8.7750e-02\n",
            "Loss for epoch 72/100: 8.7677e-02\n",
            "Loss for epoch 73/100: 8.7602e-02\n",
            "Loss for epoch 74/100: 8.7525e-02\n",
            "Loss for epoch 75/100: 8.7445e-02\n",
            "Loss for epoch 76/100: 8.7364e-02\n",
            "Loss for epoch 77/100: 8.7279e-02\n",
            "Loss for epoch 78/100: 8.7192e-02\n",
            "Loss for epoch 79/100: 8.7103e-02\n",
            "Loss for epoch 80/100: 8.7011e-02\n",
            "Loss for epoch 81/100: 8.6915e-02\n",
            "Loss for epoch 82/100: 8.6817e-02\n",
            "Loss for epoch 83/100: 8.6715e-02\n",
            "Loss for epoch 84/100: 8.6609e-02\n",
            "Loss for epoch 85/100: 8.6500e-02\n",
            "Loss for epoch 86/100: 8.6387e-02\n",
            "Loss for epoch 87/100: 8.6269e-02\n",
            "Loss for epoch 88/100: 8.6148e-02\n",
            "Loss for epoch 89/100: 8.6021e-02\n",
            "Loss for epoch 90/100: 8.5890e-02\n",
            "Loss for epoch 91/100: 8.5753e-02\n",
            "Loss for epoch 92/100: 8.5611e-02\n",
            "Loss for epoch 93/100: 8.5463e-02\n",
            "Loss for epoch 94/100: 8.5310e-02\n",
            "Loss for epoch 95/100: 8.5150e-02\n",
            "Loss for epoch 96/100: 8.4983e-02\n",
            "Loss for epoch 97/100: 8.4809e-02\n",
            "Loss for epoch 98/100: 8.4628e-02\n",
            "Loss for epoch 99/100: 8.4440e-02\n",
            "Loss for epoch 100/100: 8.4244e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 100, 'num_neurons': 10}: 0.3056\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 100, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 9.0049e-02\n",
            "Loss for epoch 2/100: 9.0011e-02\n",
            "Loss for epoch 3/100: 8.9974e-02\n",
            "Loss for epoch 4/100: 8.9936e-02\n",
            "Loss for epoch 5/100: 8.9898e-02\n",
            "Loss for epoch 6/100: 8.9860e-02\n",
            "Loss for epoch 7/100: 8.9822e-02\n",
            "Loss for epoch 8/100: 8.9784e-02\n",
            "Loss for epoch 9/100: 8.9745e-02\n",
            "Loss for epoch 10/100: 8.9707e-02\n",
            "Loss for epoch 11/100: 8.9669e-02\n",
            "Loss for epoch 12/100: 8.9630e-02\n",
            "Loss for epoch 13/100: 8.9591e-02\n",
            "Loss for epoch 14/100: 8.9552e-02\n",
            "Loss for epoch 15/100: 8.9514e-02\n",
            "Loss for epoch 16/100: 8.9474e-02\n",
            "Loss for epoch 17/100: 8.9435e-02\n",
            "Loss for epoch 18/100: 8.9396e-02\n",
            "Loss for epoch 19/100: 8.9356e-02\n",
            "Loss for epoch 20/100: 8.9316e-02\n",
            "Loss for epoch 21/100: 8.9276e-02\n",
            "Loss for epoch 22/100: 8.9236e-02\n",
            "Loss for epoch 23/100: 8.9195e-02\n",
            "Loss for epoch 24/100: 8.9155e-02\n",
            "Loss for epoch 25/100: 8.9114e-02\n",
            "Loss for epoch 26/100: 8.9073e-02\n",
            "Loss for epoch 27/100: 8.9031e-02\n",
            "Loss for epoch 28/100: 8.8989e-02\n",
            "Loss for epoch 29/100: 8.8947e-02\n",
            "Loss for epoch 30/100: 8.8905e-02\n",
            "Loss for epoch 31/100: 8.8862e-02\n",
            "Loss for epoch 32/100: 8.8819e-02\n",
            "Loss for epoch 33/100: 8.8775e-02\n",
            "Loss for epoch 34/100: 8.8732e-02\n",
            "Loss for epoch 35/100: 8.8687e-02\n",
            "Loss for epoch 36/100: 8.8643e-02\n",
            "Loss for epoch 37/100: 8.8598e-02\n",
            "Loss for epoch 38/100: 8.8552e-02\n",
            "Loss for epoch 39/100: 8.8506e-02\n",
            "Loss for epoch 40/100: 8.8460e-02\n",
            "Loss for epoch 41/100: 8.8413e-02\n",
            "Loss for epoch 42/100: 8.8366e-02\n",
            "Loss for epoch 43/100: 8.8318e-02\n",
            "Loss for epoch 44/100: 8.8270e-02\n",
            "Loss for epoch 45/100: 8.8221e-02\n",
            "Loss for epoch 46/100: 8.8172e-02\n",
            "Loss for epoch 47/100: 8.8122e-02\n",
            "Loss for epoch 48/100: 8.8071e-02\n",
            "Loss for epoch 49/100: 8.8020e-02\n",
            "Loss for epoch 50/100: 8.7968e-02\n",
            "Loss for epoch 51/100: 8.7916e-02\n",
            "Loss for epoch 52/100: 8.7863e-02\n",
            "Loss for epoch 53/100: 8.7809e-02\n",
            "Loss for epoch 54/100: 8.7754e-02\n",
            "Loss for epoch 55/100: 8.7699e-02\n",
            "Loss for epoch 56/100: 8.7643e-02\n",
            "Loss for epoch 57/100: 8.7586e-02\n",
            "Loss for epoch 58/100: 8.7528e-02\n",
            "Loss for epoch 59/100: 8.7470e-02\n",
            "Loss for epoch 60/100: 8.7410e-02\n",
            "Loss for epoch 61/100: 8.7350e-02\n",
            "Loss for epoch 62/100: 8.7289e-02\n",
            "Loss for epoch 63/100: 8.7226e-02\n",
            "Loss for epoch 64/100: 8.7163e-02\n",
            "Loss for epoch 65/100: 8.7099e-02\n",
            "Loss for epoch 66/100: 8.7034e-02\n",
            "Loss for epoch 67/100: 8.6968e-02\n",
            "Loss for epoch 68/100: 8.6900e-02\n",
            "Loss for epoch 69/100: 8.6832e-02\n",
            "Loss for epoch 70/100: 8.6762e-02\n",
            "Loss for epoch 71/100: 8.6691e-02\n",
            "Loss for epoch 72/100: 8.6618e-02\n",
            "Loss for epoch 73/100: 8.6545e-02\n",
            "Loss for epoch 74/100: 8.6470e-02\n",
            "Loss for epoch 75/100: 8.6393e-02\n",
            "Loss for epoch 76/100: 8.6315e-02\n",
            "Loss for epoch 77/100: 8.6236e-02\n",
            "Loss for epoch 78/100: 8.6155e-02\n",
            "Loss for epoch 79/100: 8.6072e-02\n",
            "Loss for epoch 80/100: 8.5988e-02\n",
            "Loss for epoch 81/100: 8.5902e-02\n",
            "Loss for epoch 82/100: 8.5814e-02\n",
            "Loss for epoch 83/100: 8.5724e-02\n",
            "Loss for epoch 84/100: 8.5632e-02\n",
            "Loss for epoch 85/100: 8.5538e-02\n",
            "Loss for epoch 86/100: 8.5442e-02\n",
            "Loss for epoch 87/100: 8.5344e-02\n",
            "Loss for epoch 88/100: 8.5244e-02\n",
            "Loss for epoch 89/100: 8.5141e-02\n",
            "Loss for epoch 90/100: 8.5036e-02\n",
            "Loss for epoch 91/100: 8.4928e-02\n",
            "Loss for epoch 92/100: 8.4818e-02\n",
            "Loss for epoch 93/100: 8.4704e-02\n",
            "Loss for epoch 94/100: 8.4588e-02\n",
            "Loss for epoch 95/100: 8.4469e-02\n",
            "Loss for epoch 96/100: 8.4347e-02\n",
            "Loss for epoch 97/100: 8.4222e-02\n",
            "Loss for epoch 98/100: 8.4093e-02\n",
            "Loss for epoch 99/100: 8.3961e-02\n",
            "Loss for epoch 100/100: 8.3825e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 100, 'num_neurons': 50}: 0.5047\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 100, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 9.0346e-02\n",
            "Loss for epoch 2/100: 9.0314e-02\n",
            "Loss for epoch 3/100: 9.0282e-02\n",
            "Loss for epoch 4/100: 9.0250e-02\n",
            "Loss for epoch 5/100: 9.0218e-02\n",
            "Loss for epoch 6/100: 9.0186e-02\n",
            "Loss for epoch 7/100: 9.0154e-02\n",
            "Loss for epoch 8/100: 9.0122e-02\n",
            "Loss for epoch 9/100: 9.0090e-02\n",
            "Loss for epoch 10/100: 9.0058e-02\n",
            "Loss for epoch 11/100: 9.0025e-02\n",
            "Loss for epoch 12/100: 8.9993e-02\n",
            "Loss for epoch 13/100: 8.9960e-02\n",
            "Loss for epoch 14/100: 8.9928e-02\n",
            "Loss for epoch 15/100: 8.9895e-02\n",
            "Loss for epoch 16/100: 8.9863e-02\n",
            "Loss for epoch 17/100: 8.9830e-02\n",
            "Loss for epoch 18/100: 8.9797e-02\n",
            "Loss for epoch 19/100: 8.9764e-02\n",
            "Loss for epoch 20/100: 8.9730e-02\n",
            "Loss for epoch 21/100: 8.9697e-02\n",
            "Loss for epoch 22/100: 8.9663e-02\n",
            "Loss for epoch 23/100: 8.9630e-02\n",
            "Loss for epoch 24/100: 8.9596e-02\n",
            "Loss for epoch 25/100: 8.9562e-02\n",
            "Loss for epoch 26/100: 8.9528e-02\n",
            "Loss for epoch 27/100: 8.9493e-02\n",
            "Loss for epoch 28/100: 8.9458e-02\n",
            "Loss for epoch 29/100: 8.9423e-02\n",
            "Loss for epoch 30/100: 8.9388e-02\n",
            "Loss for epoch 31/100: 8.9353e-02\n",
            "Loss for epoch 32/100: 8.9317e-02\n",
            "Loss for epoch 33/100: 8.9281e-02\n",
            "Loss for epoch 34/100: 8.9245e-02\n",
            "Loss for epoch 35/100: 8.9208e-02\n",
            "Loss for epoch 36/100: 8.9171e-02\n",
            "Loss for epoch 37/100: 8.9134e-02\n",
            "Loss for epoch 38/100: 8.9096e-02\n",
            "Loss for epoch 39/100: 8.9058e-02\n",
            "Loss for epoch 40/100: 8.9019e-02\n",
            "Loss for epoch 41/100: 8.8980e-02\n",
            "Loss for epoch 42/100: 8.8941e-02\n",
            "Loss for epoch 43/100: 8.8901e-02\n",
            "Loss for epoch 44/100: 8.8861e-02\n",
            "Loss for epoch 45/100: 8.8820e-02\n",
            "Loss for epoch 46/100: 8.8779e-02\n",
            "Loss for epoch 47/100: 8.8738e-02\n",
            "Loss for epoch 48/100: 8.8695e-02\n",
            "Loss for epoch 49/100: 8.8652e-02\n",
            "Loss for epoch 50/100: 8.8609e-02\n",
            "Loss for epoch 51/100: 8.8565e-02\n",
            "Loss for epoch 52/100: 8.8520e-02\n",
            "Loss for epoch 53/100: 8.8474e-02\n",
            "Loss for epoch 54/100: 8.8428e-02\n",
            "Loss for epoch 55/100: 8.8381e-02\n",
            "Loss for epoch 56/100: 8.8334e-02\n",
            "Loss for epoch 57/100: 8.8285e-02\n",
            "Loss for epoch 58/100: 8.8236e-02\n",
            "Loss for epoch 59/100: 8.8185e-02\n",
            "Loss for epoch 60/100: 8.8134e-02\n",
            "Loss for epoch 61/100: 8.8082e-02\n",
            "Loss for epoch 62/100: 8.8029e-02\n",
            "Loss for epoch 63/100: 8.7975e-02\n",
            "Loss for epoch 64/100: 8.7919e-02\n",
            "Loss for epoch 65/100: 8.7863e-02\n",
            "Loss for epoch 66/100: 8.7805e-02\n",
            "Loss for epoch 67/100: 8.7746e-02\n",
            "Loss for epoch 68/100: 8.7686e-02\n",
            "Loss for epoch 69/100: 8.7624e-02\n",
            "Loss for epoch 70/100: 8.7560e-02\n",
            "Loss for epoch 71/100: 8.7496e-02\n",
            "Loss for epoch 72/100: 8.7429e-02\n",
            "Loss for epoch 73/100: 8.7361e-02\n",
            "Loss for epoch 74/100: 8.7291e-02\n",
            "Loss for epoch 75/100: 8.7219e-02\n",
            "Loss for epoch 76/100: 8.7146e-02\n",
            "Loss for epoch 77/100: 8.7070e-02\n",
            "Loss for epoch 78/100: 8.6992e-02\n",
            "Loss for epoch 79/100: 8.6912e-02\n",
            "Loss for epoch 80/100: 8.6829e-02\n",
            "Loss for epoch 81/100: 8.6744e-02\n",
            "Loss for epoch 82/100: 8.6656e-02\n",
            "Loss for epoch 83/100: 8.6566e-02\n",
            "Loss for epoch 84/100: 8.6473e-02\n",
            "Loss for epoch 85/100: 8.6377e-02\n",
            "Loss for epoch 86/100: 8.6278e-02\n",
            "Loss for epoch 87/100: 8.6175e-02\n",
            "Loss for epoch 88/100: 8.6070e-02\n",
            "Loss for epoch 89/100: 8.5961e-02\n",
            "Loss for epoch 90/100: 8.5848e-02\n",
            "Loss for epoch 91/100: 8.5732e-02\n",
            "Loss for epoch 92/100: 8.5612e-02\n",
            "Loss for epoch 93/100: 8.5488e-02\n",
            "Loss for epoch 94/100: 8.5361e-02\n",
            "Loss for epoch 95/100: 8.5230e-02\n",
            "Loss for epoch 96/100: 8.5095e-02\n",
            "Loss for epoch 97/100: 8.4956e-02\n",
            "Loss for epoch 98/100: 8.4814e-02\n",
            "Loss for epoch 99/100: 8.4669e-02\n",
            "Loss for epoch 100/100: 8.4520e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 100, 'num_neurons': 100}: 0.3407\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 9.0546e-02\n",
            "Loss for epoch 2/100: 9.0544e-02\n",
            "Loss for epoch 3/100: 9.0541e-02\n",
            "Loss for epoch 4/100: 9.0538e-02\n",
            "Loss for epoch 5/100: 9.0535e-02\n",
            "Loss for epoch 6/100: 9.0532e-02\n",
            "Loss for epoch 7/100: 9.0530e-02\n",
            "Loss for epoch 8/100: 9.0527e-02\n",
            "Loss for epoch 9/100: 9.0524e-02\n",
            "Loss for epoch 10/100: 9.0521e-02\n",
            "Loss for epoch 11/100: 9.0519e-02\n",
            "Loss for epoch 12/100: 9.0516e-02\n",
            "Loss for epoch 13/100: 9.0513e-02\n",
            "Loss for epoch 14/100: 9.0510e-02\n",
            "Loss for epoch 15/100: 9.0508e-02\n",
            "Loss for epoch 16/100: 9.0505e-02\n",
            "Loss for epoch 17/100: 9.0502e-02\n",
            "Loss for epoch 18/100: 9.0499e-02\n",
            "Loss for epoch 19/100: 9.0497e-02\n",
            "Loss for epoch 20/100: 9.0494e-02\n",
            "Loss for epoch 21/100: 9.0491e-02\n",
            "Loss for epoch 22/100: 9.0488e-02\n",
            "Loss for epoch 23/100: 9.0486e-02\n",
            "Loss for epoch 24/100: 9.0483e-02\n",
            "Loss for epoch 25/100: 9.0480e-02\n",
            "Loss for epoch 26/100: 9.0477e-02\n",
            "Loss for epoch 27/100: 9.0474e-02\n",
            "Loss for epoch 28/100: 9.0472e-02\n",
            "Loss for epoch 29/100: 9.0469e-02\n",
            "Loss for epoch 30/100: 9.0466e-02\n",
            "Loss for epoch 31/100: 9.0463e-02\n",
            "Loss for epoch 32/100: 9.0461e-02\n",
            "Loss for epoch 33/100: 9.0458e-02\n",
            "Loss for epoch 34/100: 9.0455e-02\n",
            "Loss for epoch 35/100: 9.0452e-02\n",
            "Loss for epoch 36/100: 9.0450e-02\n",
            "Loss for epoch 37/100: 9.0447e-02\n",
            "Loss for epoch 38/100: 9.0444e-02\n",
            "Loss for epoch 39/100: 9.0441e-02\n",
            "Loss for epoch 40/100: 9.0438e-02\n",
            "Loss for epoch 41/100: 9.0436e-02\n",
            "Loss for epoch 42/100: 9.0433e-02\n",
            "Loss for epoch 43/100: 9.0430e-02\n",
            "Loss for epoch 44/100: 9.0427e-02\n",
            "Loss for epoch 45/100: 9.0425e-02\n",
            "Loss for epoch 46/100: 9.0422e-02\n",
            "Loss for epoch 47/100: 9.0419e-02\n",
            "Loss for epoch 48/100: 9.0416e-02\n",
            "Loss for epoch 49/100: 9.0414e-02\n",
            "Loss for epoch 50/100: 9.0411e-02\n",
            "Loss for epoch 51/100: 9.0408e-02\n",
            "Loss for epoch 52/100: 9.0405e-02\n",
            "Loss for epoch 53/100: 9.0402e-02\n",
            "Loss for epoch 54/100: 9.0400e-02\n",
            "Loss for epoch 55/100: 9.0397e-02\n",
            "Loss for epoch 56/100: 9.0394e-02\n",
            "Loss for epoch 57/100: 9.0391e-02\n",
            "Loss for epoch 58/100: 9.0389e-02\n",
            "Loss for epoch 59/100: 9.0386e-02\n",
            "Loss for epoch 60/100: 9.0383e-02\n",
            "Loss for epoch 61/100: 9.0380e-02\n",
            "Loss for epoch 62/100: 9.0377e-02\n",
            "Loss for epoch 63/100: 9.0375e-02\n",
            "Loss for epoch 64/100: 9.0372e-02\n",
            "Loss for epoch 65/100: 9.0369e-02\n",
            "Loss for epoch 66/100: 9.0366e-02\n",
            "Loss for epoch 67/100: 9.0363e-02\n",
            "Loss for epoch 68/100: 9.0361e-02\n",
            "Loss for epoch 69/100: 9.0358e-02\n",
            "Loss for epoch 70/100: 9.0355e-02\n",
            "Loss for epoch 71/100: 9.0352e-02\n",
            "Loss for epoch 72/100: 9.0350e-02\n",
            "Loss for epoch 73/100: 9.0347e-02\n",
            "Loss for epoch 74/100: 9.0344e-02\n",
            "Loss for epoch 75/100: 9.0341e-02\n",
            "Loss for epoch 76/100: 9.0338e-02\n",
            "Loss for epoch 77/100: 9.0336e-02\n",
            "Loss for epoch 78/100: 9.0333e-02\n",
            "Loss for epoch 79/100: 9.0330e-02\n",
            "Loss for epoch 80/100: 9.0327e-02\n",
            "Loss for epoch 81/100: 9.0324e-02\n",
            "Loss for epoch 82/100: 9.0322e-02\n",
            "Loss for epoch 83/100: 9.0319e-02\n",
            "Loss for epoch 84/100: 9.0316e-02\n",
            "Loss for epoch 85/100: 9.0313e-02\n",
            "Loss for epoch 86/100: 9.0310e-02\n",
            "Loss for epoch 87/100: 9.0308e-02\n",
            "Loss for epoch 88/100: 9.0305e-02\n",
            "Loss for epoch 89/100: 9.0302e-02\n",
            "Loss for epoch 90/100: 9.0299e-02\n",
            "Loss for epoch 91/100: 9.0296e-02\n",
            "Loss for epoch 92/100: 9.0293e-02\n",
            "Loss for epoch 93/100: 9.0291e-02\n",
            "Loss for epoch 94/100: 9.0288e-02\n",
            "Loss for epoch 95/100: 9.0285e-02\n",
            "Loss for epoch 96/100: 9.0282e-02\n",
            "Loss for epoch 97/100: 9.0279e-02\n",
            "Loss for epoch 98/100: 9.0277e-02\n",
            "Loss for epoch 99/100: 9.0274e-02\n",
            "Loss for epoch 100/100: 9.0271e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 10}: 0.1012\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 9.0066e-02\n",
            "Loss for epoch 2/100: 9.0062e-02\n",
            "Loss for epoch 3/100: 9.0059e-02\n",
            "Loss for epoch 4/100: 9.0055e-02\n",
            "Loss for epoch 5/100: 9.0051e-02\n",
            "Loss for epoch 6/100: 9.0047e-02\n",
            "Loss for epoch 7/100: 9.0043e-02\n",
            "Loss for epoch 8/100: 9.0040e-02\n",
            "Loss for epoch 9/100: 9.0036e-02\n",
            "Loss for epoch 10/100: 9.0032e-02\n",
            "Loss for epoch 11/100: 9.0028e-02\n",
            "Loss for epoch 12/100: 9.0025e-02\n",
            "Loss for epoch 13/100: 9.0021e-02\n",
            "Loss for epoch 14/100: 9.0017e-02\n",
            "Loss for epoch 15/100: 9.0013e-02\n",
            "Loss for epoch 16/100: 9.0009e-02\n",
            "Loss for epoch 17/100: 9.0006e-02\n",
            "Loss for epoch 18/100: 9.0002e-02\n",
            "Loss for epoch 19/100: 8.9998e-02\n",
            "Loss for epoch 20/100: 8.9994e-02\n",
            "Loss for epoch 21/100: 8.9991e-02\n",
            "Loss for epoch 22/100: 8.9987e-02\n",
            "Loss for epoch 23/100: 8.9983e-02\n",
            "Loss for epoch 24/100: 8.9979e-02\n",
            "Loss for epoch 25/100: 8.9975e-02\n",
            "Loss for epoch 26/100: 8.9972e-02\n",
            "Loss for epoch 27/100: 8.9968e-02\n",
            "Loss for epoch 28/100: 8.9964e-02\n",
            "Loss for epoch 29/100: 8.9960e-02\n",
            "Loss for epoch 30/100: 8.9957e-02\n",
            "Loss for epoch 31/100: 8.9953e-02\n",
            "Loss for epoch 32/100: 8.9949e-02\n",
            "Loss for epoch 33/100: 8.9945e-02\n",
            "Loss for epoch 34/100: 8.9941e-02\n",
            "Loss for epoch 35/100: 8.9938e-02\n",
            "Loss for epoch 36/100: 8.9934e-02\n",
            "Loss for epoch 37/100: 8.9930e-02\n",
            "Loss for epoch 38/100: 8.9926e-02\n",
            "Loss for epoch 39/100: 8.9922e-02\n",
            "Loss for epoch 40/100: 8.9919e-02\n",
            "Loss for epoch 41/100: 8.9915e-02\n",
            "Loss for epoch 42/100: 8.9911e-02\n",
            "Loss for epoch 43/100: 8.9907e-02\n",
            "Loss for epoch 44/100: 8.9903e-02\n",
            "Loss for epoch 45/100: 8.9900e-02\n",
            "Loss for epoch 46/100: 8.9896e-02\n",
            "Loss for epoch 47/100: 8.9892e-02\n",
            "Loss for epoch 48/100: 8.9888e-02\n",
            "Loss for epoch 49/100: 8.9884e-02\n",
            "Loss for epoch 50/100: 8.9881e-02\n",
            "Loss for epoch 51/100: 8.9877e-02\n",
            "Loss for epoch 52/100: 8.9873e-02\n",
            "Loss for epoch 53/100: 8.9869e-02\n",
            "Loss for epoch 54/100: 8.9865e-02\n",
            "Loss for epoch 55/100: 8.9862e-02\n",
            "Loss for epoch 56/100: 8.9858e-02\n",
            "Loss for epoch 57/100: 8.9854e-02\n",
            "Loss for epoch 58/100: 8.9850e-02\n",
            "Loss for epoch 59/100: 8.9846e-02\n",
            "Loss for epoch 60/100: 8.9843e-02\n",
            "Loss for epoch 61/100: 8.9839e-02\n",
            "Loss for epoch 62/100: 8.9835e-02\n",
            "Loss for epoch 63/100: 8.9831e-02\n",
            "Loss for epoch 64/100: 8.9827e-02\n",
            "Loss for epoch 65/100: 8.9824e-02\n",
            "Loss for epoch 66/100: 8.9820e-02\n",
            "Loss for epoch 67/100: 8.9816e-02\n",
            "Loss for epoch 68/100: 8.9812e-02\n",
            "Loss for epoch 69/100: 8.9808e-02\n",
            "Loss for epoch 70/100: 8.9805e-02\n",
            "Loss for epoch 71/100: 8.9801e-02\n",
            "Loss for epoch 72/100: 8.9797e-02\n",
            "Loss for epoch 73/100: 8.9793e-02\n",
            "Loss for epoch 74/100: 8.9789e-02\n",
            "Loss for epoch 75/100: 8.9786e-02\n",
            "Loss for epoch 76/100: 8.9782e-02\n",
            "Loss for epoch 77/100: 8.9778e-02\n",
            "Loss for epoch 78/100: 8.9774e-02\n",
            "Loss for epoch 79/100: 8.9770e-02\n",
            "Loss for epoch 80/100: 8.9766e-02\n",
            "Loss for epoch 81/100: 8.9763e-02\n",
            "Loss for epoch 82/100: 8.9759e-02\n",
            "Loss for epoch 83/100: 8.9755e-02\n",
            "Loss for epoch 84/100: 8.9751e-02\n",
            "Loss for epoch 85/100: 8.9747e-02\n",
            "Loss for epoch 86/100: 8.9743e-02\n",
            "Loss for epoch 87/100: 8.9740e-02\n",
            "Loss for epoch 88/100: 8.9736e-02\n",
            "Loss for epoch 89/100: 8.9732e-02\n",
            "Loss for epoch 90/100: 8.9728e-02\n",
            "Loss for epoch 91/100: 8.9724e-02\n",
            "Loss for epoch 92/100: 8.9720e-02\n",
            "Loss for epoch 93/100: 8.9717e-02\n",
            "Loss for epoch 94/100: 8.9713e-02\n",
            "Loss for epoch 95/100: 8.9709e-02\n",
            "Loss for epoch 96/100: 8.9705e-02\n",
            "Loss for epoch 97/100: 8.9701e-02\n",
            "Loss for epoch 98/100: 8.9697e-02\n",
            "Loss for epoch 99/100: 8.9694e-02\n",
            "Loss for epoch 100/100: 8.9690e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 50}: 0.1269\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 9.0361e-02\n",
            "Loss for epoch 2/100: 9.0357e-02\n",
            "Loss for epoch 3/100: 9.0354e-02\n",
            "Loss for epoch 4/100: 9.0351e-02\n",
            "Loss for epoch 5/100: 9.0348e-02\n",
            "Loss for epoch 6/100: 9.0345e-02\n",
            "Loss for epoch 7/100: 9.0341e-02\n",
            "Loss for epoch 8/100: 9.0338e-02\n",
            "Loss for epoch 9/100: 9.0335e-02\n",
            "Loss for epoch 10/100: 9.0332e-02\n",
            "Loss for epoch 11/100: 9.0329e-02\n",
            "Loss for epoch 12/100: 9.0325e-02\n",
            "Loss for epoch 13/100: 9.0322e-02\n",
            "Loss for epoch 14/100: 9.0319e-02\n",
            "Loss for epoch 15/100: 9.0316e-02\n",
            "Loss for epoch 16/100: 9.0313e-02\n",
            "Loss for epoch 17/100: 9.0309e-02\n",
            "Loss for epoch 18/100: 9.0306e-02\n",
            "Loss for epoch 19/100: 9.0303e-02\n",
            "Loss for epoch 20/100: 9.0300e-02\n",
            "Loss for epoch 21/100: 9.0297e-02\n",
            "Loss for epoch 22/100: 9.0293e-02\n",
            "Loss for epoch 23/100: 9.0290e-02\n",
            "Loss for epoch 24/100: 9.0287e-02\n",
            "Loss for epoch 25/100: 9.0284e-02\n",
            "Loss for epoch 26/100: 9.0281e-02\n",
            "Loss for epoch 27/100: 9.0277e-02\n",
            "Loss for epoch 28/100: 9.0274e-02\n",
            "Loss for epoch 29/100: 9.0271e-02\n",
            "Loss for epoch 30/100: 9.0268e-02\n",
            "Loss for epoch 31/100: 9.0265e-02\n",
            "Loss for epoch 32/100: 9.0261e-02\n",
            "Loss for epoch 33/100: 9.0258e-02\n",
            "Loss for epoch 34/100: 9.0255e-02\n",
            "Loss for epoch 35/100: 9.0252e-02\n",
            "Loss for epoch 36/100: 9.0249e-02\n",
            "Loss for epoch 37/100: 9.0245e-02\n",
            "Loss for epoch 38/100: 9.0242e-02\n",
            "Loss for epoch 39/100: 9.0239e-02\n",
            "Loss for epoch 40/100: 9.0236e-02\n",
            "Loss for epoch 41/100: 9.0233e-02\n",
            "Loss for epoch 42/100: 9.0229e-02\n",
            "Loss for epoch 43/100: 9.0226e-02\n",
            "Loss for epoch 44/100: 9.0223e-02\n",
            "Loss for epoch 45/100: 9.0220e-02\n",
            "Loss for epoch 46/100: 9.0217e-02\n",
            "Loss for epoch 47/100: 9.0213e-02\n",
            "Loss for epoch 48/100: 9.0210e-02\n",
            "Loss for epoch 49/100: 9.0207e-02\n",
            "Loss for epoch 50/100: 9.0204e-02\n",
            "Loss for epoch 51/100: 9.0201e-02\n",
            "Loss for epoch 52/100: 9.0197e-02\n",
            "Loss for epoch 53/100: 9.0194e-02\n",
            "Loss for epoch 54/100: 9.0191e-02\n",
            "Loss for epoch 55/100: 9.0188e-02\n",
            "Loss for epoch 56/100: 9.0185e-02\n",
            "Loss for epoch 57/100: 9.0181e-02\n",
            "Loss for epoch 58/100: 9.0178e-02\n",
            "Loss for epoch 59/100: 9.0175e-02\n",
            "Loss for epoch 60/100: 9.0172e-02\n",
            "Loss for epoch 61/100: 9.0169e-02\n",
            "Loss for epoch 62/100: 9.0165e-02\n",
            "Loss for epoch 63/100: 9.0162e-02\n",
            "Loss for epoch 64/100: 9.0159e-02\n",
            "Loss for epoch 65/100: 9.0156e-02\n",
            "Loss for epoch 66/100: 9.0152e-02\n",
            "Loss for epoch 67/100: 9.0149e-02\n",
            "Loss for epoch 68/100: 9.0146e-02\n",
            "Loss for epoch 69/100: 9.0143e-02\n",
            "Loss for epoch 70/100: 9.0140e-02\n",
            "Loss for epoch 71/100: 9.0136e-02\n",
            "Loss for epoch 72/100: 9.0133e-02\n",
            "Loss for epoch 73/100: 9.0130e-02\n",
            "Loss for epoch 74/100: 9.0127e-02\n",
            "Loss for epoch 75/100: 9.0124e-02\n",
            "Loss for epoch 76/100: 9.0120e-02\n",
            "Loss for epoch 77/100: 9.0117e-02\n",
            "Loss for epoch 78/100: 9.0114e-02\n",
            "Loss for epoch 79/100: 9.0111e-02\n",
            "Loss for epoch 80/100: 9.0107e-02\n",
            "Loss for epoch 81/100: 9.0104e-02\n",
            "Loss for epoch 82/100: 9.0101e-02\n",
            "Loss for epoch 83/100: 9.0098e-02\n",
            "Loss for epoch 84/100: 9.0095e-02\n",
            "Loss for epoch 85/100: 9.0091e-02\n",
            "Loss for epoch 86/100: 9.0088e-02\n",
            "Loss for epoch 87/100: 9.0085e-02\n",
            "Loss for epoch 88/100: 9.0082e-02\n",
            "Loss for epoch 89/100: 9.0079e-02\n",
            "Loss for epoch 90/100: 9.0075e-02\n",
            "Loss for epoch 91/100: 9.0072e-02\n",
            "Loss for epoch 92/100: 9.0069e-02\n",
            "Loss for epoch 93/100: 9.0066e-02\n",
            "Loss for epoch 94/100: 9.0062e-02\n",
            "Loss for epoch 95/100: 9.0059e-02\n",
            "Loss for epoch 96/100: 9.0056e-02\n",
            "Loss for epoch 97/100: 9.0053e-02\n",
            "Loss for epoch 98/100: 9.0049e-02\n",
            "Loss for epoch 99/100: 9.0046e-02\n",
            "Loss for epoch 100/100: 9.0043e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 100}: 0.1307\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 9.0547e-02\n",
            "Loss for epoch 2/100: 9.0547e-02\n",
            "Loss for epoch 3/100: 9.0547e-02\n",
            "Loss for epoch 4/100: 9.0547e-02\n",
            "Loss for epoch 5/100: 9.0546e-02\n",
            "Loss for epoch 6/100: 9.0546e-02\n",
            "Loss for epoch 7/100: 9.0546e-02\n",
            "Loss for epoch 8/100: 9.0546e-02\n",
            "Loss for epoch 9/100: 9.0545e-02\n",
            "Loss for epoch 10/100: 9.0545e-02\n",
            "Loss for epoch 11/100: 9.0545e-02\n",
            "Loss for epoch 12/100: 9.0544e-02\n",
            "Loss for epoch 13/100: 9.0544e-02\n",
            "Loss for epoch 14/100: 9.0544e-02\n",
            "Loss for epoch 15/100: 9.0544e-02\n",
            "Loss for epoch 16/100: 9.0543e-02\n",
            "Loss for epoch 17/100: 9.0543e-02\n",
            "Loss for epoch 18/100: 9.0543e-02\n",
            "Loss for epoch 19/100: 9.0543e-02\n",
            "Loss for epoch 20/100: 9.0542e-02\n",
            "Loss for epoch 21/100: 9.0542e-02\n",
            "Loss for epoch 22/100: 9.0542e-02\n",
            "Loss for epoch 23/100: 9.0541e-02\n",
            "Loss for epoch 24/100: 9.0541e-02\n",
            "Loss for epoch 25/100: 9.0541e-02\n",
            "Loss for epoch 26/100: 9.0541e-02\n",
            "Loss for epoch 27/100: 9.0540e-02\n",
            "Loss for epoch 28/100: 9.0540e-02\n",
            "Loss for epoch 29/100: 9.0540e-02\n",
            "Loss for epoch 30/100: 9.0540e-02\n",
            "Loss for epoch 31/100: 9.0539e-02\n",
            "Loss for epoch 32/100: 9.0539e-02\n",
            "Loss for epoch 33/100: 9.0539e-02\n",
            "Loss for epoch 34/100: 9.0538e-02\n",
            "Loss for epoch 35/100: 9.0538e-02\n",
            "Loss for epoch 36/100: 9.0538e-02\n",
            "Loss for epoch 37/100: 9.0538e-02\n",
            "Loss for epoch 38/100: 9.0537e-02\n",
            "Loss for epoch 39/100: 9.0537e-02\n",
            "Loss for epoch 40/100: 9.0537e-02\n",
            "Loss for epoch 41/100: 9.0536e-02\n",
            "Loss for epoch 42/100: 9.0536e-02\n",
            "Loss for epoch 43/100: 9.0536e-02\n",
            "Loss for epoch 44/100: 9.0536e-02\n",
            "Loss for epoch 45/100: 9.0535e-02\n",
            "Loss for epoch 46/100: 9.0535e-02\n",
            "Loss for epoch 47/100: 9.0535e-02\n",
            "Loss for epoch 48/100: 9.0535e-02\n",
            "Loss for epoch 49/100: 9.0534e-02\n",
            "Loss for epoch 50/100: 9.0534e-02\n",
            "Loss for epoch 51/100: 9.0534e-02\n",
            "Loss for epoch 52/100: 9.0533e-02\n",
            "Loss for epoch 53/100: 9.0533e-02\n",
            "Loss for epoch 54/100: 9.0533e-02\n",
            "Loss for epoch 55/100: 9.0533e-02\n",
            "Loss for epoch 56/100: 9.0532e-02\n",
            "Loss for epoch 57/100: 9.0532e-02\n",
            "Loss for epoch 58/100: 9.0532e-02\n",
            "Loss for epoch 59/100: 9.0531e-02\n",
            "Loss for epoch 60/100: 9.0531e-02\n",
            "Loss for epoch 61/100: 9.0531e-02\n",
            "Loss for epoch 62/100: 9.0531e-02\n",
            "Loss for epoch 63/100: 9.0530e-02\n",
            "Loss for epoch 64/100: 9.0530e-02\n",
            "Loss for epoch 65/100: 9.0530e-02\n",
            "Loss for epoch 66/100: 9.0530e-02\n",
            "Loss for epoch 67/100: 9.0529e-02\n",
            "Loss for epoch 68/100: 9.0529e-02\n",
            "Loss for epoch 69/100: 9.0529e-02\n",
            "Loss for epoch 70/100: 9.0528e-02\n",
            "Loss for epoch 71/100: 9.0528e-02\n",
            "Loss for epoch 72/100: 9.0528e-02\n",
            "Loss for epoch 73/100: 9.0528e-02\n",
            "Loss for epoch 74/100: 9.0527e-02\n",
            "Loss for epoch 75/100: 9.0527e-02\n",
            "Loss for epoch 76/100: 9.0527e-02\n",
            "Loss for epoch 77/100: 9.0527e-02\n",
            "Loss for epoch 78/100: 9.0526e-02\n",
            "Loss for epoch 79/100: 9.0526e-02\n",
            "Loss for epoch 80/100: 9.0526e-02\n",
            "Loss for epoch 81/100: 9.0525e-02\n",
            "Loss for epoch 82/100: 9.0525e-02\n",
            "Loss for epoch 83/100: 9.0525e-02\n",
            "Loss for epoch 84/100: 9.0525e-02\n",
            "Loss for epoch 85/100: 9.0524e-02\n",
            "Loss for epoch 86/100: 9.0524e-02\n",
            "Loss for epoch 87/100: 9.0524e-02\n",
            "Loss for epoch 88/100: 9.0524e-02\n",
            "Loss for epoch 89/100: 9.0523e-02\n",
            "Loss for epoch 90/100: 9.0523e-02\n",
            "Loss for epoch 91/100: 9.0523e-02\n",
            "Loss for epoch 92/100: 9.0522e-02\n",
            "Loss for epoch 93/100: 9.0522e-02\n",
            "Loss for epoch 94/100: 9.0522e-02\n",
            "Loss for epoch 95/100: 9.0522e-02\n",
            "Loss for epoch 96/100: 9.0521e-02\n",
            "Loss for epoch 97/100: 9.0521e-02\n",
            "Loss for epoch 98/100: 9.0521e-02\n",
            "Loss for epoch 99/100: 9.0520e-02\n",
            "Loss for epoch 100/100: 9.0520e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 10}: 0.0901\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 9.0068e-02\n",
            "Loss for epoch 2/100: 9.0067e-02\n",
            "Loss for epoch 3/100: 9.0067e-02\n",
            "Loss for epoch 4/100: 9.0067e-02\n",
            "Loss for epoch 5/100: 9.0066e-02\n",
            "Loss for epoch 6/100: 9.0066e-02\n",
            "Loss for epoch 7/100: 9.0065e-02\n",
            "Loss for epoch 8/100: 9.0065e-02\n",
            "Loss for epoch 9/100: 9.0065e-02\n",
            "Loss for epoch 10/100: 9.0064e-02\n",
            "Loss for epoch 11/100: 9.0064e-02\n",
            "Loss for epoch 12/100: 9.0064e-02\n",
            "Loss for epoch 13/100: 9.0063e-02\n",
            "Loss for epoch 14/100: 9.0063e-02\n",
            "Loss for epoch 15/100: 9.0062e-02\n",
            "Loss for epoch 16/100: 9.0062e-02\n",
            "Loss for epoch 17/100: 9.0062e-02\n",
            "Loss for epoch 18/100: 9.0061e-02\n",
            "Loss for epoch 19/100: 9.0061e-02\n",
            "Loss for epoch 20/100: 9.0061e-02\n",
            "Loss for epoch 21/100: 9.0060e-02\n",
            "Loss for epoch 22/100: 9.0060e-02\n",
            "Loss for epoch 23/100: 9.0059e-02\n",
            "Loss for epoch 24/100: 9.0059e-02\n",
            "Loss for epoch 25/100: 9.0059e-02\n",
            "Loss for epoch 26/100: 9.0058e-02\n",
            "Loss for epoch 27/100: 9.0058e-02\n",
            "Loss for epoch 28/100: 9.0058e-02\n",
            "Loss for epoch 29/100: 9.0057e-02\n",
            "Loss for epoch 30/100: 9.0057e-02\n",
            "Loss for epoch 31/100: 9.0056e-02\n",
            "Loss for epoch 32/100: 9.0056e-02\n",
            "Loss for epoch 33/100: 9.0056e-02\n",
            "Loss for epoch 34/100: 9.0055e-02\n",
            "Loss for epoch 35/100: 9.0055e-02\n",
            "Loss for epoch 36/100: 9.0055e-02\n",
            "Loss for epoch 37/100: 9.0054e-02\n",
            "Loss for epoch 38/100: 9.0054e-02\n",
            "Loss for epoch 39/100: 9.0053e-02\n",
            "Loss for epoch 40/100: 9.0053e-02\n",
            "Loss for epoch 41/100: 9.0053e-02\n",
            "Loss for epoch 42/100: 9.0052e-02\n",
            "Loss for epoch 43/100: 9.0052e-02\n",
            "Loss for epoch 44/100: 9.0052e-02\n",
            "Loss for epoch 45/100: 9.0051e-02\n",
            "Loss for epoch 46/100: 9.0051e-02\n",
            "Loss for epoch 47/100: 9.0050e-02\n",
            "Loss for epoch 48/100: 9.0050e-02\n",
            "Loss for epoch 49/100: 9.0050e-02\n",
            "Loss for epoch 50/100: 9.0049e-02\n",
            "Loss for epoch 51/100: 9.0049e-02\n",
            "Loss for epoch 52/100: 9.0049e-02\n",
            "Loss for epoch 53/100: 9.0048e-02\n",
            "Loss for epoch 54/100: 9.0048e-02\n",
            "Loss for epoch 55/100: 9.0047e-02\n",
            "Loss for epoch 56/100: 9.0047e-02\n",
            "Loss for epoch 57/100: 9.0047e-02\n",
            "Loss for epoch 58/100: 9.0046e-02\n",
            "Loss for epoch 59/100: 9.0046e-02\n",
            "Loss for epoch 60/100: 9.0046e-02\n",
            "Loss for epoch 61/100: 9.0045e-02\n",
            "Loss for epoch 62/100: 9.0045e-02\n",
            "Loss for epoch 63/100: 9.0044e-02\n",
            "Loss for epoch 64/100: 9.0044e-02\n",
            "Loss for epoch 65/100: 9.0044e-02\n",
            "Loss for epoch 66/100: 9.0043e-02\n",
            "Loss for epoch 67/100: 9.0043e-02\n",
            "Loss for epoch 68/100: 9.0042e-02\n",
            "Loss for epoch 69/100: 9.0042e-02\n",
            "Loss for epoch 70/100: 9.0042e-02\n",
            "Loss for epoch 71/100: 9.0041e-02\n",
            "Loss for epoch 72/100: 9.0041e-02\n",
            "Loss for epoch 73/100: 9.0041e-02\n",
            "Loss for epoch 74/100: 9.0040e-02\n",
            "Loss for epoch 75/100: 9.0040e-02\n",
            "Loss for epoch 76/100: 9.0039e-02\n",
            "Loss for epoch 77/100: 9.0039e-02\n",
            "Loss for epoch 78/100: 9.0039e-02\n",
            "Loss for epoch 79/100: 9.0038e-02\n",
            "Loss for epoch 80/100: 9.0038e-02\n",
            "Loss for epoch 81/100: 9.0038e-02\n",
            "Loss for epoch 82/100: 9.0037e-02\n",
            "Loss for epoch 83/100: 9.0037e-02\n",
            "Loss for epoch 84/100: 9.0036e-02\n",
            "Loss for epoch 85/100: 9.0036e-02\n",
            "Loss for epoch 86/100: 9.0036e-02\n",
            "Loss for epoch 87/100: 9.0035e-02\n",
            "Loss for epoch 88/100: 9.0035e-02\n",
            "Loss for epoch 89/100: 9.0035e-02\n",
            "Loss for epoch 90/100: 9.0034e-02\n",
            "Loss for epoch 91/100: 9.0034e-02\n",
            "Loss for epoch 92/100: 9.0033e-02\n",
            "Loss for epoch 93/100: 9.0033e-02\n",
            "Loss for epoch 94/100: 9.0033e-02\n",
            "Loss for epoch 95/100: 9.0032e-02\n",
            "Loss for epoch 96/100: 9.0032e-02\n",
            "Loss for epoch 97/100: 9.0032e-02\n",
            "Loss for epoch 98/100: 9.0031e-02\n",
            "Loss for epoch 99/100: 9.0031e-02\n",
            "Loss for epoch 100/100: 9.0030e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 50}: 0.1099\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'SGD', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 9.0362e-02\n",
            "Loss for epoch 2/100: 9.0362e-02\n",
            "Loss for epoch 3/100: 9.0361e-02\n",
            "Loss for epoch 4/100: 9.0361e-02\n",
            "Loss for epoch 5/100: 9.0361e-02\n",
            "Loss for epoch 6/100: 9.0360e-02\n",
            "Loss for epoch 7/100: 9.0360e-02\n",
            "Loss for epoch 8/100: 9.0360e-02\n",
            "Loss for epoch 9/100: 9.0359e-02\n",
            "Loss for epoch 10/100: 9.0359e-02\n",
            "Loss for epoch 11/100: 9.0359e-02\n",
            "Loss for epoch 12/100: 9.0358e-02\n",
            "Loss for epoch 13/100: 9.0358e-02\n",
            "Loss for epoch 14/100: 9.0358e-02\n",
            "Loss for epoch 15/100: 9.0358e-02\n",
            "Loss for epoch 16/100: 9.0357e-02\n",
            "Loss for epoch 17/100: 9.0357e-02\n",
            "Loss for epoch 18/100: 9.0357e-02\n",
            "Loss for epoch 19/100: 9.0356e-02\n",
            "Loss for epoch 20/100: 9.0356e-02\n",
            "Loss for epoch 21/100: 9.0356e-02\n",
            "Loss for epoch 22/100: 9.0355e-02\n",
            "Loss for epoch 23/100: 9.0355e-02\n",
            "Loss for epoch 24/100: 9.0355e-02\n",
            "Loss for epoch 25/100: 9.0354e-02\n",
            "Loss for epoch 26/100: 9.0354e-02\n",
            "Loss for epoch 27/100: 9.0354e-02\n",
            "Loss for epoch 28/100: 9.0353e-02\n",
            "Loss for epoch 29/100: 9.0353e-02\n",
            "Loss for epoch 30/100: 9.0353e-02\n",
            "Loss for epoch 31/100: 9.0352e-02\n",
            "Loss for epoch 32/100: 9.0352e-02\n",
            "Loss for epoch 33/100: 9.0352e-02\n",
            "Loss for epoch 34/100: 9.0351e-02\n",
            "Loss for epoch 35/100: 9.0351e-02\n",
            "Loss for epoch 36/100: 9.0351e-02\n",
            "Loss for epoch 37/100: 9.0350e-02\n",
            "Loss for epoch 38/100: 9.0350e-02\n",
            "Loss for epoch 39/100: 9.0350e-02\n",
            "Loss for epoch 40/100: 9.0350e-02\n",
            "Loss for epoch 41/100: 9.0349e-02\n",
            "Loss for epoch 42/100: 9.0349e-02\n",
            "Loss for epoch 43/100: 9.0349e-02\n",
            "Loss for epoch 44/100: 9.0348e-02\n",
            "Loss for epoch 45/100: 9.0348e-02\n",
            "Loss for epoch 46/100: 9.0348e-02\n",
            "Loss for epoch 47/100: 9.0347e-02\n",
            "Loss for epoch 48/100: 9.0347e-02\n",
            "Loss for epoch 49/100: 9.0347e-02\n",
            "Loss for epoch 50/100: 9.0346e-02\n",
            "Loss for epoch 51/100: 9.0346e-02\n",
            "Loss for epoch 52/100: 9.0346e-02\n",
            "Loss for epoch 53/100: 9.0345e-02\n",
            "Loss for epoch 54/100: 9.0345e-02\n",
            "Loss for epoch 55/100: 9.0345e-02\n",
            "Loss for epoch 56/100: 9.0344e-02\n",
            "Loss for epoch 57/100: 9.0344e-02\n",
            "Loss for epoch 58/100: 9.0344e-02\n",
            "Loss for epoch 59/100: 9.0343e-02\n",
            "Loss for epoch 60/100: 9.0343e-02\n",
            "Loss for epoch 61/100: 9.0343e-02\n",
            "Loss for epoch 62/100: 9.0342e-02\n",
            "Loss for epoch 63/100: 9.0342e-02\n",
            "Loss for epoch 64/100: 9.0342e-02\n",
            "Loss for epoch 65/100: 9.0342e-02\n",
            "Loss for epoch 66/100: 9.0341e-02\n",
            "Loss for epoch 67/100: 9.0341e-02\n",
            "Loss for epoch 68/100: 9.0341e-02\n",
            "Loss for epoch 69/100: 9.0340e-02\n",
            "Loss for epoch 70/100: 9.0340e-02\n",
            "Loss for epoch 71/100: 9.0340e-02\n",
            "Loss for epoch 72/100: 9.0339e-02\n",
            "Loss for epoch 73/100: 9.0339e-02\n",
            "Loss for epoch 74/100: 9.0339e-02\n",
            "Loss for epoch 75/100: 9.0338e-02\n",
            "Loss for epoch 76/100: 9.0338e-02\n",
            "Loss for epoch 77/100: 9.0338e-02\n",
            "Loss for epoch 78/100: 9.0337e-02\n",
            "Loss for epoch 79/100: 9.0337e-02\n",
            "Loss for epoch 80/100: 9.0337e-02\n",
            "Loss for epoch 81/100: 9.0336e-02\n",
            "Loss for epoch 82/100: 9.0336e-02\n",
            "Loss for epoch 83/100: 9.0336e-02\n",
            "Loss for epoch 84/100: 9.0335e-02\n",
            "Loss for epoch 85/100: 9.0335e-02\n",
            "Loss for epoch 86/100: 9.0335e-02\n",
            "Loss for epoch 87/100: 9.0334e-02\n",
            "Loss for epoch 88/100: 9.0334e-02\n",
            "Loss for epoch 89/100: 9.0334e-02\n",
            "Loss for epoch 90/100: 9.0334e-02\n",
            "Loss for epoch 91/100: 9.0333e-02\n",
            "Loss for epoch 92/100: 9.0333e-02\n",
            "Loss for epoch 93/100: 9.0333e-02\n",
            "Loss for epoch 94/100: 9.0332e-02\n",
            "Loss for epoch 95/100: 9.0332e-02\n",
            "Loss for epoch 96/100: 9.0332e-02\n",
            "Loss for epoch 97/100: 9.0331e-02\n",
            "Loss for epoch 98/100: 9.0331e-02\n",
            "Loss for epoch 99/100: 9.0331e-02\n",
            "Loss for epoch 100/100: 9.0330e-02\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'SGD', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 100}: 0.0979\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 1.1650e-02\n",
            "Loss for epoch 2/100: 6.7900e-03\n",
            "Loss for epoch 3/100: 5.4827e-03\n",
            "Loss for epoch 4/100: 4.9580e-03\n",
            "Loss for epoch 5/100: 4.3745e-03\n",
            "Loss for epoch 6/100: 4.1999e-03\n",
            "Loss for epoch 7/100: 3.8328e-03\n",
            "Loss for epoch 8/100: 3.7970e-03\n",
            "Loss for epoch 9/100: 3.6815e-03\n",
            "Loss for epoch 10/100: 3.5960e-03\n",
            "Loss for epoch 11/100: 3.4664e-03\n",
            "Loss for epoch 12/100: 3.5105e-03\n",
            "Loss for epoch 13/100: 3.3818e-03\n",
            "Loss for epoch 14/100: 3.4965e-03\n",
            "Loss for epoch 15/100: 3.2730e-03\n",
            "Loss for epoch 16/100: 3.2732e-03\n",
            "Loss for epoch 17/100: 3.2171e-03\n",
            "Loss for epoch 18/100: 3.5340e-03\n",
            "Loss for epoch 19/100: 3.3166e-03\n",
            "Loss for epoch 20/100: 3.3931e-03\n",
            "Loss for epoch 21/100: 3.4096e-03\n",
            "Loss for epoch 22/100: 3.4093e-03\n",
            "Loss for epoch 23/100: 3.6079e-03\n",
            "Loss for epoch 24/100: 3.6153e-03\n",
            "Loss for epoch 25/100: 3.4322e-03\n",
            "Loss for epoch 26/100: 3.7512e-03\n",
            "Loss for epoch 27/100: 3.3540e-03\n",
            "Loss for epoch 28/100: 3.3886e-03\n",
            "Loss for epoch 29/100: 3.4156e-03\n",
            "Loss for epoch 30/100: 3.5518e-03\n",
            "Loss for epoch 31/100: 3.4859e-03\n",
            "Loss for epoch 32/100: 3.7812e-03\n",
            "Loss for epoch 33/100: 3.6045e-03\n",
            "Loss for epoch 34/100: 3.5081e-03\n",
            "Loss for epoch 35/100: 3.8143e-03\n",
            "Loss for epoch 36/100: 3.4319e-03\n",
            "Loss for epoch 37/100: 3.6280e-03\n",
            "Loss for epoch 38/100: 3.8780e-03\n",
            "Loss for epoch 39/100: 4.0901e-03\n",
            "Loss for epoch 40/100: 3.3423e-03\n",
            "Loss for epoch 41/100: 3.4410e-03\n",
            "Loss for epoch 42/100: 3.5383e-03\n",
            "Loss for epoch 43/100: 3.8422e-03\n",
            "Loss for epoch 44/100: 4.0449e-03\n",
            "Loss for epoch 45/100: 3.7284e-03\n",
            "Loss for epoch 46/100: 4.0579e-03\n",
            "Loss for epoch 47/100: 3.6069e-03\n",
            "Loss for epoch 48/100: 3.3895e-03\n",
            "Loss for epoch 49/100: 3.6291e-03\n",
            "Loss for epoch 50/100: 3.3335e-03\n",
            "Loss for epoch 51/100: 3.9580e-03\n",
            "Loss for epoch 52/100: 3.6609e-03\n",
            "Loss for epoch 53/100: 3.6742e-03\n",
            "Loss for epoch 54/100: 3.9111e-03\n",
            "Loss for epoch 55/100: 3.5910e-03\n",
            "Loss for epoch 56/100: 3.7885e-03\n",
            "Loss for epoch 57/100: 3.7791e-03\n",
            "Loss for epoch 58/100: 3.7281e-03\n",
            "Loss for epoch 59/100: 3.7656e-03\n",
            "Loss for epoch 60/100: 3.2522e-03\n",
            "Loss for epoch 61/100: 3.9629e-03\n",
            "Loss for epoch 62/100: 3.4530e-03\n",
            "Loss for epoch 63/100: 3.6446e-03\n",
            "Loss for epoch 64/100: 3.4387e-03\n",
            "Loss for epoch 65/100: 3.6177e-03\n",
            "Loss for epoch 66/100: 3.6713e-03\n",
            "Loss for epoch 67/100: 3.3037e-03\n",
            "Loss for epoch 68/100: 3.7745e-03\n",
            "Loss for epoch 69/100: 3.3418e-03\n",
            "Loss for epoch 70/100: 3.5548e-03\n",
            "Loss for epoch 71/100: 4.0516e-03\n",
            "Loss for epoch 72/100: 4.2635e-03\n",
            "Loss for epoch 73/100: 3.7747e-03\n",
            "Loss for epoch 74/100: 3.6992e-03\n",
            "Loss for epoch 75/100: 4.2262e-03\n",
            "Loss for epoch 76/100: 3.7731e-03\n",
            "Loss for epoch 77/100: 3.4797e-03\n",
            "Loss for epoch 78/100: 3.5691e-03\n",
            "Loss for epoch 79/100: 3.9609e-03\n",
            "Loss for epoch 80/100: 3.7911e-03\n",
            "Loss for epoch 81/100: 4.0278e-03\n",
            "Loss for epoch 82/100: 3.9687e-03\n",
            "Loss for epoch 83/100: 3.5053e-03\n",
            "Loss for epoch 84/100: 3.8404e-03\n",
            "Loss for epoch 85/100: 3.8508e-03\n",
            "Loss for epoch 86/100: 3.7753e-03\n",
            "Loss for epoch 87/100: 3.7098e-03\n",
            "Loss for epoch 88/100: 3.8592e-03\n",
            "Loss for epoch 89/100: 3.8705e-03\n",
            "Loss for epoch 90/100: 3.9103e-03\n",
            "Loss for epoch 91/100: 3.8487e-03\n",
            "Loss for epoch 92/100: 3.6987e-03\n",
            "Loss for epoch 93/100: 3.6480e-03\n",
            "Loss for epoch 94/100: 4.0248e-03\n",
            "Loss for epoch 95/100: 3.8971e-03\n",
            "Loss for epoch 96/100: 4.4339e-03\n",
            "Loss for epoch 97/100: 3.8563e-03\n",
            "Loss for epoch 98/100: 3.3138e-03\n",
            "Loss for epoch 99/100: 3.6862e-03\n",
            "Loss for epoch 100/100: 3.7445e-03\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10, 'num_neurons': 10}: 0.9620\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 1.0960e-02\n",
            "Loss for epoch 2/100: 6.6492e-03\n",
            "Loss for epoch 3/100: 5.8339e-03\n",
            "Loss for epoch 4/100: 5.4601e-03\n",
            "Loss for epoch 5/100: 5.1271e-03\n",
            "Loss for epoch 6/100: 4.8451e-03\n",
            "Loss for epoch 7/100: 5.0061e-03\n",
            "Loss for epoch 8/100: 4.9684e-03\n",
            "Loss for epoch 9/100: 4.8252e-03\n",
            "Loss for epoch 10/100: 5.0647e-03\n",
            "Loss for epoch 11/100: 5.0051e-03\n",
            "Loss for epoch 12/100: 4.5255e-03\n",
            "Loss for epoch 13/100: 4.9388e-03\n",
            "Loss for epoch 14/100: 4.9151e-03\n",
            "Loss for epoch 15/100: 5.0748e-03\n",
            "Loss for epoch 16/100: 5.0745e-03\n",
            "Loss for epoch 17/100: 5.6822e-03\n",
            "Loss for epoch 18/100: 5.0219e-03\n",
            "Loss for epoch 19/100: 5.4781e-03\n",
            "Loss for epoch 20/100: 5.6334e-03\n",
            "Loss for epoch 21/100: 5.7238e-03\n",
            "Loss for epoch 22/100: 5.6265e-03\n",
            "Loss for epoch 23/100: 5.2668e-03\n",
            "Loss for epoch 24/100: 5.6023e-03\n",
            "Loss for epoch 25/100: 5.7964e-03\n",
            "Loss for epoch 26/100: 5.3265e-03\n",
            "Loss for epoch 27/100: 5.9110e-03\n",
            "Loss for epoch 28/100: 5.7905e-03\n",
            "Loss for epoch 29/100: 5.9488e-03\n",
            "Loss for epoch 30/100: 6.5031e-03\n",
            "Loss for epoch 31/100: 5.8840e-03\n",
            "Loss for epoch 32/100: 6.0541e-03\n",
            "Loss for epoch 33/100: 5.9133e-03\n",
            "Loss for epoch 34/100: 6.6150e-03\n",
            "Loss for epoch 35/100: 5.5593e-03\n",
            "Loss for epoch 36/100: 6.7520e-03\n",
            "Loss for epoch 37/100: 6.0686e-03\n",
            "Loss for epoch 38/100: 6.4918e-03\n",
            "Loss for epoch 39/100: 6.1239e-03\n",
            "Loss for epoch 40/100: 6.1712e-03\n",
            "Loss for epoch 41/100: 6.0297e-03\n",
            "Loss for epoch 42/100: 6.0114e-03\n",
            "Loss for epoch 43/100: 6.6775e-03\n",
            "Loss for epoch 44/100: 5.6764e-03\n",
            "Loss for epoch 45/100: 6.8173e-03\n",
            "Loss for epoch 46/100: 6.3840e-03\n",
            "Loss for epoch 47/100: 6.2871e-03\n",
            "Loss for epoch 48/100: 6.6186e-03\n",
            "Loss for epoch 49/100: 7.0466e-03\n",
            "Loss for epoch 50/100: 6.7921e-03\n",
            "Loss for epoch 51/100: 6.5655e-03\n",
            "Loss for epoch 52/100: 6.8033e-03\n",
            "Loss for epoch 53/100: 6.5192e-03\n",
            "Loss for epoch 54/100: 6.9756e-03\n",
            "Loss for epoch 55/100: 6.9253e-03\n",
            "Loss for epoch 56/100: 6.4402e-03\n",
            "Loss for epoch 57/100: 6.0411e-03\n",
            "Loss for epoch 58/100: 6.5077e-03\n",
            "Loss for epoch 59/100: 6.6652e-03\n",
            "Loss for epoch 60/100: 6.4228e-03\n",
            "Loss for epoch 61/100: 6.7542e-03\n",
            "Loss for epoch 62/100: 6.3405e-03\n",
            "Loss for epoch 63/100: 6.6941e-03\n",
            "Loss for epoch 64/100: 6.3290e-03\n",
            "Loss for epoch 65/100: 6.6851e-03\n",
            "Loss for epoch 66/100: 8.0079e-03\n",
            "Loss for epoch 67/100: 6.7138e-03\n",
            "Loss for epoch 68/100: 6.2235e-03\n",
            "Loss for epoch 69/100: 5.6549e-03\n",
            "Loss for epoch 70/100: 7.0267e-03\n",
            "Loss for epoch 71/100: 6.1854e-03\n",
            "Loss for epoch 72/100: 6.4733e-03\n",
            "Loss for epoch 73/100: 6.0091e-03\n",
            "Loss for epoch 74/100: 7.3150e-03\n",
            "Loss for epoch 75/100: 7.3409e-03\n",
            "Loss for epoch 76/100: 6.8182e-03\n",
            "Loss for epoch 77/100: 6.4785e-03\n",
            "Loss for epoch 78/100: 6.9629e-03\n",
            "Loss for epoch 79/100: 6.6920e-03\n",
            "Loss for epoch 80/100: 6.2968e-03\n",
            "Loss for epoch 81/100: 6.7925e-03\n",
            "Loss for epoch 82/100: 6.5057e-03\n",
            "Loss for epoch 83/100: 6.6316e-03\n",
            "Loss for epoch 84/100: 7.4602e-03\n",
            "Loss for epoch 85/100: 7.2940e-03\n",
            "Loss for epoch 86/100: 8.5807e-03\n",
            "Loss for epoch 87/100: 6.2841e-03\n",
            "Loss for epoch 88/100: 6.1985e-03\n",
            "Loss for epoch 89/100: 6.8192e-03\n",
            "Loss for epoch 90/100: 6.8180e-03\n",
            "Loss for epoch 91/100: 6.8608e-03\n",
            "Loss for epoch 92/100: 6.0128e-03\n",
            "Loss for epoch 93/100: 7.7503e-03\n",
            "Loss for epoch 94/100: 6.7760e-03\n",
            "Loss for epoch 95/100: 6.7634e-03\n",
            "Loss for epoch 96/100: 6.8001e-03\n",
            "Loss for epoch 97/100: 6.4236e-03\n",
            "Loss for epoch 98/100: 6.9338e-03\n",
            "Loss for epoch 99/100: 7.3183e-03\n",
            "Loss for epoch 100/100: 7.0152e-03\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10, 'num_neurons': 50}: 0.9536\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 1.1151e-02\n",
            "Loss for epoch 2/100: 7.3181e-03\n",
            "Loss for epoch 3/100: 6.3443e-03\n",
            "Loss for epoch 4/100: 6.0924e-03\n",
            "Loss for epoch 5/100: 6.0131e-03\n",
            "Loss for epoch 6/100: 5.6906e-03\n",
            "Loss for epoch 7/100: 5.9285e-03\n",
            "Loss for epoch 8/100: 6.0296e-03\n",
            "Loss for epoch 9/100: 6.0761e-03\n",
            "Loss for epoch 10/100: 6.1208e-03\n",
            "Loss for epoch 11/100: 6.2021e-03\n",
            "Loss for epoch 12/100: 6.4324e-03\n",
            "Loss for epoch 13/100: 6.7757e-03\n",
            "Loss for epoch 14/100: 6.9131e-03\n",
            "Loss for epoch 15/100: 6.6038e-03\n",
            "Loss for epoch 16/100: 7.3934e-03\n",
            "Loss for epoch 17/100: 6.8417e-03\n",
            "Loss for epoch 18/100: 6.9710e-03\n",
            "Loss for epoch 19/100: 6.7386e-03\n",
            "Loss for epoch 20/100: 6.4426e-03\n",
            "Loss for epoch 21/100: 6.6697e-03\n",
            "Loss for epoch 22/100: 7.1143e-03\n",
            "Loss for epoch 23/100: 6.9567e-03\n",
            "Loss for epoch 24/100: 6.8578e-03\n",
            "Loss for epoch 25/100: 6.6381e-03\n",
            "Loss for epoch 26/100: 7.2469e-03\n",
            "Loss for epoch 27/100: 7.9584e-03\n",
            "Loss for epoch 28/100: 7.2331e-03\n",
            "Loss for epoch 29/100: 7.0941e-03\n",
            "Loss for epoch 30/100: 7.0523e-03\n",
            "Loss for epoch 31/100: 7.5011e-03\n",
            "Loss for epoch 32/100: 7.0573e-03\n",
            "Loss for epoch 33/100: 7.0897e-03\n",
            "Loss for epoch 34/100: 7.5976e-03\n",
            "Loss for epoch 35/100: 8.0296e-03\n",
            "Loss for epoch 36/100: 7.5312e-03\n",
            "Loss for epoch 37/100: 7.1172e-03\n",
            "Loss for epoch 38/100: 7.2453e-03\n",
            "Loss for epoch 39/100: 6.8855e-03\n",
            "Loss for epoch 40/100: 7.1674e-03\n",
            "Loss for epoch 41/100: 7.5824e-03\n",
            "Loss for epoch 42/100: 7.3989e-03\n",
            "Loss for epoch 43/100: 7.7966e-03\n",
            "Loss for epoch 44/100: 8.8061e-03\n",
            "Loss for epoch 45/100: 7.9575e-03\n",
            "Loss for epoch 46/100: 7.2168e-03\n",
            "Loss for epoch 47/100: 7.8609e-03\n",
            "Loss for epoch 48/100: 7.6397e-03\n",
            "Loss for epoch 49/100: 8.0705e-03\n",
            "Loss for epoch 50/100: 8.2771e-03\n",
            "Loss for epoch 51/100: 9.2129e-03\n",
            "Loss for epoch 52/100: 8.4365e-03\n",
            "Loss for epoch 53/100: 8.8323e-03\n",
            "Loss for epoch 54/100: 8.6294e-03\n",
            "Loss for epoch 55/100: 7.3486e-03\n",
            "Loss for epoch 56/100: 8.6363e-03\n",
            "Loss for epoch 57/100: 7.4406e-03\n",
            "Loss for epoch 58/100: 7.5551e-03\n",
            "Loss for epoch 59/100: 7.8461e-03\n",
            "Loss for epoch 60/100: 7.6399e-03\n",
            "Loss for epoch 61/100: 8.5049e-03\n",
            "Loss for epoch 62/100: 8.2058e-03\n",
            "Loss for epoch 63/100: 7.8376e-03\n",
            "Loss for epoch 64/100: 7.8562e-03\n",
            "Loss for epoch 65/100: 8.1863e-03\n",
            "Loss for epoch 66/100: 8.5899e-03\n",
            "Loss for epoch 67/100: 7.8249e-03\n",
            "Loss for epoch 68/100: 8.1548e-03\n",
            "Loss for epoch 69/100: 7.9156e-03\n",
            "Loss for epoch 70/100: 8.9094e-03\n",
            "Loss for epoch 71/100: 8.9980e-03\n",
            "Loss for epoch 72/100: 8.3580e-03\n",
            "Loss for epoch 73/100: 9.6645e-03\n",
            "Loss for epoch 74/100: 8.5282e-03\n",
            "Loss for epoch 75/100: 8.4446e-03\n",
            "Loss for epoch 76/100: 7.4476e-03\n",
            "Loss for epoch 77/100: 8.0529e-03\n",
            "Loss for epoch 78/100: 7.5402e-03\n",
            "Loss for epoch 79/100: 7.9083e-03\n",
            "Loss for epoch 80/100: 6.8937e-03\n",
            "Loss for epoch 81/100: 7.0836e-03\n",
            "Loss for epoch 82/100: 9.5063e-03\n",
            "Loss for epoch 83/100: 8.2626e-03\n",
            "Loss for epoch 84/100: 7.1224e-03\n",
            "Loss for epoch 85/100: 8.0045e-03\n",
            "Loss for epoch 86/100: 7.9233e-03\n",
            "Loss for epoch 87/100: 8.5274e-03\n",
            "Loss for epoch 88/100: 8.4695e-03\n",
            "Loss for epoch 89/100: 7.4247e-03\n",
            "Loss for epoch 90/100: 8.3177e-03\n",
            "Loss for epoch 91/100: 8.0312e-03\n",
            "Loss for epoch 92/100: 7.6680e-03\n",
            "Loss for epoch 93/100: 7.7136e-03\n",
            "Loss for epoch 94/100: 7.6393e-03\n",
            "Loss for epoch 95/100: 7.7909e-03\n",
            "Loss for epoch 96/100: 7.8311e-03\n",
            "Loss for epoch 97/100: 7.6491e-03\n",
            "Loss for epoch 98/100: 8.1308e-03\n",
            "Loss for epoch 99/100: 7.9280e-03\n",
            "Loss for epoch 100/100: 8.6774e-03\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10, 'num_neurons': 100}: 0.9471\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 1.7576e-02\n",
            "Loss for epoch 2/100: 6.7278e-03\n",
            "Loss for epoch 3/100: 4.9349e-03\n",
            "Loss for epoch 4/100: 3.9312e-03\n",
            "Loss for epoch 5/100: 3.2032e-03\n",
            "Loss for epoch 6/100: 2.7379e-03\n",
            "Loss for epoch 7/100: 2.3299e-03\n",
            "Loss for epoch 8/100: 1.9741e-03\n",
            "Loss for epoch 9/100: 1.9157e-03\n",
            "Loss for epoch 10/100: 1.6190e-03\n",
            "Loss for epoch 11/100: 1.5135e-03\n",
            "Loss for epoch 12/100: 1.4040e-03\n",
            "Loss for epoch 13/100: 1.2577e-03\n",
            "Loss for epoch 14/100: 1.2216e-03\n",
            "Loss for epoch 15/100: 1.1044e-03\n",
            "Loss for epoch 16/100: 1.0451e-03\n",
            "Loss for epoch 17/100: 9.9531e-04\n",
            "Loss for epoch 18/100: 1.0110e-03\n",
            "Loss for epoch 19/100: 9.9064e-04\n",
            "Loss for epoch 20/100: 9.6727e-04\n",
            "Loss for epoch 21/100: 9.1494e-04\n",
            "Loss for epoch 22/100: 8.7019e-04\n",
            "Loss for epoch 23/100: 9.4307e-04\n",
            "Loss for epoch 24/100: 9.5600e-04\n",
            "Loss for epoch 25/100: 9.3134e-04\n",
            "Loss for epoch 26/100: 8.3330e-04\n",
            "Loss for epoch 27/100: 8.5364e-04\n",
            "Loss for epoch 28/100: 8.6907e-04\n",
            "Loss for epoch 29/100: 8.1490e-04\n",
            "Loss for epoch 30/100: 8.8697e-04\n",
            "Loss for epoch 31/100: 7.1351e-04\n",
            "Loss for epoch 32/100: 8.2585e-04\n",
            "Loss for epoch 33/100: 8.5448e-04\n",
            "Loss for epoch 34/100: 8.4694e-04\n",
            "Loss for epoch 35/100: 7.3952e-04\n",
            "Loss for epoch 36/100: 8.2513e-04\n",
            "Loss for epoch 37/100: 7.4215e-04\n",
            "Loss for epoch 38/100: 8.6558e-04\n",
            "Loss for epoch 39/100: 8.9479e-04\n",
            "Loss for epoch 40/100: 8.0501e-04\n",
            "Loss for epoch 41/100: 7.3604e-04\n",
            "Loss for epoch 42/100: 8.1035e-04\n",
            "Loss for epoch 43/100: 8.6868e-04\n",
            "Loss for epoch 44/100: 7.6213e-04\n",
            "Loss for epoch 45/100: 6.9542e-04\n",
            "Loss for epoch 46/100: 8.8464e-04\n",
            "Loss for epoch 47/100: 8.2642e-04\n",
            "Loss for epoch 48/100: 7.6411e-04\n",
            "Loss for epoch 49/100: 8.1348e-04\n",
            "Loss for epoch 50/100: 7.7033e-04\n",
            "Loss for epoch 51/100: 7.6402e-04\n",
            "Loss for epoch 52/100: 7.6464e-04\n",
            "Loss for epoch 53/100: 8.4184e-04\n",
            "Loss for epoch 54/100: 7.5029e-04\n",
            "Loss for epoch 55/100: 7.1906e-04\n",
            "Loss for epoch 56/100: 7.5728e-04\n",
            "Loss for epoch 57/100: 7.6464e-04\n",
            "Loss for epoch 58/100: 7.5219e-04\n",
            "Loss for epoch 59/100: 7.6985e-04\n",
            "Loss for epoch 60/100: 7.7853e-04\n",
            "Loss for epoch 61/100: 7.1319e-04\n",
            "Loss for epoch 62/100: 6.9028e-04\n",
            "Loss for epoch 63/100: 8.6485e-04\n",
            "Loss for epoch 64/100: 7.0808e-04\n",
            "Loss for epoch 65/100: 6.6739e-04\n",
            "Loss for epoch 66/100: 8.0049e-04\n",
            "Loss for epoch 67/100: 7.4878e-04\n",
            "Loss for epoch 68/100: 8.2444e-04\n",
            "Loss for epoch 69/100: 7.6151e-04\n",
            "Loss for epoch 70/100: 8.0684e-04\n",
            "Loss for epoch 71/100: 8.0337e-04\n",
            "Loss for epoch 72/100: 7.6857e-04\n",
            "Loss for epoch 73/100: 6.8050e-04\n",
            "Loss for epoch 74/100: 7.4566e-04\n",
            "Loss for epoch 75/100: 6.8436e-04\n",
            "Loss for epoch 76/100: 7.6607e-04\n",
            "Loss for epoch 77/100: 7.6886e-04\n",
            "Loss for epoch 78/100: 8.5693e-04\n",
            "Loss for epoch 79/100: 9.7105e-04\n",
            "Loss for epoch 80/100: 7.6573e-04\n",
            "Loss for epoch 81/100: 7.0019e-04\n",
            "Loss for epoch 82/100: 7.3709e-04\n",
            "Loss for epoch 83/100: 7.0446e-04\n",
            "Loss for epoch 84/100: 6.9275e-04\n",
            "Loss for epoch 85/100: 7.3251e-04\n",
            "Loss for epoch 86/100: 7.7608e-04\n",
            "Loss for epoch 87/100: 7.5125e-04\n",
            "Loss for epoch 88/100: 6.8315e-04\n",
            "Loss for epoch 89/100: 7.9488e-04\n",
            "Loss for epoch 90/100: 7.8837e-04\n",
            "Loss for epoch 91/100: 7.6968e-04\n",
            "Loss for epoch 92/100: 8.0947e-04\n",
            "Loss for epoch 93/100: 9.1225e-04\n",
            "Loss for epoch 94/100: 7.9641e-04\n",
            "Loss for epoch 95/100: 7.5652e-04\n",
            "Loss for epoch 96/100: 6.5927e-04\n",
            "Loss for epoch 97/100: 6.8635e-04\n",
            "Loss for epoch 98/100: 6.6642e-04\n",
            "Loss for epoch 99/100: 6.7537e-04\n",
            "Loss for epoch 100/100: 7.3006e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 10}: 0.9742\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 1.3372e-02\n",
            "Loss for epoch 2/100: 5.8272e-03\n",
            "Loss for epoch 3/100: 4.3192e-03\n",
            "Loss for epoch 4/100: 3.4463e-03\n",
            "Loss for epoch 5/100: 2.8358e-03\n",
            "Loss for epoch 6/100: 2.5219e-03\n",
            "Loss for epoch 7/100: 2.2351e-03\n",
            "Loss for epoch 8/100: 2.1127e-03\n",
            "Loss for epoch 9/100: 1.8427e-03\n",
            "Loss for epoch 10/100: 1.7016e-03\n",
            "Loss for epoch 11/100: 1.6012e-03\n",
            "Loss for epoch 12/100: 1.4653e-03\n",
            "Loss for epoch 13/100: 1.4261e-03\n",
            "Loss for epoch 14/100: 1.3869e-03\n",
            "Loss for epoch 15/100: 1.4103e-03\n",
            "Loss for epoch 16/100: 1.2660e-03\n",
            "Loss for epoch 17/100: 1.2375e-03\n",
            "Loss for epoch 18/100: 1.2364e-03\n",
            "Loss for epoch 19/100: 1.2033e-03\n",
            "Loss for epoch 20/100: 1.1257e-03\n",
            "Loss for epoch 21/100: 1.1832e-03\n",
            "Loss for epoch 22/100: 1.2105e-03\n",
            "Loss for epoch 23/100: 1.1987e-03\n",
            "Loss for epoch 24/100: 1.1131e-03\n",
            "Loss for epoch 25/100: 1.3112e-03\n",
            "Loss for epoch 26/100: 1.0905e-03\n",
            "Loss for epoch 27/100: 9.6732e-04\n",
            "Loss for epoch 28/100: 1.1440e-03\n",
            "Loss for epoch 29/100: 1.1790e-03\n",
            "Loss for epoch 30/100: 9.4418e-04\n",
            "Loss for epoch 31/100: 1.3682e-03\n",
            "Loss for epoch 32/100: 9.6326e-04\n",
            "Loss for epoch 33/100: 8.6602e-04\n",
            "Loss for epoch 34/100: 1.0038e-03\n",
            "Loss for epoch 35/100: 1.1026e-03\n",
            "Loss for epoch 36/100: 1.0414e-03\n",
            "Loss for epoch 37/100: 8.5251e-04\n",
            "Loss for epoch 38/100: 9.8513e-04\n",
            "Loss for epoch 39/100: 1.0323e-03\n",
            "Loss for epoch 40/100: 1.0309e-03\n",
            "Loss for epoch 41/100: 9.9985e-04\n",
            "Loss for epoch 42/100: 8.9563e-04\n",
            "Loss for epoch 43/100: 1.0633e-03\n",
            "Loss for epoch 44/100: 1.0249e-03\n",
            "Loss for epoch 45/100: 9.3580e-04\n",
            "Loss for epoch 46/100: 9.5866e-04\n",
            "Loss for epoch 47/100: 8.8049e-04\n",
            "Loss for epoch 48/100: 9.5834e-04\n",
            "Loss for epoch 49/100: 8.5009e-04\n",
            "Loss for epoch 50/100: 9.7953e-04\n",
            "Loss for epoch 51/100: 9.2864e-04\n",
            "Loss for epoch 52/100: 9.9135e-04\n",
            "Loss for epoch 53/100: 9.8951e-04\n",
            "Loss for epoch 54/100: 8.7318e-04\n",
            "Loss for epoch 55/100: 1.0406e-03\n",
            "Loss for epoch 56/100: 9.2592e-04\n",
            "Loss for epoch 57/100: 8.5513e-04\n",
            "Loss for epoch 58/100: 1.3781e-03\n",
            "Loss for epoch 59/100: 1.1621e-03\n",
            "Loss for epoch 60/100: 1.0244e-03\n",
            "Loss for epoch 61/100: 1.0833e-03\n",
            "Loss for epoch 62/100: 9.6598e-04\n",
            "Loss for epoch 63/100: 8.9239e-04\n",
            "Loss for epoch 64/100: 1.0275e-03\n",
            "Loss for epoch 65/100: 1.1033e-03\n",
            "Loss for epoch 66/100: 8.9705e-04\n",
            "Loss for epoch 67/100: 9.5596e-04\n",
            "Loss for epoch 68/100: 9.5954e-04\n",
            "Loss for epoch 69/100: 9.5786e-04\n",
            "Loss for epoch 70/100: 9.6187e-04\n",
            "Loss for epoch 71/100: 9.0254e-04\n",
            "Loss for epoch 72/100: 1.0477e-03\n",
            "Loss for epoch 73/100: 8.2377e-04\n",
            "Loss for epoch 74/100: 8.9610e-04\n",
            "Loss for epoch 75/100: 1.0860e-03\n",
            "Loss for epoch 76/100: 9.7599e-04\n",
            "Loss for epoch 77/100: 9.3317e-04\n",
            "Loss for epoch 78/100: 9.3784e-04\n",
            "Loss for epoch 79/100: 8.5036e-04\n",
            "Loss for epoch 80/100: 9.4853e-04\n",
            "Loss for epoch 81/100: 1.1727e-03\n",
            "Loss for epoch 82/100: 9.1733e-04\n",
            "Loss for epoch 83/100: 1.0427e-03\n",
            "Loss for epoch 84/100: 1.1655e-03\n",
            "Loss for epoch 85/100: 1.2348e-03\n",
            "Loss for epoch 86/100: 1.1700e-03\n",
            "Loss for epoch 87/100: 9.4218e-04\n",
            "Loss for epoch 88/100: 9.5352e-04\n",
            "Loss for epoch 89/100: 1.2080e-03\n",
            "Loss for epoch 90/100: 1.1629e-03\n",
            "Loss for epoch 91/100: 9.4902e-04\n",
            "Loss for epoch 92/100: 8.3326e-04\n",
            "Loss for epoch 93/100: 1.1464e-03\n",
            "Loss for epoch 94/100: 1.1953e-03\n",
            "Loss for epoch 95/100: 1.0771e-03\n",
            "Loss for epoch 96/100: 1.2292e-03\n",
            "Loss for epoch 97/100: 9.4792e-04\n",
            "Loss for epoch 98/100: 9.3065e-04\n",
            "Loss for epoch 99/100: 9.5838e-04\n",
            "Loss for epoch 100/100: 8.9277e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 50}: 0.9736\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 1.3073e-02\n",
            "Loss for epoch 2/100: 5.7451e-03\n",
            "Loss for epoch 3/100: 4.2826e-03\n",
            "Loss for epoch 4/100: 3.5066e-03\n",
            "Loss for epoch 5/100: 2.9565e-03\n",
            "Loss for epoch 6/100: 2.5766e-03\n",
            "Loss for epoch 7/100: 2.2850e-03\n",
            "Loss for epoch 8/100: 2.1065e-03\n",
            "Loss for epoch 9/100: 1.9171e-03\n",
            "Loss for epoch 10/100: 1.8194e-03\n",
            "Loss for epoch 11/100: 1.7499e-03\n",
            "Loss for epoch 12/100: 1.5166e-03\n",
            "Loss for epoch 13/100: 1.5482e-03\n",
            "Loss for epoch 14/100: 1.4157e-03\n",
            "Loss for epoch 15/100: 1.4084e-03\n",
            "Loss for epoch 16/100: 1.4652e-03\n",
            "Loss for epoch 17/100: 1.4718e-03\n",
            "Loss for epoch 18/100: 1.2447e-03\n",
            "Loss for epoch 19/100: 1.1616e-03\n",
            "Loss for epoch 20/100: 1.2127e-03\n",
            "Loss for epoch 21/100: 1.3707e-03\n",
            "Loss for epoch 22/100: 1.3606e-03\n",
            "Loss for epoch 23/100: 1.2670e-03\n",
            "Loss for epoch 24/100: 1.4823e-03\n",
            "Loss for epoch 25/100: 1.2047e-03\n",
            "Loss for epoch 26/100: 1.2548e-03\n",
            "Loss for epoch 27/100: 1.2346e-03\n",
            "Loss for epoch 28/100: 1.0731e-03\n",
            "Loss for epoch 29/100: 1.1400e-03\n",
            "Loss for epoch 30/100: 1.1488e-03\n",
            "Loss for epoch 31/100: 1.2627e-03\n",
            "Loss for epoch 32/100: 1.2575e-03\n",
            "Loss for epoch 33/100: 1.2008e-03\n",
            "Loss for epoch 34/100: 1.3799e-03\n",
            "Loss for epoch 35/100: 1.0810e-03\n",
            "Loss for epoch 36/100: 1.0935e-03\n",
            "Loss for epoch 37/100: 1.0871e-03\n",
            "Loss for epoch 38/100: 1.1864e-03\n",
            "Loss for epoch 39/100: 1.2400e-03\n",
            "Loss for epoch 40/100: 1.2480e-03\n",
            "Loss for epoch 41/100: 1.3076e-03\n",
            "Loss for epoch 42/100: 1.1155e-03\n",
            "Loss for epoch 43/100: 1.2165e-03\n",
            "Loss for epoch 44/100: 1.2044e-03\n",
            "Loss for epoch 45/100: 1.0158e-03\n",
            "Loss for epoch 46/100: 1.1055e-03\n",
            "Loss for epoch 47/100: 9.4933e-04\n",
            "Loss for epoch 48/100: 1.3103e-03\n",
            "Loss for epoch 49/100: 1.3233e-03\n",
            "Loss for epoch 50/100: 1.2982e-03\n",
            "Loss for epoch 51/100: 1.1379e-03\n",
            "Loss for epoch 52/100: 1.1433e-03\n",
            "Loss for epoch 53/100: 1.2211e-03\n",
            "Loss for epoch 54/100: 1.1765e-03\n",
            "Loss for epoch 55/100: 1.1093e-03\n",
            "Loss for epoch 56/100: 1.0351e-03\n",
            "Loss for epoch 57/100: 1.1298e-03\n",
            "Loss for epoch 58/100: 1.0664e-03\n",
            "Loss for epoch 59/100: 1.2129e-03\n",
            "Loss for epoch 60/100: 1.3274e-03\n",
            "Loss for epoch 61/100: 1.1936e-03\n",
            "Loss for epoch 62/100: 1.4021e-03\n",
            "Loss for epoch 63/100: 1.2908e-03\n",
            "Loss for epoch 64/100: 1.2726e-03\n",
            "Loss for epoch 65/100: 1.5831e-03\n",
            "Loss for epoch 66/100: 1.4214e-03\n",
            "Loss for epoch 67/100: 1.3924e-03\n",
            "Loss for epoch 68/100: 1.0980e-03\n",
            "Loss for epoch 69/100: 1.0525e-03\n",
            "Loss for epoch 70/100: 1.1416e-03\n",
            "Loss for epoch 71/100: 1.1162e-03\n",
            "Loss for epoch 72/100: 1.0248e-03\n",
            "Loss for epoch 73/100: 1.0512e-03\n",
            "Loss for epoch 74/100: 1.1069e-03\n",
            "Loss for epoch 75/100: 1.0813e-03\n",
            "Loss for epoch 76/100: 1.2083e-03\n",
            "Loss for epoch 77/100: 1.3471e-03\n",
            "Loss for epoch 78/100: 1.2692e-03\n",
            "Loss for epoch 79/100: 1.1744e-03\n",
            "Loss for epoch 80/100: 1.2368e-03\n",
            "Loss for epoch 81/100: 1.3744e-03\n",
            "Loss for epoch 82/100: 1.4081e-03\n",
            "Loss for epoch 83/100: 1.2757e-03\n",
            "Loss for epoch 84/100: 1.2171e-03\n",
            "Loss for epoch 85/100: 1.0470e-03\n",
            "Loss for epoch 86/100: 1.2241e-03\n",
            "Loss for epoch 87/100: 1.3056e-03\n",
            "Loss for epoch 88/100: 1.2319e-03\n",
            "Loss for epoch 89/100: 1.3596e-03\n",
            "Loss for epoch 90/100: 1.3229e-03\n",
            "Loss for epoch 91/100: 1.2204e-03\n",
            "Loss for epoch 92/100: 1.2235e-03\n",
            "Loss for epoch 93/100: 1.1891e-03\n",
            "Loss for epoch 94/100: 1.3635e-03\n",
            "Loss for epoch 95/100: 1.3689e-03\n",
            "Loss for epoch 96/100: 1.2558e-03\n",
            "Loss for epoch 97/100: 1.1604e-03\n",
            "Loss for epoch 98/100: 1.5265e-03\n",
            "Loss for epoch 99/100: 1.3434e-03\n",
            "Loss for epoch 100/100: 1.3396e-03\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 100}: 0.9736\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 5.4833e-02\n",
            "Loss for epoch 2/100: 1.7182e-02\n",
            "Loss for epoch 3/100: 1.0764e-02\n",
            "Loss for epoch 4/100: 8.4712e-03\n",
            "Loss for epoch 5/100: 7.0768e-03\n",
            "Loss for epoch 6/100: 6.1022e-03\n",
            "Loss for epoch 7/100: 5.3524e-03\n",
            "Loss for epoch 8/100: 4.7592e-03\n",
            "Loss for epoch 9/100: 4.2291e-03\n",
            "Loss for epoch 10/100: 3.8227e-03\n",
            "Loss for epoch 11/100: 3.4336e-03\n",
            "Loss for epoch 12/100: 3.1256e-03\n",
            "Loss for epoch 13/100: 2.8680e-03\n",
            "Loss for epoch 14/100: 2.6189e-03\n",
            "Loss for epoch 15/100: 2.4227e-03\n",
            "Loss for epoch 16/100: 2.2402e-03\n",
            "Loss for epoch 17/100: 2.0543e-03\n",
            "Loss for epoch 18/100: 1.9010e-03\n",
            "Loss for epoch 19/100: 1.7656e-03\n",
            "Loss for epoch 20/100: 1.6455e-03\n",
            "Loss for epoch 21/100: 1.5231e-03\n",
            "Loss for epoch 22/100: 1.4533e-03\n",
            "Loss for epoch 23/100: 1.3606e-03\n",
            "Loss for epoch 24/100: 1.2808e-03\n",
            "Loss for epoch 25/100: 1.2089e-03\n",
            "Loss for epoch 26/100: 1.1526e-03\n",
            "Loss for epoch 27/100: 1.1108e-03\n",
            "Loss for epoch 28/100: 1.0416e-03\n",
            "Loss for epoch 29/100: 9.8727e-04\n",
            "Loss for epoch 30/100: 9.5458e-04\n",
            "Loss for epoch 31/100: 8.9183e-04\n",
            "Loss for epoch 32/100: 8.5654e-04\n",
            "Loss for epoch 33/100: 8.1359e-04\n",
            "Loss for epoch 34/100: 7.8028e-04\n",
            "Loss for epoch 35/100: 7.3196e-04\n",
            "Loss for epoch 36/100: 6.9408e-04\n",
            "Loss for epoch 37/100: 6.7082e-04\n",
            "Loss for epoch 38/100: 6.7128e-04\n",
            "Loss for epoch 39/100: 6.5881e-04\n",
            "Loss for epoch 40/100: 6.2878e-04\n",
            "Loss for epoch 41/100: 6.0488e-04\n",
            "Loss for epoch 42/100: 5.9396e-04\n",
            "Loss for epoch 43/100: 5.7399e-04\n",
            "Loss for epoch 44/100: 5.5710e-04\n",
            "Loss for epoch 45/100: 5.4721e-04\n",
            "Loss for epoch 46/100: 5.3698e-04\n",
            "Loss for epoch 47/100: 5.4431e-04\n",
            "Loss for epoch 48/100: 5.3599e-04\n",
            "Loss for epoch 49/100: 5.3842e-04\n",
            "Loss for epoch 50/100: 5.3059e-04\n",
            "Loss for epoch 51/100: 5.4295e-04\n",
            "Loss for epoch 52/100: 5.4681e-04\n",
            "Loss for epoch 53/100: 5.3327e-04\n",
            "Loss for epoch 54/100: 5.2290e-04\n",
            "Loss for epoch 55/100: 5.1750e-04\n",
            "Loss for epoch 56/100: 5.3462e-04\n",
            "Loss for epoch 57/100: 5.3707e-04\n",
            "Loss for epoch 58/100: 5.1435e-04\n",
            "Loss for epoch 59/100: 5.0613e-04\n",
            "Loss for epoch 60/100: 5.2318e-04\n",
            "Loss for epoch 61/100: 5.4784e-04\n",
            "Loss for epoch 62/100: 5.6079e-04\n",
            "Loss for epoch 63/100: 6.0595e-04\n",
            "Loss for epoch 64/100: 6.0858e-04\n",
            "Loss for epoch 65/100: 5.8546e-04\n",
            "Loss for epoch 66/100: 6.1685e-04\n",
            "Loss for epoch 67/100: 5.7012e-04\n",
            "Loss for epoch 68/100: 5.3976e-04\n",
            "Loss for epoch 69/100: 5.2718e-04\n",
            "Loss for epoch 70/100: 5.0689e-04\n",
            "Loss for epoch 71/100: 4.6003e-04\n",
            "Loss for epoch 72/100: 4.5265e-04\n",
            "Loss for epoch 73/100: 4.5202e-04\n",
            "Loss for epoch 74/100: 4.5081e-04\n",
            "Loss for epoch 75/100: 4.4947e-04\n",
            "Loss for epoch 76/100: 4.4645e-04\n",
            "Loss for epoch 77/100: 4.4340e-04\n",
            "Loss for epoch 78/100: 4.4323e-04\n",
            "Loss for epoch 79/100: 4.4294e-04\n",
            "Loss for epoch 80/100: 4.4196e-04\n",
            "Loss for epoch 81/100: 4.4484e-04\n",
            "Loss for epoch 82/100: 4.3880e-04\n",
            "Loss for epoch 83/100: 4.4771e-04\n",
            "Loss for epoch 84/100: 4.4109e-04\n",
            "Loss for epoch 85/100: 4.4224e-04\n",
            "Loss for epoch 86/100: 4.3910e-04\n",
            "Loss for epoch 87/100: 4.4070e-04\n",
            "Loss for epoch 88/100: 4.3826e-04\n",
            "Loss for epoch 89/100: 4.4132e-04\n",
            "Loss for epoch 90/100: 4.3822e-04\n",
            "Loss for epoch 91/100: 4.3797e-04\n",
            "Loss for epoch 92/100: 4.4096e-04\n",
            "Loss for epoch 93/100: 4.4429e-04\n",
            "Loss for epoch 94/100: 4.6686e-04\n",
            "Loss for epoch 95/100: 5.7137e-04\n",
            "Loss for epoch 96/100: 7.3121e-04\n",
            "Loss for epoch 97/100: 8.5258e-04\n",
            "Loss for epoch 98/100: 8.2409e-04\n",
            "Loss for epoch 99/100: 6.9017e-04\n",
            "Loss for epoch 100/100: 5.8436e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 10}: 0.9712\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 3.8311e-02\n",
            "Loss for epoch 2/100: 1.0905e-02\n",
            "Loss for epoch 3/100: 7.6924e-03\n",
            "Loss for epoch 4/100: 6.1571e-03\n",
            "Loss for epoch 5/100: 5.1289e-03\n",
            "Loss for epoch 6/100: 4.3263e-03\n",
            "Loss for epoch 7/100: 3.6807e-03\n",
            "Loss for epoch 8/100: 3.2174e-03\n",
            "Loss for epoch 9/100: 2.8009e-03\n",
            "Loss for epoch 10/100: 2.4876e-03\n",
            "Loss for epoch 11/100: 2.1694e-03\n",
            "Loss for epoch 12/100: 1.9098e-03\n",
            "Loss for epoch 13/100: 1.7591e-03\n",
            "Loss for epoch 14/100: 1.5637e-03\n",
            "Loss for epoch 15/100: 1.4160e-03\n",
            "Loss for epoch 16/100: 1.2626e-03\n",
            "Loss for epoch 17/100: 1.1340e-03\n",
            "Loss for epoch 18/100: 1.0473e-03\n",
            "Loss for epoch 19/100: 9.7086e-04\n",
            "Loss for epoch 20/100: 8.8456e-04\n",
            "Loss for epoch 21/100: 8.1943e-04\n",
            "Loss for epoch 22/100: 7.6751e-04\n",
            "Loss for epoch 23/100: 7.4204e-04\n",
            "Loss for epoch 24/100: 6.8559e-04\n",
            "Loss for epoch 25/100: 6.6237e-04\n",
            "Loss for epoch 26/100: 6.4269e-04\n",
            "Loss for epoch 27/100: 6.3485e-04\n",
            "Loss for epoch 28/100: 5.9393e-04\n",
            "Loss for epoch 29/100: 5.4754e-04\n",
            "Loss for epoch 30/100: 5.1128e-04\n",
            "Loss for epoch 31/100: 4.9701e-04\n",
            "Loss for epoch 32/100: 4.7844e-04\n",
            "Loss for epoch 33/100: 4.7345e-04\n",
            "Loss for epoch 34/100: 4.8646e-04\n",
            "Loss for epoch 35/100: 4.6007e-04\n",
            "Loss for epoch 36/100: 4.5330e-04\n",
            "Loss for epoch 37/100: 4.3791e-04\n",
            "Loss for epoch 38/100: 4.2497e-04\n",
            "Loss for epoch 39/100: 4.4873e-04\n",
            "Loss for epoch 40/100: 4.4041e-04\n",
            "Loss for epoch 41/100: 4.1440e-04\n",
            "Loss for epoch 42/100: 3.8658e-04\n",
            "Loss for epoch 43/100: 3.7498e-04\n",
            "Loss for epoch 44/100: 3.7163e-04\n",
            "Loss for epoch 45/100: 3.9934e-04\n",
            "Loss for epoch 46/100: 4.5031e-04\n",
            "Loss for epoch 47/100: 5.5787e-04\n",
            "Loss for epoch 48/100: 7.2297e-04\n",
            "Loss for epoch 49/100: 8.2397e-04\n",
            "Loss for epoch 50/100: 6.9869e-04\n",
            "Loss for epoch 51/100: 6.2483e-04\n",
            "Loss for epoch 52/100: 5.6850e-04\n",
            "Loss for epoch 53/100: 4.5097e-04\n",
            "Loss for epoch 54/100: 4.1488e-04\n",
            "Loss for epoch 55/100: 4.0952e-04\n",
            "Loss for epoch 56/100: 3.9152e-04\n",
            "Loss for epoch 57/100: 4.1005e-04\n",
            "Loss for epoch 58/100: 3.8428e-04\n",
            "Loss for epoch 59/100: 3.8577e-04\n",
            "Loss for epoch 60/100: 3.8869e-04\n",
            "Loss for epoch 61/100: 3.7154e-04\n",
            "Loss for epoch 62/100: 3.7052e-04\n",
            "Loss for epoch 63/100: 3.6604e-04\n",
            "Loss for epoch 64/100: 3.7804e-04\n",
            "Loss for epoch 65/100: 3.7211e-04\n",
            "Loss for epoch 66/100: 3.7326e-04\n",
            "Loss for epoch 67/100: 3.7006e-04\n",
            "Loss for epoch 68/100: 3.6456e-04\n",
            "Loss for epoch 69/100: 3.5926e-04\n",
            "Loss for epoch 70/100: 3.5801e-04\n",
            "Loss for epoch 71/100: 3.5512e-04\n",
            "Loss for epoch 72/100: 3.5626e-04\n",
            "Loss for epoch 73/100: 3.5475e-04\n",
            "Loss for epoch 74/100: 3.4844e-04\n",
            "Loss for epoch 75/100: 3.4925e-04\n",
            "Loss for epoch 76/100: 3.5091e-04\n",
            "Loss for epoch 77/100: 3.5800e-04\n",
            "Loss for epoch 78/100: 3.7836e-04\n",
            "Loss for epoch 79/100: 4.3051e-04\n",
            "Loss for epoch 80/100: 7.1176e-04\n",
            "Loss for epoch 81/100: 8.3626e-04\n",
            "Loss for epoch 82/100: 8.7151e-04\n",
            "Loss for epoch 83/100: 7.0023e-04\n",
            "Loss for epoch 84/100: 6.4228e-04\n",
            "Loss for epoch 85/100: 5.2419e-04\n",
            "Loss for epoch 86/100: 4.7062e-04\n",
            "Loss for epoch 87/100: 4.0720e-04\n",
            "Loss for epoch 88/100: 3.6669e-04\n",
            "Loss for epoch 89/100: 3.4739e-04\n",
            "Loss for epoch 90/100: 3.4771e-04\n",
            "Loss for epoch 91/100: 3.3183e-04\n",
            "Loss for epoch 92/100: 3.2877e-04\n",
            "Loss for epoch 93/100: 3.2749e-04\n",
            "Loss for epoch 94/100: 3.3167e-04\n",
            "Loss for epoch 95/100: 3.3539e-04\n",
            "Loss for epoch 96/100: 3.3374e-04\n",
            "Loss for epoch 97/100: 3.2986e-04\n",
            "Loss for epoch 98/100: 3.2974e-04\n",
            "Loss for epoch 99/100: 3.3289e-04\n",
            "Loss for epoch 100/100: 3.3263e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 50}: 0.9742\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 3.7000e-02\n",
            "Loss for epoch 2/100: 9.8450e-03\n",
            "Loss for epoch 3/100: 7.0185e-03\n",
            "Loss for epoch 4/100: 5.6237e-03\n",
            "Loss for epoch 5/100: 4.6808e-03\n",
            "Loss for epoch 6/100: 3.9299e-03\n",
            "Loss for epoch 7/100: 3.3464e-03\n",
            "Loss for epoch 8/100: 2.9222e-03\n",
            "Loss for epoch 9/100: 2.5754e-03\n",
            "Loss for epoch 10/100: 2.2480e-03\n",
            "Loss for epoch 11/100: 1.9579e-03\n",
            "Loss for epoch 12/100: 1.7013e-03\n",
            "Loss for epoch 13/100: 1.5855e-03\n",
            "Loss for epoch 14/100: 1.4243e-03\n",
            "Loss for epoch 15/100: 1.2894e-03\n",
            "Loss for epoch 16/100: 1.1765e-03\n",
            "Loss for epoch 17/100: 1.0614e-03\n",
            "Loss for epoch 18/100: 9.5505e-04\n",
            "Loss for epoch 19/100: 8.9168e-04\n",
            "Loss for epoch 20/100: 8.1380e-04\n",
            "Loss for epoch 21/100: 7.4548e-04\n",
            "Loss for epoch 22/100: 7.0913e-04\n",
            "Loss for epoch 23/100: 6.7659e-04\n",
            "Loss for epoch 24/100: 6.8581e-04\n",
            "Loss for epoch 25/100: 6.3358e-04\n",
            "Loss for epoch 26/100: 6.1263e-04\n",
            "Loss for epoch 27/100: 5.6499e-04\n",
            "Loss for epoch 28/100: 5.5879e-04\n",
            "Loss for epoch 29/100: 5.9127e-04\n",
            "Loss for epoch 30/100: 5.6174e-04\n",
            "Loss for epoch 31/100: 5.4198e-04\n",
            "Loss for epoch 32/100: 5.0683e-04\n",
            "Loss for epoch 33/100: 5.4826e-04\n",
            "Loss for epoch 34/100: 5.4195e-04\n",
            "Loss for epoch 35/100: 5.2558e-04\n",
            "Loss for epoch 36/100: 5.1203e-04\n",
            "Loss for epoch 37/100: 5.0325e-04\n",
            "Loss for epoch 38/100: 5.6469e-04\n",
            "Loss for epoch 39/100: 6.4628e-04\n",
            "Loss for epoch 40/100: 6.9093e-04\n",
            "Loss for epoch 41/100: 7.7810e-04\n",
            "Loss for epoch 42/100: 6.9203e-04\n",
            "Loss for epoch 43/100: 6.3757e-04\n",
            "Loss for epoch 44/100: 5.6922e-04\n",
            "Loss for epoch 45/100: 5.0001e-04\n",
            "Loss for epoch 46/100: 4.7520e-04\n",
            "Loss for epoch 47/100: 4.5878e-04\n",
            "Loss for epoch 48/100: 4.6140e-04\n",
            "Loss for epoch 49/100: 4.3734e-04\n",
            "Loss for epoch 50/100: 4.3932e-04\n",
            "Loss for epoch 51/100: 4.2656e-04\n",
            "Loss for epoch 52/100: 4.2871e-04\n",
            "Loss for epoch 53/100: 4.3508e-04\n",
            "Loss for epoch 54/100: 4.3092e-04\n",
            "Loss for epoch 55/100: 4.3451e-04\n",
            "Loss for epoch 56/100: 4.2922e-04\n",
            "Loss for epoch 57/100: 4.1705e-04\n",
            "Loss for epoch 58/100: 4.2136e-04\n",
            "Loss for epoch 59/100: 4.3481e-04\n",
            "Loss for epoch 60/100: 4.7468e-04\n",
            "Loss for epoch 61/100: 4.7028e-04\n",
            "Loss for epoch 62/100: 5.2130e-04\n",
            "Loss for epoch 63/100: 7.5477e-04\n",
            "Loss for epoch 64/100: 1.0520e-03\n",
            "Loss for epoch 65/100: 8.8817e-04\n",
            "Loss for epoch 66/100: 7.7915e-04\n",
            "Loss for epoch 67/100: 7.0923e-04\n",
            "Loss for epoch 68/100: 7.2036e-04\n",
            "Loss for epoch 69/100: 6.5079e-04\n",
            "Loss for epoch 70/100: 6.0795e-04\n",
            "Loss for epoch 71/100: 4.9287e-04\n",
            "Loss for epoch 72/100: 4.9080e-04\n",
            "Loss for epoch 73/100: 4.5281e-04\n",
            "Loss for epoch 74/100: 4.5113e-04\n",
            "Loss for epoch 75/100: 4.4283e-04\n",
            "Loss for epoch 76/100: 4.4055e-04\n",
            "Loss for epoch 77/100: 4.3614e-04\n",
            "Loss for epoch 78/100: 4.3168e-04\n",
            "Loss for epoch 79/100: 4.3357e-04\n",
            "Loss for epoch 80/100: 4.3467e-04\n",
            "Loss for epoch 81/100: 4.2668e-04\n",
            "Loss for epoch 82/100: 4.2257e-04\n",
            "Loss for epoch 83/100: 4.1692e-04\n",
            "Loss for epoch 84/100: 4.7428e-04\n",
            "Loss for epoch 85/100: 5.6010e-04\n",
            "Loss for epoch 86/100: 5.5829e-04\n",
            "Loss for epoch 87/100: 5.7021e-04\n",
            "Loss for epoch 88/100: 6.7450e-04\n",
            "Loss for epoch 89/100: 7.0316e-04\n",
            "Loss for epoch 90/100: 6.2223e-04\n",
            "Loss for epoch 91/100: 5.8178e-04\n",
            "Loss for epoch 92/100: 5.9401e-04\n",
            "Loss for epoch 93/100: 5.4754e-04\n",
            "Loss for epoch 94/100: 5.8198e-04\n",
            "Loss for epoch 95/100: 5.2166e-04\n",
            "Loss for epoch 96/100: 5.1221e-04\n",
            "Loss for epoch 97/100: 5.2748e-04\n",
            "Loss for epoch 98/100: 5.1337e-04\n",
            "Loss for epoch 99/100: 4.7607e-04\n",
            "Loss for epoch 100/100: 4.7207e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 1000, 'num_neurons': 100}: 0.9739\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 10}\n",
            "Loss for epoch 1/100: 8.7750e-02\n",
            "Loss for epoch 2/100: 7.8662e-02\n",
            "Loss for epoch 3/100: 6.9243e-02\n",
            "Loss for epoch 4/100: 6.0838e-02\n",
            "Loss for epoch 5/100: 5.2953e-02\n",
            "Loss for epoch 6/100: 4.6252e-02\n",
            "Loss for epoch 7/100: 4.0172e-02\n",
            "Loss for epoch 8/100: 3.4642e-02\n",
            "Loss for epoch 9/100: 2.9922e-02\n",
            "Loss for epoch 10/100: 2.6182e-02\n",
            "Loss for epoch 11/100: 2.2894e-02\n",
            "Loss for epoch 12/100: 1.9892e-02\n",
            "Loss for epoch 13/100: 1.7512e-02\n",
            "Loss for epoch 14/100: 1.5820e-02\n",
            "Loss for epoch 15/100: 1.4526e-02\n",
            "Loss for epoch 16/100: 1.3506e-02\n",
            "Loss for epoch 17/100: 1.2673e-02\n",
            "Loss for epoch 18/100: 1.1954e-02\n",
            "Loss for epoch 19/100: 1.1338e-02\n",
            "Loss for epoch 20/100: 1.0793e-02\n",
            "Loss for epoch 21/100: 1.0303e-02\n",
            "Loss for epoch 22/100: 9.8607e-03\n",
            "Loss for epoch 23/100: 9.4621e-03\n",
            "Loss for epoch 24/100: 9.0960e-03\n",
            "Loss for epoch 25/100: 8.7584e-03\n",
            "Loss for epoch 26/100: 8.4492e-03\n",
            "Loss for epoch 27/100: 8.1507e-03\n",
            "Loss for epoch 28/100: 7.8790e-03\n",
            "Loss for epoch 29/100: 7.6230e-03\n",
            "Loss for epoch 30/100: 7.3865e-03\n",
            "Loss for epoch 31/100: 7.1598e-03\n",
            "Loss for epoch 32/100: 6.9456e-03\n",
            "Loss for epoch 33/100: 6.7457e-03\n",
            "Loss for epoch 34/100: 6.5576e-03\n",
            "Loss for epoch 35/100: 6.3775e-03\n",
            "Loss for epoch 36/100: 6.2093e-03\n",
            "Loss for epoch 37/100: 6.0442e-03\n",
            "Loss for epoch 38/100: 5.8893e-03\n",
            "Loss for epoch 39/100: 5.7401e-03\n",
            "Loss for epoch 40/100: 5.5994e-03\n",
            "Loss for epoch 41/100: 5.4646e-03\n",
            "Loss for epoch 42/100: 5.3330e-03\n",
            "Loss for epoch 43/100: 5.2018e-03\n",
            "Loss for epoch 44/100: 5.0806e-03\n",
            "Loss for epoch 45/100: 4.9664e-03\n",
            "Loss for epoch 46/100: 4.8542e-03\n",
            "Loss for epoch 47/100: 4.7460e-03\n",
            "Loss for epoch 48/100: 4.6438e-03\n",
            "Loss for epoch 49/100: 4.5408e-03\n",
            "Loss for epoch 50/100: 4.4385e-03\n",
            "Loss for epoch 51/100: 4.3434e-03\n",
            "Loss for epoch 52/100: 4.2490e-03\n",
            "Loss for epoch 53/100: 4.1593e-03\n",
            "Loss for epoch 54/100: 4.0707e-03\n",
            "Loss for epoch 55/100: 3.9860e-03\n",
            "Loss for epoch 56/100: 3.9047e-03\n",
            "Loss for epoch 57/100: 3.8216e-03\n",
            "Loss for epoch 58/100: 3.7432e-03\n",
            "Loss for epoch 59/100: 3.6615e-03\n",
            "Loss for epoch 60/100: 3.5920e-03\n",
            "Loss for epoch 61/100: 3.5176e-03\n",
            "Loss for epoch 62/100: 3.4400e-03\n",
            "Loss for epoch 63/100: 3.3724e-03\n",
            "Loss for epoch 64/100: 3.3008e-03\n",
            "Loss for epoch 65/100: 3.2344e-03\n",
            "Loss for epoch 66/100: 3.1744e-03\n",
            "Loss for epoch 67/100: 3.1039e-03\n",
            "Loss for epoch 68/100: 3.0455e-03\n",
            "Loss for epoch 69/100: 2.9890e-03\n",
            "Loss for epoch 70/100: 2.9271e-03\n",
            "Loss for epoch 71/100: 2.8733e-03\n",
            "Loss for epoch 72/100: 2.8137e-03\n",
            "Loss for epoch 73/100: 2.7599e-03\n",
            "Loss for epoch 74/100: 2.7061e-03\n",
            "Loss for epoch 75/100: 2.6545e-03\n",
            "Loss for epoch 76/100: 2.6110e-03\n",
            "Loss for epoch 77/100: 2.5570e-03\n",
            "Loss for epoch 78/100: 2.5059e-03\n",
            "Loss for epoch 79/100: 2.4548e-03\n",
            "Loss for epoch 80/100: 2.4121e-03\n",
            "Loss for epoch 81/100: 2.3645e-03\n",
            "Loss for epoch 82/100: 2.3215e-03\n",
            "Loss for epoch 83/100: 2.2775e-03\n",
            "Loss for epoch 84/100: 2.2341e-03\n",
            "Loss for epoch 85/100: 2.1949e-03\n",
            "Loss for epoch 86/100: 2.1557e-03\n",
            "Loss for epoch 87/100: 2.1175e-03\n",
            "Loss for epoch 88/100: 2.0821e-03\n",
            "Loss for epoch 89/100: 2.0474e-03\n",
            "Loss for epoch 90/100: 2.0058e-03\n",
            "Loss for epoch 91/100: 1.9712e-03\n",
            "Loss for epoch 92/100: 1.9363e-03\n",
            "Loss for epoch 93/100: 1.9041e-03\n",
            "Loss for epoch 94/100: 1.8730e-03\n",
            "Loss for epoch 95/100: 1.8392e-03\n",
            "Loss for epoch 96/100: 1.8082e-03\n",
            "Loss for epoch 97/100: 1.7813e-03\n",
            "Loss for epoch 98/100: 1.7539e-03\n",
            "Loss for epoch 99/100: 1.7216e-03\n",
            "Loss for epoch 100/100: 1.6952e-03\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 10}: 0.9662\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 50}\n",
            "Loss for epoch 1/100: 8.6278e-02\n",
            "Loss for epoch 2/100: 7.2251e-02\n",
            "Loss for epoch 3/100: 5.2213e-02\n",
            "Loss for epoch 4/100: 3.8325e-02\n",
            "Loss for epoch 5/100: 2.9829e-02\n",
            "Loss for epoch 6/100: 2.3152e-02\n",
            "Loss for epoch 7/100: 1.9207e-02\n",
            "Loss for epoch 8/100: 1.6310e-02\n",
            "Loss for epoch 9/100: 1.4327e-02\n",
            "Loss for epoch 10/100: 1.2934e-02\n",
            "Loss for epoch 11/100: 1.1833e-02\n",
            "Loss for epoch 12/100: 1.0945e-02\n",
            "Loss for epoch 13/100: 1.0198e-02\n",
            "Loss for epoch 14/100: 9.5762e-03\n",
            "Loss for epoch 15/100: 9.0151e-03\n",
            "Loss for epoch 16/100: 8.5312e-03\n",
            "Loss for epoch 17/100: 8.0945e-03\n",
            "Loss for epoch 18/100: 7.7044e-03\n",
            "Loss for epoch 19/100: 7.3482e-03\n",
            "Loss for epoch 20/100: 7.0272e-03\n",
            "Loss for epoch 21/100: 6.7282e-03\n",
            "Loss for epoch 22/100: 6.4523e-03\n",
            "Loss for epoch 23/100: 6.2078e-03\n",
            "Loss for epoch 24/100: 5.9701e-03\n",
            "Loss for epoch 25/100: 5.7603e-03\n",
            "Loss for epoch 26/100: 5.5478e-03\n",
            "Loss for epoch 27/100: 5.3502e-03\n",
            "Loss for epoch 28/100: 5.1633e-03\n",
            "Loss for epoch 29/100: 4.9876e-03\n",
            "Loss for epoch 30/100: 4.8261e-03\n",
            "Loss for epoch 31/100: 4.6561e-03\n",
            "Loss for epoch 32/100: 4.4981e-03\n",
            "Loss for epoch 33/100: 4.3501e-03\n",
            "Loss for epoch 34/100: 4.2069e-03\n",
            "Loss for epoch 35/100: 4.0701e-03\n",
            "Loss for epoch 36/100: 3.9383e-03\n",
            "Loss for epoch 37/100: 3.8061e-03\n",
            "Loss for epoch 38/100: 3.6929e-03\n",
            "Loss for epoch 39/100: 3.5723e-03\n",
            "Loss for epoch 40/100: 3.4547e-03\n",
            "Loss for epoch 41/100: 3.3478e-03\n",
            "Loss for epoch 42/100: 3.2469e-03\n",
            "Loss for epoch 43/100: 3.1487e-03\n",
            "Loss for epoch 44/100: 3.0577e-03\n",
            "Loss for epoch 45/100: 2.9621e-03\n",
            "Loss for epoch 46/100: 2.8803e-03\n",
            "Loss for epoch 47/100: 2.8012e-03\n",
            "Loss for epoch 48/100: 2.7158e-03\n",
            "Loss for epoch 49/100: 2.6322e-03\n",
            "Loss for epoch 50/100: 2.5591e-03\n",
            "Loss for epoch 51/100: 2.4865e-03\n",
            "Loss for epoch 52/100: 2.4141e-03\n",
            "Loss for epoch 53/100: 2.3537e-03\n",
            "Loss for epoch 54/100: 2.2923e-03\n",
            "Loss for epoch 55/100: 2.2263e-03\n",
            "Loss for epoch 56/100: 2.1665e-03\n",
            "Loss for epoch 57/100: 2.1130e-03\n",
            "Loss for epoch 58/100: 2.0620e-03\n",
            "Loss for epoch 59/100: 2.0106e-03\n",
            "Loss for epoch 60/100: 1.9595e-03\n",
            "Loss for epoch 61/100: 1.9095e-03\n",
            "Loss for epoch 62/100: 1.8651e-03\n",
            "Loss for epoch 63/100: 1.8173e-03\n",
            "Loss for epoch 64/100: 1.7733e-03\n",
            "Loss for epoch 65/100: 1.7316e-03\n",
            "Loss for epoch 66/100: 1.6882e-03\n",
            "Loss for epoch 67/100: 1.6469e-03\n",
            "Loss for epoch 68/100: 1.6070e-03\n",
            "Loss for epoch 69/100: 1.5726e-03\n",
            "Loss for epoch 70/100: 1.5341e-03\n",
            "Loss for epoch 71/100: 1.5025e-03\n",
            "Loss for epoch 72/100: 1.4712e-03\n",
            "Loss for epoch 73/100: 1.4349e-03\n",
            "Loss for epoch 74/100: 1.4000e-03\n",
            "Loss for epoch 75/100: 1.3712e-03\n",
            "Loss for epoch 76/100: 1.3439e-03\n",
            "Loss for epoch 77/100: 1.3149e-03\n",
            "Loss for epoch 78/100: 1.2914e-03\n",
            "Loss for epoch 79/100: 1.2604e-03\n",
            "Loss for epoch 80/100: 1.2317e-03\n",
            "Loss for epoch 81/100: 1.2044e-03\n",
            "Loss for epoch 82/100: 1.1774e-03\n",
            "Loss for epoch 83/100: 1.1559e-03\n",
            "Loss for epoch 84/100: 1.1339e-03\n",
            "Loss for epoch 85/100: 1.1110e-03\n",
            "Loss for epoch 86/100: 1.0915e-03\n",
            "Loss for epoch 87/100: 1.0720e-03\n",
            "Loss for epoch 88/100: 1.0553e-03\n",
            "Loss for epoch 89/100: 1.0380e-03\n",
            "Loss for epoch 90/100: 1.0182e-03\n",
            "Loss for epoch 91/100: 1.0007e-03\n",
            "Loss for epoch 92/100: 9.8267e-04\n",
            "Loss for epoch 93/100: 9.6774e-04\n",
            "Loss for epoch 94/100: 9.5362e-04\n",
            "Loss for epoch 95/100: 9.3470e-04\n",
            "Loss for epoch 96/100: 9.2085e-04\n",
            "Loss for epoch 97/100: 9.0445e-04\n",
            "Loss for epoch 98/100: 8.8850e-04\n",
            "Loss for epoch 99/100: 8.7344e-04\n",
            "Loss for epoch 100/100: 8.6124e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 50}: 0.9714\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "Testing combination: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 100}\n",
            "Loss for epoch 1/100: 8.6716e-02\n",
            "Loss for epoch 2/100: 7.3345e-02\n",
            "Loss for epoch 3/100: 5.5286e-02\n",
            "Loss for epoch 4/100: 3.7609e-02\n",
            "Loss for epoch 5/100: 2.6076e-02\n",
            "Loss for epoch 6/100: 1.9357e-02\n",
            "Loss for epoch 7/100: 1.5825e-02\n",
            "Loss for epoch 8/100: 1.3718e-02\n",
            "Loss for epoch 9/100: 1.2282e-02\n",
            "Loss for epoch 10/100: 1.1192e-02\n",
            "Loss for epoch 11/100: 1.0293e-02\n",
            "Loss for epoch 12/100: 9.5385e-03\n",
            "Loss for epoch 13/100: 8.8932e-03\n",
            "Loss for epoch 14/100: 8.3569e-03\n",
            "Loss for epoch 15/100: 7.8773e-03\n",
            "Loss for epoch 16/100: 7.4650e-03\n",
            "Loss for epoch 17/100: 7.0892e-03\n",
            "Loss for epoch 18/100: 6.7548e-03\n",
            "Loss for epoch 19/100: 6.4435e-03\n",
            "Loss for epoch 20/100: 6.1630e-03\n",
            "Loss for epoch 21/100: 5.9019e-03\n",
            "Loss for epoch 22/100: 5.6589e-03\n",
            "Loss for epoch 23/100: 5.4388e-03\n",
            "Loss for epoch 24/100: 5.2222e-03\n",
            "Loss for epoch 25/100: 5.0281e-03\n",
            "Loss for epoch 26/100: 4.8298e-03\n",
            "Loss for epoch 27/100: 4.6424e-03\n",
            "Loss for epoch 28/100: 4.4763e-03\n",
            "Loss for epoch 29/100: 4.3125e-03\n",
            "Loss for epoch 30/100: 4.1659e-03\n",
            "Loss for epoch 31/100: 4.0112e-03\n",
            "Loss for epoch 32/100: 3.8705e-03\n",
            "Loss for epoch 33/100: 3.7380e-03\n",
            "Loss for epoch 34/100: 3.6113e-03\n",
            "Loss for epoch 35/100: 3.4875e-03\n",
            "Loss for epoch 36/100: 3.3709e-03\n",
            "Loss for epoch 37/100: 3.2559e-03\n",
            "Loss for epoch 38/100: 3.1642e-03\n",
            "Loss for epoch 39/100: 3.0582e-03\n",
            "Loss for epoch 40/100: 2.9566e-03\n",
            "Loss for epoch 41/100: 2.8614e-03\n",
            "Loss for epoch 42/100: 2.7764e-03\n",
            "Loss for epoch 43/100: 2.6924e-03\n",
            "Loss for epoch 44/100: 2.6186e-03\n",
            "Loss for epoch 45/100: 2.5365e-03\n",
            "Loss for epoch 46/100: 2.4627e-03\n",
            "Loss for epoch 47/100: 2.3951e-03\n",
            "Loss for epoch 48/100: 2.3211e-03\n",
            "Loss for epoch 49/100: 2.2484e-03\n",
            "Loss for epoch 50/100: 2.1880e-03\n",
            "Loss for epoch 51/100: 2.1272e-03\n",
            "Loss for epoch 52/100: 2.0624e-03\n",
            "Loss for epoch 53/100: 2.0024e-03\n",
            "Loss for epoch 54/100: 1.9476e-03\n",
            "Loss for epoch 55/100: 1.8962e-03\n",
            "Loss for epoch 56/100: 1.8412e-03\n",
            "Loss for epoch 57/100: 1.7945e-03\n",
            "Loss for epoch 58/100: 1.7471e-03\n",
            "Loss for epoch 59/100: 1.7045e-03\n",
            "Loss for epoch 60/100: 1.6532e-03\n",
            "Loss for epoch 61/100: 1.6065e-03\n",
            "Loss for epoch 62/100: 1.5628e-03\n",
            "Loss for epoch 63/100: 1.5221e-03\n",
            "Loss for epoch 64/100: 1.4830e-03\n",
            "Loss for epoch 65/100: 1.4460e-03\n",
            "Loss for epoch 66/100: 1.4120e-03\n",
            "Loss for epoch 67/100: 1.3777e-03\n",
            "Loss for epoch 68/100: 1.3418e-03\n",
            "Loss for epoch 69/100: 1.3119e-03\n",
            "Loss for epoch 70/100: 1.2774e-03\n",
            "Loss for epoch 71/100: 1.2482e-03\n",
            "Loss for epoch 72/100: 1.2235e-03\n",
            "Loss for epoch 73/100: 1.1929e-03\n",
            "Loss for epoch 74/100: 1.1650e-03\n",
            "Loss for epoch 75/100: 1.1388e-03\n",
            "Loss for epoch 76/100: 1.1110e-03\n",
            "Loss for epoch 77/100: 1.0875e-03\n",
            "Loss for epoch 78/100: 1.0658e-03\n",
            "Loss for epoch 79/100: 1.0433e-03\n",
            "Loss for epoch 80/100: 1.0180e-03\n",
            "Loss for epoch 81/100: 9.9705e-04\n",
            "Loss for epoch 82/100: 9.7649e-04\n",
            "Loss for epoch 83/100: 9.5690e-04\n",
            "Loss for epoch 84/100: 9.3904e-04\n",
            "Loss for epoch 85/100: 9.2101e-04\n",
            "Loss for epoch 86/100: 9.0587e-04\n",
            "Loss for epoch 87/100: 8.8950e-04\n",
            "Loss for epoch 88/100: 8.7652e-04\n",
            "Loss for epoch 89/100: 8.6145e-04\n",
            "Loss for epoch 90/100: 8.4669e-04\n",
            "Loss for epoch 91/100: 8.3038e-04\n",
            "Loss for epoch 92/100: 8.1629e-04\n",
            "Loss for epoch 93/100: 8.0256e-04\n",
            "Loss for epoch 94/100: 7.9015e-04\n",
            "Loss for epoch 95/100: 7.7866e-04\n",
            "Loss for epoch 96/100: 7.6779e-04\n",
            "Loss for epoch 97/100: 7.5572e-04\n",
            "Loss for epoch 98/100: 7.4572e-04\n",
            "Loss for epoch 99/100: 7.3506e-04\n",
            "Loss for epoch 100/100: 7.2490e-04\n",
            "----------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------\n",
            "The accuracy of the model with varying hyperparameters {'opt': 'ADAM', 'epochs': 100, 'batch_size': 10000, 'num_neurons': 100}: 0.9709\n",
            "----------------------------------------------------\n",
            "NEXT MODEL\n",
            "----------------------------------------------------\n",
            "All accuracies {'optSGD_batch10_neurons10': 0.9214, 'optSGD_batch10_neurons50': 0.9315, 'optSGD_batch10_neurons100': 0.9344, 'optSGD_batch100_neurons10': 0.3056, 'optSGD_batch100_neurons50': 0.5047, 'optSGD_batch100_neurons100': 0.3407, 'optSGD_batch1000_neurons10': 0.1012, 'optSGD_batch1000_neurons50': 0.1269, 'optSGD_batch1000_neurons100': 0.1307, 'optSGD_batch10000_neurons10': 0.0901, 'optSGD_batch10000_neurons50': 0.1099, 'optSGD_batch10000_neurons100': 0.0979, 'optADAM_batch10_neurons10': 0.962, 'optADAM_batch10_neurons50': 0.9536, 'optADAM_batch10_neurons100': 0.9471, 'optADAM_batch100_neurons10': 0.9742, 'optADAM_batch100_neurons50': 0.9736, 'optADAM_batch100_neurons100': 0.9736, 'optADAM_batch1000_neurons10': 0.9712, 'optADAM_batch1000_neurons50': 0.9742, 'optADAM_batch1000_neurons100': 0.9739, 'optADAM_batch10000_neurons10': 0.9662, 'optADAM_batch10000_neurons50': 0.9714, 'optADAM_batch10000_neurons100': 0.9709}\n",
            "Best Accuracy: 0.9742\n",
            "Best Parameters: {'opt': 'ADAM', 'epochs': 100, 'batch_size': 100, 'num_neurons': 10}\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "param_grid = {\n",
        "    'opt': ['SGD', 'ADAM'],\n",
        "    'epochs': [100],   # how many times do we want to go through the whole data set\n",
        "    'batch_size': [10, 100, 1000, 10000], # how many samples do we process before updating weights\n",
        "    'num_neurons': [10, 50, 100]\n",
        "}\n",
        "\n",
        "# Generate all possible combinations of hyperparameters\n",
        "keys, values = zip(*param_grid.items())\n",
        "param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "best_accuracy = -1\n",
        "best_params = None\n",
        "all_accuracies = {}\n",
        "\n",
        "for params in param_combinations:\n",
        "      print(f\"Testing combination: {params}\")\n",
        "\n",
        "      # define parameters\n",
        "      opt = params['opt']\n",
        "      epochs = params['epochs']\n",
        "      batch_size = params['batch_size']\n",
        "      num_neurons = params['num_neurons']\n",
        "\n",
        "      # running the model\n",
        "      accuracy = evaluate_implementations(x_train, x_test, y_train, y_train_cat, opt, epochs, batch_size, num_neurons)\n",
        "\n",
        "      # store accuracy\n",
        "      all_accuracies[f'opt{opt}_batch{batch_size}_neurons{num_neurons}'] = accuracy\n",
        "\n",
        "      print('----------------------------------------------------')\n",
        "      print('RESULTS')\n",
        "      print('----------------------------------------------------')\n",
        "      print(f'The accuracy of the model with varying hyperparameters {params}: {accuracy:.4f}')\n",
        "      print('----------------------------------------------------')\n",
        "      print('NEXT MODEL')\n",
        "      print('----------------------------------------------------')\n",
        "\n",
        "      # Update best accuracy and best parameters if this is the best so far\n",
        "      if accuracy > best_accuracy:\n",
        "          best_accuracy = accuracy\n",
        "          best_params = params\n",
        "\n",
        "print(\"All accuracies\", all_accuracies)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "print(\"Best Parameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "zVMIwfNuJAZU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVMIwfNuJAZU",
        "outputId": "a93d9db7-339e-4132-bd7b-46e9e9ee2401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9742\n"
          ]
        }
      ],
      "source": [
        "print(all_accuracies.get(\"optADAM_batch100_neurons10\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "QE40w_Mgro7U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE40w_Mgro7U",
        "outputId": "b6a3bd30-2428-47af-cf3b-c8f778cb0c72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'optSGD_batch10_neurons10': 0.9214,\n",
              " 'optSGD_batch10_neurons50': 0.9315,\n",
              " 'optSGD_batch10_neurons100': 0.9344,\n",
              " 'optSGD_batch100_neurons10': 0.3056,\n",
              " 'optSGD_batch100_neurons50': 0.5047,\n",
              " 'optSGD_batch100_neurons100': 0.3407,\n",
              " 'optSGD_batch1000_neurons10': 0.1012,\n",
              " 'optSGD_batch1000_neurons50': 0.1269,\n",
              " 'optSGD_batch1000_neurons100': 0.1307,\n",
              " 'optSGD_batch10000_neurons10': 0.0901,\n",
              " 'optSGD_batch10000_neurons50': 0.1099,\n",
              " 'optSGD_batch10000_neurons100': 0.0979,\n",
              " 'optADAM_batch10_neurons10': 0.962,\n",
              " 'optADAM_batch10_neurons50': 0.9536,\n",
              " 'optADAM_batch10_neurons100': 0.9471,\n",
              " 'optADAM_batch100_neurons10': 0.9742,\n",
              " 'optADAM_batch100_neurons50': 0.9736,\n",
              " 'optADAM_batch100_neurons100': 0.9736,\n",
              " 'optADAM_batch1000_neurons10': 0.9712,\n",
              " 'optADAM_batch1000_neurons50': 0.9742,\n",
              " 'optADAM_batch1000_neurons100': 0.9739,\n",
              " 'optADAM_batch10000_neurons10': 0.9662,\n",
              " 'optADAM_batch10000_neurons50': 0.9714,\n",
              " 'optADAM_batch10000_neurons100': 0.9709}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ALdMtO3eQVZ2",
      "metadata": {
        "id": "ALdMtO3eQVZ2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
