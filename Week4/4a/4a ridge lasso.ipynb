{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d89b57",
   "metadata": {
    "cellIdentifier": "nhfikpgr65s0qpzkbweycr"
   },
   "source": [
    "# Lab: Ridge Regression and the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0febcab",
   "metadata": {
    "cellIdentifier": "f0310hrrq6txaaex064nbl"
   },
   "source": [
    "**Information:**  \n",
    "We are using the book 'G. James et al. -  An Introduction to Statistical Learning (with Applications in Python)'. You can find a copy of it for free [here](https://www.statlearning.com/).\n",
    "\n",
    "This lab is based on Section 6.6 Lab 2: Ridge Regression and the Lasso (p. 273 - 280) of the book.\n",
    "\n",
    "For this lab, we use the `Hitters` data set. It contains Major League Baseball Data from the 1986 and 1987 seasons. \n",
    "For more information about the data set, click [here](https://www.rdocumentation.org/packages/ISLR/versions/1.2/topics/Hitters).\n",
    "\n",
    "**Goal of the Lab:**    \n",
    "In this lab, we aim to fit a ridge regression model and lasso model to the `Hitters` data. We wish to predict a baseball player's `Salary` based on the statistics associated with performance in the previous year. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c6164",
   "metadata": {
    "cellIdentifier": "8yi8dfepd7j3r5agtfazlv"
   },
   "source": [
    "## Import modules, packages and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dcd75d",
   "metadata": {
    "cellIdentifier": "mqknzibk6dfxlvjtepo8j"
   },
   "source": [
    "First, we import some useful modules, packages and libraries. These are needed for carrying out the computations and for plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371beb72",
   "metadata": {
    "cellIdentifier": "11iq2wea9vlrm5feut7gt2l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# sci-kit learn specifics\n",
    "# We will use the sklearn package to obtain ridge regression and lasso models.\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428c1d2",
   "metadata": {
    "cellIdentifier": "ntalxsgc2ni51oxyvjcruk"
   },
   "source": [
    "## Load the `Hitters` data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c5a99",
   "metadata": {
    "cellIdentifier": "ib52ofx59gfzrht3h8nrw"
   },
   "source": [
    "We import the `Hitters` data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4822855",
   "metadata": {
    "cellIdentifier": "3j2xers83xco0awrnp0trn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 322 entries, -Andy Allanson to -Willie Wilson\n",
      "Data columns (total 20 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   AtBat      322 non-null    int64  \n",
      " 1   Hits       322 non-null    int64  \n",
      " 2   HmRun      322 non-null    int64  \n",
      " 3   Runs       322 non-null    int64  \n",
      " 4   RBI        322 non-null    int64  \n",
      " 5   Walks      322 non-null    int64  \n",
      " 6   Years      322 non-null    int64  \n",
      " 7   CAtBat     322 non-null    int64  \n",
      " 8   CHits      322 non-null    int64  \n",
      " 9   CHmRun     322 non-null    int64  \n",
      " 10  CRuns      322 non-null    int64  \n",
      " 11  CRBI       322 non-null    int64  \n",
      " 12  CWalks     322 non-null    int64  \n",
      " 13  League     322 non-null    object \n",
      " 14  Division   322 non-null    object \n",
      " 15  PutOuts    322 non-null    int64  \n",
      " 16  Assists    322 non-null    int64  \n",
      " 17  Errors     322 non-null    int64  \n",
      " 18  Salary     263 non-null    float64\n",
      " 19  NewLeague  322 non-null    object \n",
      "dtypes: float64(1), int64(16), object(3)\n",
      "memory usage: 52.8+ KB\n"
     ]
    }
   ],
   "source": [
    "hitters = pd.read_csv('data/Hitters.csv', index_col = 0)\n",
    "\n",
    "# Display information about the data set\n",
    "hitters.info()\n",
    "\n",
    "# Check for NaN values\n",
    "# hitters.isna().sum()\n",
    "\n",
    "# Return summary statistics for each column\n",
    "# hitters.describe()\n",
    "\n",
    "# Return first ten rows of the data set\n",
    "# hitters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f4853",
   "metadata": {
    "cellIdentifier": "c4gsm20oebfn5qnlz336ee"
   },
   "source": [
    "From the call on `hitters.info()`, we obtain that the variable `Salary` has only 263 counts which are non-null; this hints to the fact that there might be missing data. Furthermore, we observe that `League`, `Division`, and `NewLeague` are `objects`.\n",
    "\n",
    "With the code `hitters.isna().sum()`, we confirm that `Salary` indeed has `NaN` values. \n",
    "This is also confirmed when we returned the first 20 rows of the dataframe with `hitters.head(20)`; the first row has a `NaN` value for `Salary`. Additionally, we observe that `League` and `NewLeague` have only the values `A` and `N`, and division has the values `S` and `W`.\n",
    "\n",
    "We need to delete the columns which have missing data for `Salary` and convert the categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251338ce",
   "metadata": {
    "cellIdentifier": "mwyqh56gv6lsdqqyweisb"
   },
   "source": [
    "## Preprocess the `Hitters` data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052634e",
   "metadata": {
    "cellIdentifier": "3yleuyi45ayzcw5vpj5n3"
   },
   "source": [
    "In this part, we preprocess the `Hitters` data so that it is ready for the data fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05a631",
   "metadata": {
    "cellIdentifier": "r1jt7orrddizx1k0k8nzji"
   },
   "outputs": [],
   "source": [
    "# Remove any row containing at least one NaN value\n",
    "hitters_clean = hitters.dropna(axis = 0, how = 'any')\n",
    "\n",
    "# Display information about the data set\n",
    "hitters_clean.info()\n",
    "\n",
    "# First 10 rows of data set\n",
    "# hitters_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba144b80",
   "metadata": {
    "cellIdentifier": "gvhrp29u90is8mrfpav6e9"
   },
   "source": [
    "We obtain the 'cleaned' data with 263 row and 20 columns. \n",
    "This is in agreement with the result in p.244 of the book and also with our earlier observation when calling `hitters.info()`. \n",
    "\n",
    "We continue with transforming the *categorical* variables `League`, `Division`, and `NewLeague` to *indicator* variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194056c",
   "metadata": {
    "cellIdentifier": "scsoghg4t4mzjtzkdr2ryc"
   },
   "outputs": [],
   "source": [
    "#create dummies variable\n",
    "dummies = pd.get_dummies(hitters_clean[['League', 'Division', 'NewLeague']], dtype=int)\n",
    "\n",
    "# Display information about the data set\n",
    "# dummies.info()\n",
    "\n",
    "# Return summary statistics for each column\n",
    "# dummies.describe()\n",
    "\n",
    "# First 10 rows of data set\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c6092",
   "metadata": {
    "cellIdentifier": "je1ir4dn4gajxrnx2h0wqr"
   },
   "outputs": [],
   "source": [
    "# Make a copy of hitters_clean data set\n",
    "hitters_ind = hitters_clean.copy()\n",
    "\n",
    "# Replace the columns with their 0/1 values\n",
    "# League = 'N' is assigned the value 1\n",
    "# Division = 'W' is assigned the value 1\n",
    "# NewLeague = 'N' is assigned the value 1\n",
    "hitters_ind['League'] = dummies['League_N']\n",
    "hitters_ind['Division'] = dummies['Division_W']\n",
    "hitters_ind['NewLeague'] = dummies['NewLeague_N']\n",
    "\n",
    "# Note that hitters_clean is not changed, but hitters_ind is.\n",
    "hitters_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee990a",
   "metadata": {
    "cellIdentifier": "26ncb1k9q2tx8nkunwpmek"
   },
   "outputs": [],
   "source": [
    "# Note that hitters_clean is not changed, but hitters_ind is.\n",
    "hitters_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3c90d",
   "metadata": {
    "cellIdentifier": "gnha275a47e844tx8twq"
   },
   "source": [
    "We set up the variables needed for the Ridge Regression and Lasso fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda5a01",
   "metadata": {
    "cellIdentifier": "cj6vfjvd11n7rjopvijd7a"
   },
   "outputs": [],
   "source": [
    "# We set up the variables for the Ridge Regression fit.\n",
    "\n",
    "# The X variable containing the predictors\n",
    "X = hitters_ind.drop('Salary', axis=1)\n",
    "\n",
    "# The y variable containing the response\n",
    "y = hitters_ind['Salary']\n",
    "\n",
    "# Standardize the X variable\n",
    "xscaler = StandardScaler()\n",
    "X_scaled = xscaler.fit_transform(X)\n",
    "\n",
    "# Check if X_scaled has mean zero for each column\n",
    "print('\\nMean of the standardized X:')\n",
    "print(X_scaled.mean(axis = 0))\n",
    "print('\\nVariance of the standardized X:')\n",
    "print(X_scaled.var(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742a432",
   "metadata": {
    "cellIdentifier": "euwi1s189uqp3o7xi406i"
   },
   "source": [
    "## Ridge Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b2c33",
   "metadata": {
    "cellIdentifier": "hsc0ck31hkozxq7x1bs15"
   },
   "source": [
    "We perform ridge regression in order to predict the baseball player's `Salary` based on the statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c825d9",
   "metadata": {
    "cellIdentifier": "pyvggq5rhj59ijka7n23b"
   },
   "outputs": [],
   "source": [
    "# Range of values for lambda, the tuning parameter\n",
    "lambdas = 10**np.linspace(8, -2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbf7b0",
   "metadata": {
    "cellIdentifier": "lntsqj21j3odirhu4m8t"
   },
   "source": [
    "With the chosen range for `lambdas`, we cover the full range of scenarios from the null model containing only the intercept ($ \\lambda = 10^{8} $), to the least squares fit ($ \\lambda = 10^{-2} $).\n",
    "\n",
    "For each particular value in `lambdas`, we obtain a vector of ridge regression coefficients + an intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383bffc",
   "metadata": {
    "cellIdentifier": "ey0lh6mgvkgqyq48rbqh1e"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(fit_intercept=True)\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefs_ridge = pd.DataFrame(columns=np.append('Intercept', X.columns))\n",
    "\n",
    "# Loop through lambdas\n",
    "for i, l in enumerate(lambdas):\n",
    "    # Update the lambda value (note that the argument is called alpha)\n",
    "    ridge.set_params(alpha=l)\n",
    "    ridge.fit(X_scaled, y)\n",
    "    coefs_ridge.loc[i, :] = np.append([ridge.intercept_], ridge.coef_)\n",
    "\n",
    "coefs_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7a212",
   "metadata": {
    "cellIdentifier": "mowyfyvrm5i3owqx25xfgd"
   },
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(lambdas, coefs_ridge)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('estimated coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da174b67-4cca-480e-b735-620c54750ad7",
   "metadata": {
    "cellIdentifier": "nkfpcb5vrcws90njz8b4s"
   },
   "source": [
    "We now look more closely at two specific lambdas, at index $40$ and $60$. We can see from the code below what the $\\lambda$ values, coefficients, and magnitude of the penalty term was for these two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acd181",
   "metadata": {
    "cellIdentifier": "j8k91fc64wggpqouxgzor"
   },
   "outputs": [],
   "source": [
    "# First, we check the coefficients at index 40\n",
    "idx = 40\n",
    "l = lambdas[idx]\n",
    "coefs_idx = coefs_ridge.iloc[idx,:].drop('Intercept')\n",
    "norm = (coefs_idx**2).sum()**0.5\n",
    "\n",
    "print('At index {:d}, we find a lambda value of {:.4e}, a penalty norm of {:.4f}, and the following coefficients:'.format(idx, l, norm))\n",
    "print(coefs_idx)\n",
    "\n",
    "# Now, we check the coefficients at index 60\n",
    "idx = 60\n",
    "l = lambdas[idx]\n",
    "coefs_idx = coefs_ridge.iloc[idx,:].drop('Intercept')\n",
    "norm = (coefs_idx**2).sum()**0.5\n",
    "\n",
    "print('\\nAt index {:d}, we find a lambda value of {:.4e}, a penalty norm of {:.4f}, and the following coefficients:'.format(idx, l, norm))\n",
    "print(coefs_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77b485",
   "metadata": {
    "cellIdentifier": "f1ri3134mqcgpquwfszsiv"
   },
   "source": [
    "## Split data into training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b8c91",
   "metadata": {
    "cellIdentifier": "9a2wu3sktdd80gj6uc0wsl"
   },
   "source": [
    "We are going to split the data; half of it will be training data while the remaining portion is going to be test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0cd72",
   "metadata": {
    "cellIdentifier": "h3x2ihh8elsfn1o6hvvf14"
   },
   "outputs": [],
   "source": [
    "# Obtain training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1a631",
   "metadata": {
    "cellIdentifier": "tbrncxpjt89pqs7dhnyt2"
   },
   "source": [
    "Now, we fit a ridge regression model with three different $\\lambda$ values ($0$, $4$ and $10^{10}$) using the training data and evaluate its MSE on the test set. The model where $\\lambda=0$ corresponds to the least squares fit, whereas $\\lambda = 10^{10}$ corresponds to a model with only the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3970f",
   "metadata": {
    "cellIdentifier": "7nqi8yaxq3il438392a77"
   },
   "outputs": [],
   "source": [
    "lambdas_sel = [0, 4, 10**10]\n",
    "mse_lambdas_sel = []\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefs_ridge_sel = pd.DataFrame(columns=np.append('Intercept', X.columns))\n",
    "\n",
    "# Loop through lambdas\n",
    "for i, l in enumerate(lambdas_sel):\n",
    "    ridge.set_params(alpha=l)\n",
    "\n",
    "    # Use training data to fit the model\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs_ridge_sel.loc[i, :] = np.append(ridge.intercept_, ridge.coef_)\n",
    "\n",
    "    # Compute the error based on test data\n",
    "    mse_lambdas_sel.append(mean_squared_error(y_test, ridge.predict(X_test)))\n",
    "\n",
    "# Print the MSE for the different values of lambda\n",
    "print('\\n For lambda equal to {}, we obtain a mean squared error of {:.4f}'.format(lambdas_sel[0], mse_lambdas_sel[0]))\n",
    "print('\\n For lambda equal to {}, we obtain a mean squared error of {:.4f}'.format(lambdas_sel[1], mse_lambdas_sel[1]))\n",
    "print('\\n For lambda equal to {:.0e}, we obtain a mean squared error of {:.4f}'.format(lambdas_sel[2], mse_lambdas_sel[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc286995",
   "metadata": {
    "cellIdentifier": "bxsske2upwbh81ldspimle"
   },
   "source": [
    "So we see that fitting a ridge regression model with $\\lambda = 4$ leads to a much lower test MSE than fitting a model with just an intercept. The MSE score is also smaller than for the least squares fitting without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f604dd7",
   "metadata": {
    "cellIdentifier": "fmsmupfc3duq0219gcvzgl"
   },
   "source": [
    "## Use Cross-validation to choose the tuning parameter $ \\lambda $. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e62ef",
   "metadata": {
    "cellIdentifier": "afzsue7bflgingl7tdkr9e"
   },
   "source": [
    "Until now, we have chosen a specific value for $ \\lambda $. We can use 10-fold cross-validation to obtain an optimal value for $ \\lambda $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022bb0e",
   "metadata": {
    "cellIdentifier": "6pqz0pc5wysejz2cuj3l"
   },
   "outputs": [],
   "source": [
    "# Optimize alpha with cross-validation\n",
    "param_grid = {'alpha':lambdas}\n",
    "ridgeCV = GridSearchCV(ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "ridgeCV.fit(X_train, y_train)\n",
    "\n",
    "lambda_cv = ridgeCV.best_params_['alpha']\n",
    "validation_mse = -ridgeCV.best_score_\n",
    "\n",
    "# Print the cross-validation MSE for the optimized value of lambda\n",
    "print('\\n For lambda equal to {:.4f}, we obtain a mean squared cross-validation error of {:.4f}'.format(lambda_cv, validation_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede3e59",
   "metadata": {
    "cellIdentifier": "u0om9islosec7dw4s8cc6"
   },
   "source": [
    "Using the above code, we obtain the value for $ \\lambda $ with the smallest cross-validation error. What is now the test MSE associated to this value of $ \\lambda $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72691fa2",
   "metadata": {
    "cellIdentifier": "l1wan2kaynpfy8skwtr9tk"
   },
   "outputs": [],
   "source": [
    "# Create a pandas dataframe to store the coefficients\n",
    "coefs_ridge_cv = pd.DataFrame(columns=np.append('Intercept', X.columns))\n",
    "\n",
    "# Use training data to fit the model\n",
    "ridge.set_params(alpha=lambda_cv)\n",
    "ridge.fit(X_train, y_train)\n",
    "coefs_ridge_cv.loc[0, :] = np.append(ridge.intercept_, ridge.coef_)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "\n",
    "# Print the test MSE for the optimized value of lambda\n",
    "print('\\n For lambda equal to {:.4f}, we obtain a mean squared test error of {:.4f}'.format(lambda_cv, ridge_test_mse))\n",
    "\n",
    "coefs_ridge_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf8b0d",
   "metadata": {
    "cellIdentifier": "uz2sihsbb7ihuws0e3rcnc"
   },
   "source": [
    "Notice that this MSE value is smaller than the MSE value for $\\lambda = 4$.\n",
    "As expected, none of the coefficients are exactly zero - ridge regression does not perform variable selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea194c1",
   "metadata": {
    "cellIdentifier": "rmd1dlb1gbimw5ye4bgr"
   },
   "source": [
    "## The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5230e",
   "metadata": {
    "cellIdentifier": "cw9xaz7lfuia897p1ly43"
   },
   "source": [
    "We have seen that ridge regression with a wise choice of $ \\lambda $ can outperform least squares as well as the null model (having only the Intercept) on the `Hitters` data set. \n",
    "We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be3e19",
   "metadata": {
    "cellIdentifier": "453rhjb9e4o22nuq7ug2qu"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(fit_intercept=True, max_iter=10000)\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefs_lasso = pd.DataFrame(columns=np.append('Intercept', X.columns))\n",
    "\n",
    "# Loop through lambdas\n",
    "for i, l in enumerate(lambdas):\n",
    "    lasso.set_params(alpha=l)\n",
    "\n",
    "    # Use training data to fit the model\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs_lasso.loc[i, :] = np.append(lasso.intercept_, lasso.coef_)\n",
    "\n",
    "coefs_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037ef1b",
   "metadata": {
    "cellIdentifier": "4tzgrrfgven3y434wgefhd"
   },
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(lambdas, coefs_lasso)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('lambda')\n",
    "ax.set_ylabel('estimated coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc2318",
   "metadata": {
    "cellIdentifier": "qfcrbqhys8n94xxhzwnj"
   },
   "source": [
    "We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461a665",
   "metadata": {
    "cellIdentifier": "eyv6yk2pm5gwsqr2lx94"
   },
   "outputs": [],
   "source": [
    "# Optimize alpha with cross-validation\n",
    "param_grid = {'alpha':lambdas}\n",
    "lassoCV = GridSearchCV(lasso, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "lassoCV.fit(X_train, y_train)\n",
    "\n",
    "lambda_cv = lassoCV.best_params_['alpha']\n",
    "validation_mse = -lassoCV.best_score_\n",
    "\n",
    "# Print the cross-validation MSE for the optimized value of lambda\n",
    "print('\\n For lambda equal to {:.4f}, we obtain a mean squared cross-validation error of {:.4f}'.format(lambda_cv, validation_mse))\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefs_lasso_cv = pd.DataFrame(columns=np.append('Intercept', X.columns))\n",
    "\n",
    "# Use training data to fit the model\n",
    "lasso.set_params(alpha=lambda_cv)\n",
    "lasso.fit(X_train, y_train)\n",
    "coefs_lasso_cv.loc[0, :] = np.append(lasso.intercept_, lasso.coef_)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso.predict(X_test))\n",
    "\n",
    "# Print the test MSE for the optimized value of lambda\n",
    "print('\\n For lambda equal to {:.4f}, we obtain a mean squared test error of {:.4f}'.format(lambda_cv, lasso_test_mse))\n",
    "\n",
    "coefs_lasso_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4a406",
   "metadata": {
    "cellIdentifier": "r04e24pynjghve899eckri"
   },
   "source": [
    "This is lower than the test set MSE of the null model and of least squares, and similar to the test MSE of ridge regression with $\\lambda = 4$.\n",
    "However, the lasso has the advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that $2$ of the $19$ coefficient estimates have been set to $0$, so the lasso model with $\\lambda=1.0476$ contains only $17$ variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
