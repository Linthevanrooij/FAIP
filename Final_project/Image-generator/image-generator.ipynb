{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGConformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EEGConformer, self).__init__()\n",
    "\n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(40, 40, kernel_size=(1, input_dim), stride=(1, 1))\n",
    "        self.batchnorm = nn.BatchNorm2d(40)\n",
    "\n",
    "        # Transformer\n",
    "        self.layernorm1 = nn.LayerNorm(40)\n",
    "        self.multiheadattention = nn.MultiheadAttention(40, 1)\n",
    "        self.layernorm2 = nn.LayerNorm(40)\n",
    "\n",
    "        self.feedworward_block = nn.Sequential(\n",
    "            nn.Linear(40, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32, 40)\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.fc1 = nn.Linear(40, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = x.unsqueeze(1).unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        # Transformer\n",
    "        x = x.squeeze()\n",
    "        x = self.layernorm1(x)\n",
    "        attn_out = self.multiheadattention(x, x, x)\n",
    "        x = x + nn.Dropout(0.1)(attn_out[0])\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.feedworward_block(x)\n",
    "        x = nn.Dropout(p=0.1)(x)\n",
    "\n",
    "        # MLP\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = nn.Dropout(p=0.5)(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = nn.Dropout(p=0.3)(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2524  \n",
    "num_classes = 3   \n",
    "model = EEGConformer(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGConformer(\n",
       "  (conv1): Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1))\n",
       "  (conv2): Conv2d(40, 40, kernel_size=(1, 2524), stride=(1, 1))\n",
       "  (batchnorm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layernorm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "  (multiheadattention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=40, out_features=40, bias=True)\n",
       "  )\n",
       "  (layernorm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "  (feedworward_block): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=32, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=40, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=40, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'models\\Conformer_EEG.pth'\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_emotions_data = pd.read_csv('emotions.csv')\n",
    "eeg_emotions_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2548)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = eeg_emotions_data.iloc[0:3,:].drop(['label'], axis=1)\n",
    "y = eeg_emotions_data['label']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_emotions = LabelEncoder()\n",
    "y = labelencoder_emotions.fit_transform(y)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "type(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'NEUTRAL', 'POSITIVE'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(torch.from_numpy(X).float()).detach().numpy().argmax(axis=1)\n",
    "prediction = labelencoder_emotions.inverse_transform(prediction)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation using diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c97893c38f4639b58de2e968e80398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#device = \"cuda\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m----> 8\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:431\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been loaded in 8bit and moving it to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    434\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[0;32m    438\u001b[0m ):\n\u001b[0;32m    439\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\transformers\\modeling_utils.py:2952\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2948\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2949\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2950\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2951\u001b[0m         )\n\u001b[1;32m-> 2952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 779 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\josebencao9\\miniconda3\\envs\\faip\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cpu\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation for each feeling \n",
    "- does not run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373f72518aca45d99ce82a51b281c55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(prediction)):\n",
    "    prompt = 'a photo of an engineering sutdent going through' + prediction[i] + 'feelings'\n",
    "    image = pipe(prompt)\n",
    "    imageFile = 'predicted_image_'+str(i)+'.png'\n",
    "    image.save(imageFile)\n",
    "    image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
